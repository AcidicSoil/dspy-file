You're asking about which DSPy feature or function to use when working with a small dataset.<cite />

## Answer

For small datasets, DSPy offers several optimizers specifically designed to work effectively with limited data:

### 1. **`dspy.BootstrapFewShot`** - Best for very small datasets (5-50 examples)

This optimizer generates demonstrations from your small dataset using a teacher model and validates them with your metric. [1](#0-0)  It requires only `max_labeled_demos` (randomly selected from your trainset) and `max_bootstrapped_demos` (generated by the teacher). [1](#0-0)

### 2. **`dspy.BootstrapFewShotWithRandomSearch`** - For slightly larger small datasets (10-100 examples)

This applies `BootstrapFewShot` multiple times with random search and selects the best program. [2](#0-1)  It evaluates `num_candidate_programs` different configurations to find the optimal one. [2](#0-1)

Example usage:

```python
config = dict(max_bootstrapped_demos=4, max_labeled_demos=4, num_candidate_programs=10, num_threads=4)
teleprompter = BootstrapFewShotWithRandomSearch(metric=YOUR_METRIC_HERE, **config)
optimized_program = teleprompter.compile(YOUR_PROGRAM_HERE, trainset=YOUR_TRAINSET_HERE)
``` [3](#0-2)

### 3. **`dspy.MIPROv2`** with `auto="light"` mode - For 30-300 examples

MIPROv2 can work with small datasets when configured conservatively. [4](#0-3)  The `auto="light"` mode reduces the optimization budget, making it suitable for smaller datasets. [5](#0-4)

### Key Considerations

DSPy is explicitly designed to work with small datasets - you can start with as few as 5-10 examples and still get strong results. [6](#0-5)  The documentation emphasizes that "you can start small and get strong results." [7](#0-6)

For training and validation, it's recommended to have 30-300 examples each, though optimizers can work with less. [8](#0-7)  For prompt optimizers specifically, it's often better to pass more validation examples than training examples. [9](#0-8)

## Notes

For audio or multimodal tasks with small datasets, the documentation specifically recommends configuring optimizers like `dspy.BootstrapFewShotWithRandomSearch` or `dspy.MIPROv2` conservatively with 0-2 few-shot examples. [10](#0-9)  The general principle is that DSPy's bootstrapping approach allows you to generate synthetic training data from your small dataset, effectively amplifying its utility.<cite />

Wiki pages you might want to explore:
- [Overview (stanfordnlp/dspy)](/wiki/stanfordnlp/dspy#1)

### Citations

**File:** docs/docs/learn/optimization/optimizers.md (L17-19)
```markdown
- A few **training inputs**. This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels).

If you happen to have a lot of data, DSPy can leverage that. But you can start small and get strong results.
```

**File:** docs/docs/learn/optimization/optimizers.md (L46-46)

```markdown
2. [**`BootstrapFewShot`**](../../api/optimizers/BootstrapFewShot.md): Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program, along with labeled examples in `trainset`. Parameters include `max_labeled_demos` (the number of demonstrations randomly selected from the `trainset`) and `max_bootstrapped_demos` (the number of additional examples generated by the `teacher`). The bootstrapping process employs the metric to validate demonstrations, including only those that pass the metric in the "compiled" prompt. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.
```

**File:** docs/docs/learn/optimization/optimizers.md (L48-48)

```markdown
3. [**`BootstrapFewShotWithRandomSearch`**](../../api/optimizers/BootstrapFewShotWithRandomSearch.md): Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program over the optimization. Parameters mirror those of `BootstrapFewShot`, with the addition of `num_candidate_programs`, which specifies the number of random programs evaluated over the optimization, including candidates of the uncompiled program, `LabeledFewShot` optimized program, `BootstrapFewShot` compiled program with unshuffled examples and `num_candidate_programs` of `BootstrapFewShot` compiled programs with randomized example sets.
```

**File:** docs/docs/learn/optimization/optimizers.md (L98-104)

```markdown
# Set up the optimizer: we want to "bootstrap" (i.e., self-generate) 8-shot examples of your program's steps.
# The optimizer will repeat this 10 times (plus some initial attempts) before selecting its best attempt on the devset.
config = dict(max_bootstrapped_demos=4, max_labeled_demos=4, num_candidate_programs=10, num_threads=4)

teleprompter = BootstrapFewShotWithRandomSearch(metric=YOUR_METRIC_HERE, **config)
optimized_program = teleprompter.compile(YOUR_PROGRAM_HERE, trainset=YOUR_TRAINSET_HERE)
```

```

**File:** docs/docs/learn/optimization/optimizers.md (L130-131)
```markdown
        tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto="light", num_threads=24)
        optimized_react = tp.compile(react, trainset=trainset)
```

**File:** docs/docs/index.md (L354-355)

```markdown
        tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto="light", num_threads=24)
        optimized_react = tp.compile(react, trainset=trainset)
```

**File:** docs/docs/tutorials/rag/index.ipynb (L345-345)

```text
    "    - It's good to have 30--300 examples for training and validation each.\n",
```

**File:** docs/docs/tutorials/rag/index.ipynb (L346-346)

```text
    "    - For prompt optimizers in particular, it's often better to pass _more_ validation than training.\n",
```

**File:** docs/docs/tutorials/audio/index.ipynb (L154-154)

```text
    "Note: Audio tokens can be costly so it is recommended to configure optimizers like `dspy.BootstrapFewShotWithRandomSearch` or `dspy.MIPROv2` conservatively with 0-2 few shot examples and less candidates / trials than the optimizer default parameters."
```
