# TODO

* -v | verbose: output config specs that model loaded with

* iterate on templates of choice that were previously generated for mass iteratation variations

* option for users to inject /nothink for qwen3 models

* TUI for easy prompt selection picker etc... and many other useful functions

* add a percentage of progress during inferences so users can see how much longer is left on the job.

* gather llm provider models options to further fine-tune the jobs by tweaking model parameters.

* analyze and research bug fixes for errors (using web searching functionality from top rated sites docs etc...)

