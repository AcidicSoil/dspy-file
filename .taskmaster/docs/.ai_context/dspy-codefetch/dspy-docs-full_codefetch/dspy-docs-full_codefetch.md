Project Structure:
├── api
│   ├── adapters
│   │   ├── Adapter.md
│   │   ├── ChatAdapter.md
│   │   ├── JSONAdapter.md
│   │   └── TwoStepAdapter.md
│   ├── evaluation
│   │   ├── CompleteAndGrounded.md
│   │   ├── Evaluate.md
│   │   ├── EvaluationResult.md
│   │   ├── SemanticF1.md
│   │   ├── answer_exact_match.md
│   │   └── answer_passage_match.md
│   ├── experimental
│   │   ├── Citations.md
│   │   └── Document.md
│   ├── index.md
│   ├── models
│   │   ├── Embedder.md
│   │   └── LM.md
│   ├── modules
│   │   ├── BestOfN.md
│   │   ├── ChainOfThought.md
│   │   ├── CodeAct.md
│   │   ├── Module.md
│   │   ├── MultiChainComparison.md
│   │   ├── Parallel.md
│   │   ├── Predict.md
│   │   ├── ProgramOfThought.md
│   │   ├── ReAct.md
│   │   └── Refine.md
│   ├── optimizers
│   │   ├── BetterTogether.md
│   │   ├── BootstrapFewShot.md
│   │   ├── BootstrapFewShotWithRandomSearch.md
│   │   ├── BootstrapFinetune.md
│   │   ├── BootstrapRS.md
│   │   ├── COPRO.md
│   │   ├── Ensemble.md
│   │   ├── GEPA
│   │   │   ├── GEPA_Advanced.md
│   │   │   └── overview.md
│   │   ├── InferRules.md
│   │   ├── KNN.md
│   │   ├── KNNFewShot.md
│   │   ├── LabeledFewShot.md
│   │   ├── MIPROv2.md
│   │   └── SIMBA.md
│   ├── primitives
│   │   ├── Audio.md
│   │   ├── Code.md
│   │   ├── Example.md
│   │   ├── History.md
│   │   ├── Image.md
│   │   ├── Prediction.md
│   │   ├── Tool.md
│   │   └── ToolCalls.md
│   ├── signatures
│   │   ├── InputField.md
│   │   ├── OutputField.md
│   │   └── Signature.md
│   ├── tools
│   │   ├── ColBERTv2.md
│   │   ├── Embeddings.md
│   │   └── PythonInterpreter.md
│   └── utils
│       ├── StatusMessage.md
│       ├── StatusMessageProvider.md
│       ├── StreamListener.md
│       ├── asyncify.md
│       ├── configure_cache.md
│       ├── disable_litellm_logging.md
│       ├── disable_logging.md
│       ├── enable_litellm_logging.md
│       ├── enable_logging.md
│       ├── inspect_history.md
│       ├── load.md
│       └── streamify.md
├── cheatsheet.md
├── community
│   ├── community-resources.md
│   ├── how-to-contribute.md
│   └── use-cases.md
├── deep-dive
│   └── data-handling
│       ├── built-in-datasets.md
│       ├── examples.md
│       ├── img
│       │   └── data-loading.png
│       └── loading-custom-data.md
├── faqs.md
├── index.md
├── js
│   └── runllm-widget.js
├── learn
│   ├── evaluation
│   │   ├── data.md
│   │   ├── metrics.md
│   │   └── overview.md
│   ├── figures
│   │   ├── native_tool_call.png
│   │   └── teleprompter-classes.png
│   ├── index.md
│   ├── optimization
│   │   ├── optimizers.md
│   │   └── overview.md
│   └── programming
│       ├── 7-assertions.md
│       ├── adapters.md
│       ├── language_models.md
│       ├── modules.md
│       ├── overview.md
│       ├── signatures.md
│       └── tools.md
├── production
│   └── index.md
├── roadmap.md
├── static
│   └── img
│       ├── dspy_logo.png
│       ├── logo.png
│       ├── mlflow-tracing-rag.png
│       ├── modular.png
│       ├── optimize.png
│       ├── undraw_docusaurus_mountain.svg
│       ├── undraw_docusaurus_react.svg
│       ├── undraw_docusaurus_tree.svg
│       └── universal_compatibility.png
├── stylesheets
│   └── extra.css
└── tutorials
    ├── agents
    │   ├── index.ipynb
    │   └── mlflow-tracing-agent.png
    ├── ai_text_game
    │   └── index.md
    ├── async
    │   └── index.md
    ├── audio
    │   └── index.ipynb
    ├── build_ai_program
    │   └── index.md
    ├── cache
    │   └── index.md
    ├── classification
    │   └── index.md
    ├── classification_finetuning
    │   ├── index.ipynb
    │   └── mlflow-tracing-classification.png
    ├── codefetch
    ├── conversation_history
    │   └── index.md
    ├── core_development
    │   └── index.md
    ├── custom_module
    │   ├── index.ipynb
    │   └── mlflow-tracing-custom-module.png
    ├── customer_service_agent
    │   ├── index.ipynb
    │   └── mlflow-tracing-customer-service-agent.png
    ├── deployment
    │   ├── dspy_mlflow_ui.png
    │   └── index.md
    ├── email_extraction
    │   ├── index.md
    │   └── mlflow-tracing-email-extraction.png
    ├── entity_extraction
    │   ├── index.ipynb
    │   └── mlflow-tracing-entity-extraction.png
    ├── games
    │   ├── index.ipynb
    │   └── mlflow-tracing-agent.png
    ├── gepa_ai_program
    │   └── index.md
    ├── gepa_aime
    │   ├── index.ipynb
    │   ├── mlflow-tracing-gepa-aime.png
    │   └── mlflow-tracking-gepa-aime-optimization.png
    ├── gepa_facilitysupportanalyzer
    │   ├── index.ipynb
    │   ├── mlflow-tracing-gepa-support.png
    │   └── mlflow-tracking-gepa-support-optimization.png
    ├── gepa_papillon
    │   ├── index.ipynb
    │   ├── mlflow-tracing-gepa-papilon.png
    │   └── mlflow-tracking-gepa-papilon-optimization.png
    ├── image_generation_prompting
    │   └── index.ipynb
    ├── index.md
    ├── llms_txt_generation
    │   └── index.md
    ├── math
    │   ├── index.ipynb
    │   └── mlflow-tracing-math.png
    ├── mcp
    │   └── index.md
    ├── mem0_react_agent
    │   └── index.md
    ├── multihop_search
    │   ├── index.ipynb
    │   └── mlflow-tracing-multi-hop.png
    ├── observability
    │   ├── index.md
    │   ├── mlflow_trace_ui.png
    │   ├── mlflow_trace_ui_navigation.gif
    │   └── mlflow_trace_view.png
    ├── optimize_ai_program
    │   └── index.md
    ├── optimizer_tracking
    │   ├── child_run.png
    │   ├── experiment.png
    │   ├── index.md
    │   └── parent_run.png
    ├── output_refinement
    │   └── best-of-n-and-refine.md
    ├── papillon
    │   └── index.md
    ├── program_of_thought
    │   └── index.ipynb
    ├── rag
    │   ├── index.ipynb
    │   └── mlflow-tracing-rag.png
    ├── real_world_examples
    │   └── index.md
    ├── rl_ai_program
    │   └── index.md
    ├── rl_multihop
    │   └── index.ipynb
    ├── rl_papillon
    │   └── index.ipynb
    ├── sample_code_generation
    │   └── index.md
    ├── saving
    │   └── index.md
    ├── streaming
    │   └── index.md
    ├── tool_use
    │   └── index.ipynb
    └── yahoo_finance_react
        └── index.md


cheatsheet.md
```
1 | ---
2 | sidebar_position: 999
3 | ---
4 | 
5 | # DSPy Cheatsheet
6 | 
7 | This page will contain snippets for frequent usage patterns.
8 | 
9 | ## DSPy Programs
10 | 
11 | ### Forcing fresh LM outputs
12 | 
13 | DSPy caches LM calls. Provide a unique ``rollout_id`` and set a non-zero
14 | ``temperature`` (e.g., 1.0) to bypass an existing cache entry while still caching
15 | the new result:
16 | 
17 | ```python
18 | predict = dspy.Predict("question -> answer")
19 | predict(question="1+1", config={"rollout_id": 1, "temperature": 1.0})
20 | ```
21 | 
22 | ### dspy.Signature
23 | 
24 | ```python
25 | class BasicQA(dspy.Signature):
26 |     """Answer questions with short factoid answers."""
27 | 
28 |     question: str = dspy.InputField()
29 |     answer: str = dspy.OutputField(desc="often between 1 and 5 words")
30 | ```
31 | 
32 | ### dspy.ChainOfThought
33 | 
34 | ```python
35 | generate_answer = dspy.ChainOfThought(BasicQA)
36 | 
37 | # Call the predictor on a particular input alongside a hint.
38 | question='What is the color of the sky?'
39 | pred = generate_answer(question=question)
40 | ```
41 | 
42 | ### dspy.ProgramOfThought
43 | 
44 | ```python
45 | pot = dspy.ProgramOfThought(BasicQA)
46 | 
47 | question = 'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?'
48 | result = pot(question=question)
49 | 
50 | print(f"Question: {question}")
51 | print(f"Final Predicted Answer (after ProgramOfThought process): {result.answer}")
52 | ```
53 | 
54 | ### dspy.ReAct
55 | 
56 | ```python
57 | react_module = dspy.ReAct(BasicQA)
58 | 
59 | question = 'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?'
60 | result = react_module(question=question)
61 | 
62 | print(f"Question: {question}")
63 | print(f"Final Predicted Answer (after ReAct process): {result.answer}")
64 | ```
65 | 
66 | ### dspy.Retrieve
67 | 
68 | ```python
69 | colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')
70 | dspy.settings.configure(rm=colbertv2_wiki17_abstracts)
71 | 
72 | #Define Retrieve Module
73 | retriever = dspy.Retrieve(k=3)
74 | 
75 | query='When was the first FIFA World Cup held?'
76 | 
77 | # Call the retriever on a particular query.
78 | topK_passages = retriever(query).passages
79 | 
80 | for idx, passage in enumerate(topK_passages):
81 |     print(f'{idx+1}]', passage, '\n')
82 | ```
83 | 
84 | ### dspy.CodeAct
85 | 
86 | ```python
87 | from dspy import CodeAct
88 | 
89 | def factorial(n):
90 |     """Calculate factorial of n"""
91 |     if n == 1:
92 |         return 1
93 |     return n * factorial(n-1)
94 | 
95 | act = CodeAct("n->factorial", tools=[factorial])
96 | result = act(n=5)
97 | result # Returns 120
98 | ```
99 | 
100 | ### dspy.Parallel
101 | 
102 | ```python
103 | import dspy
104 | 
105 | parallel = dspy.Parallel(num_threads=2)
106 | predict = dspy.Predict("question -> answer")
107 | result = parallel(
108 |     [
109 |         (predict, dspy.Example(question="1+1").with_inputs("question")),
110 |         (predict, dspy.Example(question="2+2").with_inputs("question"))
111 |     ]
112 | )
113 | result
114 | ```
115 | 
116 | ## DSPy Metrics
117 | 
118 | ### Function as Metric
119 | 
120 | To create a custom metric you can create a function that returns either a number or a boolean value:
121 | 
122 | ```python
123 | def parse_integer_answer(answer, only_first_line=True):
124 |     try:
125 |         if only_first_line:
126 |             answer = answer.strip().split('\n')[0]
127 | 
128 |         # find the last token that has a number in it
129 |         answer = [token for token in answer.split() if any(c.isdigit() for c in token)][-1]
130 |         answer = answer.split('.')[0]
131 |         answer = ''.join([c for c in answer if c.isdigit()])
132 |         answer = int(answer)
133 | 
134 |     except (ValueError, IndexError):
135 |         # print(answer)
136 |         answer = 0
137 | 
138 |     return answer
139 | 
140 | # Metric Function
141 | def gsm8k_metric(gold, pred, trace=None) -> int:
142 |     return int(parse_integer_answer(str(gold.answer))) == int(parse_integer_answer(str(pred.answer)))
143 | ```
144 | 
145 | ### LLM as Judge
146 | 
147 | ```python
148 | class FactJudge(dspy.Signature):
149 |     """Judge if the answer is factually correct based on the context."""
150 | 
151 |     context = dspy.InputField(desc="Context for the prediction")
152 |     question = dspy.InputField(desc="Question to be answered")
153 |     answer = dspy.InputField(desc="Answer for the question")
154 |     factually_correct: bool = dspy.OutputField(desc="Is the answer factually correct based on the context?")
155 | 
156 | judge = dspy.ChainOfThought(FactJudge)
157 | 
158 | def factuality_metric(example, pred):
159 |     factual = judge(context=example.context, question=example.question, answer=pred.answer)
160 |     return factual.factually_correct
161 | ```
162 | 
163 | ## DSPy Evaluation
164 | 
165 | ```python
166 | from dspy.evaluate import Evaluate
167 | 
168 | evaluate_program = Evaluate(devset=devset, metric=your_defined_metric, num_threads=NUM_THREADS, display_progress=True, display_table=num_rows_to_display)
169 | 
170 | evaluate_program(your_dspy_program)
171 | ```
172 | 
173 | ## DSPy Optimizers
174 | 
175 | ### LabeledFewShot
176 | 
177 | ```python
178 | from dspy.teleprompt import LabeledFewShot
179 | 
180 | labeled_fewshot_optimizer = LabeledFewShot(k=8)
181 | your_dspy_program_compiled = labeled_fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)
182 | ```
183 | 
184 | ### BootstrapFewShot
185 | 
186 | ```python
187 | from dspy.teleprompt import BootstrapFewShot
188 | 
189 | fewshot_optimizer = BootstrapFewShot(metric=your_defined_metric, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=10)
190 | 
191 | your_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)
192 | ```
193 | 
194 | #### Using another LM for compilation, specifying in teacher_settings
195 | 
196 | ```python
197 | from dspy.teleprompt import BootstrapFewShot
198 | 
199 | fewshot_optimizer = BootstrapFewShot(metric=your_defined_metric, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=10, teacher_settings=dict(lm=gpt4))
200 | 
201 | your_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset)
202 | ```
203 | 
204 | #### Compiling a compiled program - bootstrapping a bootstrapped program
205 | 
206 | ```python
207 | your_dspy_program_compiledx2 = teleprompter.compile(
208 |     your_dspy_program,
209 |     teacher=your_dspy_program_compiled,
210 |     trainset=trainset,
211 | )
212 | ```
213 | 
214 | #### Saving/loading a compiled program
215 | 
216 | ```python
217 | save_path = './v1.json'
218 | your_dspy_program_compiledx2.save(save_path)
219 | ```
220 | 
221 | ```python
222 | loaded_program = YourProgramClass()
223 | loaded_program.load(path=save_path)
224 | ```
225 | 
226 | ### BootstrapFewShotWithRandomSearch
227 | 
228 | Detailed documentation on BootstrapFewShotWithRandomSearch can be found [here](api/optimizers/BootstrapFewShot.md).
229 | 
230 | ```python
231 | from dspy.teleprompt import BootstrapFewShotWithRandomSearch
232 | 
233 | fewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)
234 | 
235 | your_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset, valset=devset)
236 | 
237 | ```
238 | 
239 | Other custom configurations are similar to customizing the `BootstrapFewShot` optimizer.
240 | 
241 | ### Ensemble
242 | 
243 | ```python
244 | from dspy.teleprompt import BootstrapFewShotWithRandomSearch
245 | from dspy.teleprompt.ensemble import Ensemble
246 | 
247 | fewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)
248 | your_dspy_program_compiled = fewshot_optimizer.compile(student = your_dspy_program, trainset=trainset, valset=devset)
249 | 
250 | ensemble_optimizer = Ensemble(reduce_fn=dspy.majority)
251 | programs = [x[-1] for x in your_dspy_program_compiled.candidate_programs]
252 | your_dspy_program_compiled_ensemble = ensemble_optimizer.compile(programs[:3])
253 | ```
254 | 
255 | ### BootstrapFinetune
256 | 
257 | ```python
258 | from dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune
259 | 
260 | #Compile program on current dspy.settings.lm
261 | fewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=your_defined_metric, max_bootstrapped_demos=2, num_threads=NUM_THREADS)
262 | your_dspy_program_compiled = tp.compile(your_dspy_program, trainset=trainset[:some_num], valset=trainset[some_num:])
263 | 
264 | #Configure model to finetune
265 | config = dict(target=model_to_finetune, epochs=2, bf16=True, bsize=6, accumsteps=2, lr=5e-5)
266 | 
267 | #Compile program on BootstrapFinetune
268 | finetune_optimizer = BootstrapFinetune(metric=your_defined_metric)
269 | finetune_program = finetune_optimizer.compile(your_dspy_program, trainset=some_new_dataset_for_finetuning_model, **config)
270 | 
271 | finetune_program = your_dspy_program
272 | 
273 | #Load program and activate model's parameters in program before evaluation
274 | ckpt_path = "saved_checkpoint_path_from_finetuning"
275 | LM = dspy.HFModel(checkpoint=ckpt_path, model=model_to_finetune)
276 | 
277 | for p in finetune_program.predictors():
278 |     p.lm = LM
279 |     p.activated = False
280 | ```
281 | 
282 | ### COPRO
283 | 
284 | Detailed documentation on COPRO can be found [here](api/optimizers/COPRO.md).
285 | 
286 | ```python
287 | from dspy.teleprompt import COPRO
288 | 
289 | eval_kwargs = dict(num_threads=16, display_progress=True, display_table=0)
290 | 
291 | copro_teleprompter = COPRO(prompt_model=model_to_generate_prompts, metric=your_defined_metric, breadth=num_new_prompts_generated, depth=times_to_generate_prompts, init_temperature=prompt_generation_temperature, verbose=False)
292 | 
293 | compiled_program_optimized_signature = copro_teleprompter.compile(your_dspy_program, trainset=trainset, eval_kwargs=eval_kwargs)
294 | ```
295 | 
296 | ### MIPROv2
297 | 
298 | Note: detailed documentation can be found [here](api/optimizers/MIPROv2.md). `MIPROv2` is the latest extension of `MIPRO` which includes updates such as (1) improvements to instruction proposal and (2) more efficient search with minibatching.
299 | 
300 | #### Optimizing with MIPROv2
301 | 
302 | This shows how to perform an easy out-of-the box run with `auto=light`, which configures many hyperparameters for you and performs a light optimization run. You can alternatively set `auto=medium` or `auto=heavy` to perform longer optimization runs. The more detailed `MIPROv2` documentation [here](api/optimizers/MIPROv2.md) also provides more information about how to set hyperparameters by hand.
303 | 
304 | ```python
305 | # Import the optimizer
306 | from dspy.teleprompt import MIPROv2
307 | 
308 | # Initialize optimizer
309 | teleprompter = MIPROv2(
310 |     metric=gsm8k_metric,
311 |     auto="light", # Can choose between light, medium, and heavy optimization runs
312 | )
313 | 
314 | # Optimize program
315 | print(f"Optimizing program with MIPRO...")
316 | optimized_program = teleprompter.compile(
317 |     program.deepcopy(),
318 |     trainset=trainset,
319 |     max_bootstrapped_demos=3,
320 |     max_labeled_demos=4,
321 | )
322 | 
323 | # Save optimize program for future use
324 | optimized_program.save(f"mipro_optimized")
325 | 
326 | # Evaluate optimized program
327 | print(f"Evaluate optimized program...")
328 | evaluate(optimized_program, devset=devset[:])
329 | ```
330 | 
331 | #### Optimizing instructions only with MIPROv2 (0-Shot)
332 | 
333 | ```python
334 | # Import the optimizer
335 | from dspy.teleprompt import MIPROv2
336 | 
337 | # Initialize optimizer
338 | teleprompter = MIPROv2(
339 |     metric=gsm8k_metric,
340 |     auto="light", # Can choose between light, medium, and heavy optimization runs
341 | )
342 | 
343 | # Optimize program
344 | print(f"Optimizing program with MIPRO...")
345 | optimized_program = teleprompter.compile(
346 |     program.deepcopy(),
347 |     trainset=trainset,
348 |     max_bootstrapped_demos=0,
349 |     max_labeled_demos=0,
350 | )
351 | 
352 | # Save optimize program for future use
353 | optimized_program.save(f"mipro_optimized")
354 | 
355 | # Evaluate optimized program
356 | print(f"Evaluate optimized program...")
357 | evaluate(optimized_program, devset=devset[:])
358 | ```
359 | 
360 | ### KNNFewShot
361 | 
362 | ```python
363 | from sentence_transformers import SentenceTransformer
364 | from dspy import Embedder
365 | from dspy.teleprompt import KNNFewShot
366 | from dspy import ChainOfThought
367 | 
368 | knn_optimizer = KNNFewShot(k=3, trainset=trainset, vectorizer=Embedder(SentenceTransformer("all-MiniLM-L6-v2").encode))
369 | 
370 | qa_compiled = knn_optimizer.compile(student=ChainOfThought("question -> answer"))
371 | ```
372 | 
373 | ### BootstrapFewShotWithOptuna
374 | 
375 | ```python
376 | from dspy.teleprompt import BootstrapFewShotWithOptuna
377 | 
378 | fewshot_optuna_optimizer = BootstrapFewShotWithOptuna(metric=your_defined_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=NUM_THREADS)
379 | 
380 | your_dspy_program_compiled = fewshot_optuna_optimizer.compile(student=your_dspy_program, trainset=trainset, valset=devset)
381 | ```
382 | 
383 | Other custom configurations are similar to customizing the `dspy.BootstrapFewShot` optimizer.
384 | 
385 | 
386 | ### SIMBA
387 | 
388 | SIMBA, which stands for Stochastic Introspective Mini-Batch Ascent, is a prompt optimizer that accepts arbitrary DSPy programs and proceeds in a sequence of mini-batches seeking to make incremental improvements to the prompt instructions or few-shot examples.
389 | 
390 | ```python
391 | from dspy.teleprompt import SIMBA
392 | 
393 | simba = SIMBA(metric=your_defined_metric, max_steps=12, max_demos=10)
394 | 
395 | optimized_program = simba.compile(student=your_dspy_program, trainset=trainset)
396 | ```
397 | 
398 | 
399 | ## DSPy Tools and Utilities
400 | 
401 | ### dspy.Tool
402 | 
403 | ```python
404 | import dspy
405 | 
406 | def search_web(query: str) -> str:
407 |     """Search the web for information"""
408 |     return f"Search results for: {query}"
409 | 
410 | tool = dspy.Tool(search_web)
411 | result = tool(query="Python programming")
412 | ```
413 | 
414 | ### dspy.streamify
415 | 
416 | ```python
417 | import dspy
418 | import asyncio
419 | 
420 | predict = dspy.Predict("question->answer")
421 | 
422 | stream_predict = dspy.streamify(
423 |     predict,
424 |     stream_listeners=[dspy.streaming.StreamListener(signature_field_name="answer")],
425 | )
426 | 
427 | async def read_output_stream():
428 |     output_stream = stream_predict(question="Why did a chicken cross the kitchen?")
429 | 
430 |     async for chunk in output_stream:
431 |         print(chunk)
432 | 
433 | asyncio.run(read_output_stream())
434 | ```
435 | 
436 | 
437 | ### dspy.asyncify
438 | 
439 | ```python
440 | import dspy
441 | 
442 | dspy_program = dspy.ChainOfThought("question -> answer")
443 | dspy_program = dspy.asyncify(dspy_program)
444 | 
445 | asyncio.run(dspy_program(question="What is DSPy"))
446 | ```
447 | 
448 | 
449 | ### Track Usage
450 | 
451 | ```python
452 | import dspy
453 | dspy.settings.configure(track_usage=True)
454 | 
455 | result = dspy.ChainOfThought(BasicQA)(question="What is 2+2?")
456 | print(f"Token usage: {result.get_lm_usage()}")
457 | ```
458 | 
459 | ### dspy.configure_cache
460 | 
461 | ```python
462 | import dspy
463 | 
464 | # Configure cache settings
465 | dspy.configure_cache(
466 |     enable_disk_cache=False,
467 |     enable_memory_cache=False,
468 | )
469 | ```
470 | 
471 | ## DSPy `Refine` and `BestofN`
472 | 
473 | >`dspy.Suggest` and `dspy.Assert` are replaced by `dspy.Refine` and `dspy.BestofN` in DSPy 2.6.
474 | 
475 | ### BestofN
476 | 
477 | Runs a module up to `N` times with different rollout IDs (bypassing cache) and returns the best prediction, as defined by the `reward_fn`, or the first prediction that passes the `threshold`.
478 | 
479 | ```python
480 | import dspy
481 | 
482 | qa = dspy.ChainOfThought("question -> answer")
483 | def one_word_answer(args, pred):
484 |     return 1.0 if len(pred.answer) == 1 else 0.0
485 | best_of_3 = dspy.BestOfN(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0)
486 | best_of_3(question="What is the capital of Belgium?").answer
487 | # Brussels
488 | ```
489 | 
490 | ### Refine
491 | 
492 | Refines a module by running it up to `N` times with different rollout IDs (bypassing cache) and returns the best prediction, as defined by the `reward_fn`, or the first prediction that passes the `threshold`. After each attempt (except the final one), `Refine` automatically generates detailed feedback about the module's performance and uses this feedback as hints for subsequent runs, creating an iterative refinement process.
493 | 
494 | ```python
495 | import dspy
496 | 
497 | qa = dspy.ChainOfThought("question -> answer")
498 | def one_word_answer(args, pred):
499 |     return 1.0 if len(pred.answer) == 1 else 0.0
500 | best_of_3 = dspy.Refine(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0)
501 | best_of_3(question="What is the capital of Belgium?").answer
502 | # Brussels
503 | ```
504 | 
505 | #### Error Handling
506 | 
507 | By default, `Refine` will try to run the module up to N times until the threshold is met. If the module encounters an error, it will keep going up to N failed attempts. You can change this behavior by setting `fail_count` to a smaller number than `N`.
508 | 
509 | ```python
510 | refine = dspy.Refine(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0, fail_count=1)
511 | ...
512 | refine(question="What is the capital of Belgium?")
513 | # If we encounter just one failed attempt, the module will raise an error.
514 | ```
515 | 
516 | If you want to run the module up to N times without any error handling, you can set `fail_count` to `N`. This is the default behavior.
517 | 
518 | ```python
519 | refine = dspy.Refine(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0, fail_count=3)
520 | ...
521 | refine(question="What is the capital of Belgium?")
522 | ```
```

faqs.md
```
1 | ---
2 | sidebar_position: 998
3 | ---
4 | 
5 | !!! warning "This page is outdated and may not be fully accurate in DSPy 2.5 and 2.6"
6 | 
7 | 
8 | # FAQs
9 | 
10 | ## Is DSPy right for me? DSPy vs. other frameworks
11 | 
12 | The **DSPy** philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when **DSPy** is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable **yes**. If you're a practitioner doing other things, please read on.
13 | 
14 | **DSPy vs. thin wrappers for prompts (OpenAI API, MiniChain, basic templating)** In other words: _Why can't I just write my prompts directly as string templates?_ Well, for extremely simple settings, this _might_ work just fine. (If you're familiar with neural networks, this is like expressing a tiny two-layer NN as a Python for-loop. It kinda works.) However, when you need higher quality (or manageable cost), then you need to iteratively explore multi-stage decomposition, improved prompting, data bootstrapping, careful finetuning, retrieval augmentation, and/or using smaller (or cheaper, or local) models. The true expressive power of building with foundation models lies in the interactions between these pieces. But every time you change one piece, you likely break (or weaken) multiple other components. **DSPy** cleanly abstracts away (_and_ powerfully optimizes) the parts of these interactions that are external to your actual system design. It lets you focus on designing the module-level interactions: the _same program_ expressed in 10 or 20 lines of **DSPy** can easily be compiled into multi-stage instructions for `GPT-4`, detailed prompts for `Llama2-13b`, or finetunes for `T5-base`. Oh, and you wouldn't need to maintain long, brittle, model-specific strings at the core of your project anymore.
15 | 
16 | **DSPy vs. application development libraries like LangChain, LlamaIndex** LangChain and LlamaIndex target high-level application development; they offer _batteries-included_, pre-built application modules that plug in with your data or configuration. If you'd be happy to use a generic, off-the-shelf prompt for question answering over PDFs or standard text-to-SQL, you will find a rich ecosystem in these libraries. **DSPy** doesn't internally contain hand-crafted prompts that target specific applications. Instead, **DSPy** introduces a small set of much more powerful and general-purpose modules _that can learn to prompt (or finetune) your LM within your pipeline on your data_. when you change your data, make tweaks to your program's control flow, or change your target LM, the **DSPy compiler** can map your program into a new set of prompts (or finetunes) that are optimized specifically for this pipeline. Because of this, you may find that **DSPy** obtains the highest quality for your task, with the least effort, provided you're willing to implement (or extend) your own short program. In short, **DSPy** is for when you need a lightweight but automatically-optimizing programming model — not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing **DSPy**) and HuggingFace Transformers (i.e., representing the higher-level libraries).
17 | 
18 | **DSPy vs. generation control libraries like Guidance, LMQL, RELM, Outlines** These are all exciting new libraries for controlling the individual completions of LMs, e.g., if you want to enforce JSON output schema or constrain sampling to a particular regular expression. This is very useful in many settings, but it's generally focused on low-level, structured control of a single LM call. It doesn't help ensure the JSON (or structured output) you get is going to be correct or useful for your task. In contrast, **DSPy** automatically optimizes the prompts in your programs to align them with various task needs, which may also include producing valid structured outputs. That said, we are considering allowing **Signatures** in **DSPy** to express regex-like constraints that are implemented by these libraries.
19 | 
20 | ## Basic Usage
21 | 
22 | **How should I use DSPy for my task?** We wrote a [eight-step guide](learn/index.md) on this. In short, using DSPy is an iterative process. You first define your task and the metrics you want to maximize, and prepare a few example inputs — typically without labels (or only with labels for the final outputs, if your metric requires them). Then, you build your pipeline by selecting built-in layers (`modules`) to use, giving each layer a `signature` (input/output spec), and then calling your modules freely in your Python code. Lastly, you use a DSPy `optimizer` to compile your code into high-quality instructions, automatic few-shot examples, or updated LM weights for your LM.
23 | 
24 | **How do I convert my complex prompt into a DSPy pipeline?** See the same answer above.
25 | 
26 | **What do DSPy optimizers tune?** Or, _what does compiling actually do?_ Each optimizer is different, but they all seek to maximize a metric on your program by updating prompts or LM weights. Current DSPy `optimizers` can inspect your data, simulate traces through your program to generate good/bad examples of each step, propose or refine instructions for each step based on past results, finetune the weights of your LM on self-generated examples, or combine several of these to improve quality or cut cost. We'd love to merge new optimizers that explore a richer space: most manual steps you currently go through for prompt engineering, "synthetic data" generation, or self-improvement can probably generalized into a DSPy optimizer that acts on arbitrary LM programs.
27 | 
28 | Other FAQs. We welcome PRs to add formal answers to each of these here. You will find the answer in existing issues, tutorials, or the papers for all or most of these.
29 | 
30 | - **How do I get multiple outputs?**
31 | 
32 | You can specify multiple output fields. For the short-form signature, you can list multiple outputs as comma separated values, following the "->" indicator (e.g. "inputs -> output1, output2"). For the long-form signature, you can include multiple `dspy.OutputField`s.
33 | 
34 | 
35 | - **How do I define my own metrics? Can metrics return a float?**
36 | 
37 | You can define metrics as simply Python functions that process model generations and evaluate them based on user-defined requirements. Metrics can compare existent data (e.g. gold labels) to model predictions or they can be used to assess various components of an output using validation feedback from LMs (e.g. LLMs-as-Judges). Metrics can return `bool`, `int`, and `float` types scores. Check out the official [Metrics docs](learn/evaluation/metrics.md) to learn more about defining custom metrics and advanced evaluations using AI feedback and/or DSPy programs.
38 | 
39 | - **How expensive or slow is compiling??**
40 | 
41 | To reflect compiling metrics, we highlight an experiment for reference, compiling a program using the [BootstrapFewShotWithRandomSearch](api/optimizers/BootstrapFewShotWithRandomSearch.md) optimizer on the `gpt-3.5-turbo-1106` model over 7 candidate programs and 10 threads. We report that compiling this program takes around 6 minutes with 3200 API calls, 2.7 million input tokens and 156,000 output tokens, reporting a total cost of $3 USD (at the current pricing of the OpenAI model).
42 | 
43 | Compiling DSPy `optimizers` naturally will incur additional LM calls, but we substantiate this overhead with minimalistic executions with the goal of maximizing performance. This invites avenues to enhance performance of smaller models by compiling DSPy programs with larger models to learn enhanced behavior during compile-time and propagate such behavior to the tested smaller model during inference-time.  
44 | 
45 | 
46 | ## Deployment or Reproducibility Concerns
47 | 
48 | - **How do I save a checkpoint of my compiled program?**
49 | 
50 | Here is an example of saving/loading a compiled module:
51 | 
52 | ```python
53 | cot_compiled = teleprompter.compile(CoT(), trainset=trainset, valset=devset)
54 | 
55 | #Saving
56 | cot_compiled.save('compiled_cot_gsm8k.json')
57 | 
58 | #Loading:
59 | cot = CoT()
60 | cot.load('compiled_cot_gsm8k.json')
61 | ```
62 | 
63 | - **How do I export for deployment?**
64 | 
65 | Exporting DSPy programs is simply saving them as highlighted above!
66 | 
67 | - **How do I search my own data?**
68 | 
69 | Open source libraries such as [RAGautouille](https://github.com/bclavie/ragatouille) enable you to search for your own data through advanced retrieval models like ColBERT with tools to embed and index documents. Feel free to integrate such libraries to create searchable datasets while developing your DSPy programs!
70 | 
71 | - **How do I turn off the cache? How do I export the cache?**
72 | 
73 | From v2.5, you can turn off the cache by setting `cache` parameter in `dspy.LM` to `False`:
74 | 
75 | ```python
76 | dspy.LM('openai/gpt-4o-mini',  cache=False)
77 | ```
78 | 
79 | Your local cache will be saved to the global env directory `os.environ["DSP_CACHEDIR"]` or for notebooks `os.environ["DSP_NOTEBOOK_CACHEDIR"]`. You can usually set the cachedir to `os.path.join(repo_path, 'cache')` and export this cache from here:
80 | ```python
81 | os.environ["DSP_NOTEBOOK_CACHEDIR"] = os.path.join(os.getcwd(), 'cache')
82 | ```
83 | 
84 | !!! warning "Important"
85 |     `DSP_CACHEDIR` is responsible for old clients (including dspy.OpenAI, dspy.ColBERTv2, etc.) and `DSPY_CACHEDIR` is responsible for the new dspy.LM client.
86 | 
87 |     In the AWS lambda deployment, you should disable both DSP_\* and DSPY_\*.
88 | 
89 | 
90 | ## Advanced Usage
91 | 
92 | - **How do I parallelize?**
93 | You can parallelize DSPy programs during both compilation and evaluation by specifying multiple thread settings in the respective DSPy `optimizers` or within the `dspy.Evaluate` utility function.
94 | 
95 | - **How do freeze a module?**
96 | 
97 | Modules can be frozen by setting their `._compiled` attribute to be True, indicating the module has gone through optimizer compilation and should not have its parameters adjusted. This is handled internally in optimizers such as `dspy.BootstrapFewShot` where the student program is ensured to be frozen before the teacher propagates the gathered few-shot demonstrations in the bootstrapping process. 
98 | 
99 | - **How do I use DSPy assertions?**
100 | 
101 |     a) **How to Add Assertions to Your Program**:
102 |     - **Define Constraints**: Use `dspy.Assert` and/or `dspy.Suggest` to define constraints within your DSPy program. These are based on boolean validation checks for the outcomes you want to enforce, which can simply be Python functions to validate the model outputs.
103 |     - **Integrating Assertions**: Keep your Assertion statements following a model generations (hint: following a module layer)
104 | 
105 |     b) **How to Activate the Assertions**:
106 |     1. **Using `assert_transform_module`**:
107 |         - Wrap your DSPy module with assertions using the `assert_transform_module` function, along with a `backtrack_handler`. This function transforms your program to include internal assertions backtracking and retry logic, which can be customized as well:
108 |         `program_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler)`
109 |     2. **Activate Assertions**:
110 |         - Directly call `activate_assertions` on your DSPy program with assertions: `program_with_assertions = ProgramWithAssertions().activate_assertions()`
111 | 
112 |     **Note**: To use Assertions properly, you must **activate** a DSPy program that includes `dspy.Assert` or `dspy.Suggest` statements from either of the methods above. 
113 | 
114 | ## Errors
115 | 
116 | - **How do I deal with "context too long" errors?**
117 | 
118 | If you're dealing with "context too long" errors in DSPy, you're likely using DSPy optimizers to include demonstrations within your prompt, and this is exceeding your current context window. Try reducing these parameters (e.g. `max_bootstrapped_demos` and `max_labeled_demos`). Additionally, you can also reduce the number of retrieved passages/docs/embeddings to ensure your prompt is fitting within your model context length.
119 | 
120 | A more general fix is simply increasing the number of `max_tokens` specified to the LM request (e.g. `lm = dspy.OpenAI(model = ..., max_tokens = ...`).
121 | 
122 | ## Set Verbose Level
123 | DSPy utilizes the [logging library](https://docs.python.org/3/library/logging.html) to print logs. If you want to debug your DSPy code, set the logging level to DEBUG with the example code below.
124 | 
125 | ```python
126 | import logging
127 | logging.getLogger("dspy").setLevel(logging.DEBUG)
128 | ```
129 | 
130 | Alternatively, if you want to reduce the amount of logs, set the logging level to WARNING or ERROR.
131 | 
132 | ```python
133 | import logging
134 | logging.getLogger("dspy").setLevel(logging.WARNING)
135 | ```
```

index.md
```
1 | ---
2 | sidebar_position: 1
3 | hide:
4 |   - navigation
5 |   - toc
6 | 
7 | ---
8 | 
9 | ![DSPy](static/img/dspy_logo.png){ width="200", align=left }
10 | 
11 | # _Programming_—not prompting—_LMs_
12 | 
13 | [![PyPI Downloads](https://static.pepy.tech/badge/dspy/month)](https://pepy.tech/projects/dspy)
14 | 
15 | DSPy is a declarative framework for building modular AI software. It allows you to **iterate fast on structured code**, rather than brittle strings, and offers algorithms that **compile AI programs into effective prompts and weights** for your language models, whether you're building simple classifiers, sophisticated RAG pipelines, or Agent loops.
16 | 
17 | Instead of wrangling prompts or training jobs, DSPy (Declarative Self-improving Python) enables you to **build AI software from natural-language modules** and to _generically compose them_ with different models, inference strategies, or learning algorithms. This makes AI software **more reliable, maintainable, and portable** across models and strategies.
18 | 
19 | *tl;dr* Think of DSPy as a higher-level language for AI programming ([lecture](https://www.youtube.com/watch?v=JEMYuzrKLUw)), like the shift from assembly to C or pointer arithmetic to SQL. Meet the community, seek help, or start contributing via [GitHub](https://github.com/stanfordnlp/dspy) and [Discord](https://discord.gg/XCGy2WDCQB).
20 | 
21 | <!-- Its abstractions make your AI software more reliable and maintainable, and allow it to become more portable as new models and learning techniques emerge. It's also just rather elegant! -->
22 | 
23 | !!! info "Getting Started I: Install DSPy and set up your LM"
24 | 
25 |     ```bash
26 |     > pip install -U dspy
27 |     ```
28 | 
29 |     === "OpenAI"
30 |         You can authenticate by setting the `OPENAI_API_KEY` env variable or passing `api_key` below.
31 | 
32 |         ```python linenums="1"
33 |         import dspy
34 |         lm = dspy.LM("openai/gpt-4o-mini", api_key="YOUR_OPENAI_API_KEY")
35 |         dspy.configure(lm=lm)
36 |         ```
37 | 
38 |     === "Anthropic"
39 |         You can authenticate by setting the `ANTHROPIC_API_KEY` env variable or passing `api_key` below.
40 | 
41 |         ```python linenums="1"
42 |         import dspy
43 |         lm = dspy.LM("anthropic/claude-3-opus-20240229", api_key="YOUR_ANTHROPIC_API_KEY")
44 |         dspy.configure(lm=lm)
45 |         ```
46 | 
47 |     === "Databricks"
48 |         If you're on the Databricks platform, authentication is automatic via their SDK. If not, you can set the env variables `DATABRICKS_API_KEY` and `DATABRICKS_API_BASE`, or pass `api_key` and `api_base` below.
49 | 
50 |         ```python linenums="1"
51 |         import dspy
52 |         lm = dspy.LM(
53 |             "databricks/databricks-llama-4-maverick",
54 |             api_key="YOUR_DATABRICKS_ACCESS_TOKEN",
55 |             api_base="YOUR_DATABRICKS_WORKSPACE_URL",  # e.g.: https://dbc-64bf4923-e39e.cloud.databricks.com/serving-endpoints
56 |         )
57 |         dspy.configure(lm=lm)
58 |         ```
59 | 
60 |     === "Gemini"
61 |         You can authenticate by setting the `GEMINI_API_KEY` env variable or passing `api_key` below.
62 | 
63 |         ```python linenums="1"
64 |         import dspy
65 |         lm = dspy.LM("gemini/gemini-2.5-flash", api_key="YOUR_GEMINI_API_KEY")
66 |         dspy.configure(lm=lm)
67 |         ```
68 | 
69 |     === "Local LMs on your laptop"
70 |           First, install [Ollama](https://github.com/ollama/ollama) and launch its server with your LM.
71 | 
72 |           ```bash
73 |           > curl -fsSL https://ollama.ai/install.sh | sh
74 |           > ollama run llama3.2:1b
75 |           ```
76 | 
77 |           Then, connect to it from your DSPy code.
78 | 
79 |         ```python linenums="1"
80 |         import dspy
81 |         lm = dspy.LM("ollama_chat/llama3.2:1b", api_base="http://localhost:11434", api_key="")
82 |         dspy.configure(lm=lm)
83 |         ```
84 | 
85 |     === "Local LMs on a GPU server"
86 |           First, install [SGLang](https://docs.sglang.ai/get_started/install.html) and launch its server with your LM.
87 | 
88 |           ```bash
89 |           > pip install "sglang[all]"
90 |           > pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/ 
91 | 
92 |           > CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server --port 7501 --model-path meta-llama/Llama-3.1-8B-Instruct
93 |           ```
94 |         
95 |         If you don't have access from Meta to download `meta-llama/Llama-3.1-8B-Instruct`, use `Qwen/Qwen2.5-7B-Instruct` for example.
96 | 
97 |         Next, connect to your local LM from your DSPy code as an `OpenAI`-compatible endpoint.
98 | 
99 |           ```python linenums="1"
100 |           lm = dspy.LM("openai/meta-llama/Llama-3.1-8B-Instruct",
101 |                        api_base="http://localhost:7501/v1",  # ensure this points to your port
102 |                        api_key="local", model_type="chat")
103 |           dspy.configure(lm=lm)
104 |           ```
105 | 
106 |     === "Other providers"
107 |         In DSPy, you can use any of the dozens of [LLM providers supported by LiteLLM](https://docs.litellm.ai/docs/providers). Simply follow their instructions for which `{PROVIDER}_API_KEY` to set and how to write pass the `{provider_name}/{model_name}` to the constructor.
108 | 
109 |         Some examples:
110 | 
111 |         - `anyscale/mistralai/Mistral-7B-Instruct-v0.1`, with `ANYSCALE_API_KEY`
112 |         - `together_ai/togethercomputer/llama-2-70b-chat`, with `TOGETHERAI_API_KEY`
113 |         - `sagemaker/<your-endpoint-name>`, with `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_REGION_NAME`
114 |         - `azure/<your_deployment_name>`, with `AZURE_API_KEY`, `AZURE_API_BASE`, `AZURE_API_VERSION`, and the optional `AZURE_AD_TOKEN` and `AZURE_API_TYPE`
115 | 
116 |         
117 |         If your provider offers an OpenAI-compatible endpoint, just add an `openai/` prefix to your full model name.
118 | 
119 |         ```python linenums="1"
120 |         import dspy
121 |         lm = dspy.LM("openai/your-model-name", api_key="PROVIDER_API_KEY", api_base="YOUR_PROVIDER_URL")
122 |         dspy.configure(lm=lm)
123 |         ```
124 | 
125 | ??? "Calling the LM directly."
126 | 
127 |      Idiomatic DSPy involves using _modules_, which we define in the rest of this page. However, it's still easy to call the `lm` you configured above directly. This gives you a unified API and lets you benefit from utilities like automatic caching.
128 | 
129 |      ```python linenums="1"       
130 |      lm("Say this is a test!", temperature=0.7)  # => ['This is a test!']
131 |      lm(messages=[{"role": "user", "content": "Say this is a test!"}])  # => ['This is a test!']
132 |      ``` 
133 | 
134 | 
135 | ## 1) **Modules** help you describe AI behavior as _code_, not strings.
136 | 
137 | To build reliable AI systems, you must iterate fast. But maintaining prompts makes that hard: it forces you to tinker with strings or data _every time you change your LM, metrics, or pipeline_. Having built over a dozen best-in-class compound LM systems since 2020, we learned this the hard way—and so built DSPy to decouple AI system design from messy incidental choices about specific LMs or prompting strategies.
138 | 
139 | DSPy shifts your focus from tinkering with prompt strings to **programming with structured and declarative natural-language modules**. For every AI component in your system, you specify input/output behavior as a _signature_ and select a _module_ to assign a strategy for invoking your LM. DSPy expands your signatures into prompts and parses your typed outputs, so you can compose different modules together into ergonomic, portable, and optimizable AI systems.
140 | 
141 | 
142 | !!! info "Getting Started II: Build DSPy modules for various tasks"
143 |     Try the examples below after configuring your `lm` above. Adjust the fields to explore what tasks your LM can do well out of the box. Each tab below sets up a DSPy module, like `dspy.Predict`, `dspy.ChainOfThought`, or `dspy.ReAct`, with a task-specific _signature_. For example, `question -> answer: float` tells the module to take a question and to produce a `float` answer.
144 | 
145 |     === "Math"
146 | 
147 |         ```python linenums="1"
148 |         math = dspy.ChainOfThought("question -> answer: float")
149 |         math(question="Two dice are tossed. What is the probability that the sum equals two?")
150 |         ```
151 |         
152 |         **Possible Output:**
153 |         ```text
154 |         Prediction(
155 |             reasoning='When two dice are tossed, each die has 6 faces, resulting in a total of 6 x 6 = 36 possible outcomes. The sum of the numbers on the two dice equals two only when both dice show a 1. This is just one specific outcome: (1, 1). Therefore, there is only 1 favorable outcome. The probability of the sum being two is the number of favorable outcomes divided by the total number of possible outcomes, which is 1/36.',
156 |             answer=0.0277776
157 |         )
158 |         ```
159 | 
160 |     === "RAG"
161 | 
162 |         ```python linenums="1"       
163 |         def search_wikipedia(query: str) -> list[str]:
164 |             results = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")(query, k=3)
165 |             return [x["text"] for x in results]
166 |         
167 |         rag = dspy.ChainOfThought("context, question -> response")
168 | 
169 |         question = "What's the name of the castle that David Gregory inherited?"
170 |         rag(context=search_wikipedia(question), question=question)
171 |         ```
172 |         
173 |         **Possible Output:**
174 |         ```text
175 |         Prediction(
176 |             reasoning='The context provides information about David Gregory, a Scottish physician and inventor. It specifically mentions that he inherited Kinnairdy Castle in 1664. This detail directly answers the question about the name of the castle that David Gregory inherited.',
177 |             response='Kinnairdy Castle'
178 |         )
179 |         ```
180 | 
181 |     === "Classification"
182 | 
183 |         ```python linenums="1"
184 |         from typing import Literal
185 | 
186 |         class Classify(dspy.Signature):
187 |             """Classify sentiment of a given sentence."""
188 |             
189 |             sentence: str = dspy.InputField()
190 |             sentiment: Literal["positive", "negative", "neutral"] = dspy.OutputField()
191 |             confidence: float = dspy.OutputField()
192 | 
193 |         classify = dspy.Predict(Classify)
194 |         classify(sentence="This book was super fun to read, though not the last chapter.")
195 |         ```
196 |         
197 |         **Possible Output:**
198 | 
199 |         ```text
200 |         Prediction(
201 |             sentiment='positive',
202 |             confidence=0.75
203 |         )
204 |         ```
205 | 
206 |     === "Information Extraction"
207 | 
208 |         ```python linenums="1"        
209 |         class ExtractInfo(dspy.Signature):
210 |             """Extract structured information from text."""
211 |             
212 |             text: str = dspy.InputField()
213 |             title: str = dspy.OutputField()
214 |             headings: list[str] = dspy.OutputField()
215 |             entities: list[dict[str, str]] = dspy.OutputField(desc="a list of entities and their metadata")
216 |         
217 |         module = dspy.Predict(ExtractInfo)
218 | 
219 |         text = "Apple Inc. announced its latest iPhone 14 today." \
220 |             "The CEO, Tim Cook, highlighted its new features in a press release."
221 |         response = module(text=text)
222 | 
223 |         print(response.title)
224 |         print(response.headings)
225 |         print(response.entities)
226 |         ```
227 |         
228 |         **Possible Output:**
229 |         ```text
230 |         Apple Inc. Announces iPhone 14
231 |         ['Introduction', "CEO's Statement", 'New Features']
232 |         [{'name': 'Apple Inc.', 'type': 'Organization'}, {'name': 'iPhone 14', 'type': 'Product'}, {'name': 'Tim Cook', 'type': 'Person'}]
233 |         ```
234 | 
235 |     === "Agents"
236 | 
237 |         ```python linenums="1"       
238 |         def evaluate_math(expression: str):
239 |             return dspy.PythonInterpreter({}).execute(expression)
240 | 
241 |         def search_wikipedia(query: str):
242 |             results = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")(query, k=3)
243 |             return [x["text"] for x in results]
244 | 
245 |         react = dspy.ReAct("question -> answer: float", tools=[evaluate_math, search_wikipedia])
246 | 
247 |         pred = react(question="What is 9362158 divided by the year of birth of David Gregory of Kinnairdy castle?")
248 |         print(pred.answer)
249 |         ```
250 |         
251 |         **Possible Output:**
252 | 
253 |         ```text
254 |         5761.328
255 |         ```
256 |     
257 |     === "Multi-Stage Pipelines"
258 | 
259 |         ```python linenums="1"       
260 |         class Outline(dspy.Signature):
261 |             """Outline a thorough overview of a topic."""
262 |             
263 |             topic: str = dspy.InputField()
264 |             title: str = dspy.OutputField()
265 |             sections: list[str] = dspy.OutputField()
266 |             section_subheadings: dict[str, list[str]] = dspy.OutputField(desc="mapping from section headings to subheadings")
267 | 
268 |         class DraftSection(dspy.Signature):
269 |             """Draft a top-level section of an article."""
270 |             
271 |             topic: str = dspy.InputField()
272 |             section_heading: str = dspy.InputField()
273 |             section_subheadings: list[str] = dspy.InputField()
274 |             content: str = dspy.OutputField(desc="markdown-formatted section")
275 | 
276 |         class DraftArticle(dspy.Module):
277 |             def __init__(self):
278 |                 self.build_outline = dspy.ChainOfThought(Outline)
279 |                 self.draft_section = dspy.ChainOfThought(DraftSection)
280 | 
281 |             def forward(self, topic):
282 |                 outline = self.build_outline(topic=topic)
283 |                 sections = []
284 |                 for heading, subheadings in outline.section_subheadings.items():
285 |                     section, subheadings = f"## {heading}", [f"### {subheading}" for subheading in subheadings]
286 |                     section = self.draft_section(topic=outline.title, section_heading=section, section_subheadings=subheadings)
287 |                     sections.append(section.content)
288 |                 return dspy.Prediction(title=outline.title, sections=sections)
289 | 
290 |         draft_article = DraftArticle()
291 |         article = draft_article(topic="World Cup 2002")
292 |         ```
293 |         
294 |         **Possible Output:**
295 | 
296 |         A 1500-word article on the topic, e.g.
297 | 
298 |         ```text
299 |         ## Qualification Process
300 | 
301 |         The qualification process for the 2002 FIFA World Cup involved a series of..... [shortened here for presentation].
302 | 
303 |         ### UEFA Qualifiers
304 | 
305 |         The UEFA qualifiers involved 50 teams competing for 13..... [shortened here for presentation].
306 | 
307 |         .... [rest of the article]
308 |         ```
309 | 
310 |         Note that DSPy makes it straightforward to optimize multi-stage modules like this. As long as you can evaluate the _final_ output of the system, every DSPy optimizer can tune all of the intermediate modules.
311 | 
312 | ??? "Using DSPy in practice: from quick scripting to building sophisticated systems."
313 | 
314 |     Standard prompts conflate interface ("what should the LM do?") with implementation ("how do we tell it to do that?"). DSPy isolates the former as _signatures_ so we can infer the latter or learn it from data — in the context of a bigger program.
315 |     
316 |     Even before you start using optimizers, DSPy's modules allow you to script effective LM systems as ergonomic, portable _code_. Across many tasks and LMs, we maintain _signature test suites_ that assess the reliability of the built-in DSPy adapters. Adapters are the components that map signatures to prompts prior to optimization. If you find a task where a simple prompt consistently outperforms idiomatic DSPy for your LM, consider that a bug and [file an issue](https://github.com/stanfordnlp/dspy/issues). We'll use this to improve the built-in adapters.
317 | 
318 | 
319 | ## 2) **Optimizers** tune the prompts and weights of your AI modules.
320 | 
321 | DSPy provides you with the tools to compile high-level code with natural language annotations into the low-level computations, prompts, or weight updates that align your LM with your program's structure and metrics. If you change your code or your metrics, you can simply re-compile accordingly.
322 | 
323 | Given a few tens or hundreds of representative _inputs_ of your task and a _metric_ that can measure the quality of your system's outputs, you can use a DSPy optimizer. Different optimizers in DSPy work by **synthesizing good few-shot examples** for every module, like `dspy.BootstrapRS`,<sup>[1](https://arxiv.org/abs/2310.03714)</sup> **proposing and intelligently exploring better natural-language instructions** for every prompt, like [`dspy.GEPA`](https://dspy.ai/tutorials/gepa_ai_program/)<sup>[2](https://arxiv.org/abs/2507.19457)</sup>, `dspy.MIPROv2`,<sup>[3](https://arxiv.org/abs/2406.11695)</sup> and **building datasets for your modules and using them to finetune the LM weights** in your system, like `dspy.BootstrapFinetune`.<sup>[4](https://arxiv.org/abs/2407.10930)</sup> For detailed tutorials on running `dspy.GEPA`, please take a look at [dspy.GEPA tutorials](https://dspy.ai/tutorials/gepa_ai_program/).
324 | 
325 | 
326 | !!! info "Getting Started III: Optimizing the LM prompts or weights in DSPy programs"
327 |     A typical simple optimization run costs on the order of $2 USD and takes around 20 minutes, but be careful when running optimizers with very large LMs or very large datasets.
328 |     Optimization can cost as little as a few cents or up to tens of dollars, depending on your LM, dataset, and configuration.
329 | 
330 |     Examples below rely on HuggingFace/datasets, you can install it by the command below.
331 | 
332 |     ```bash
333 |     > pip install -U datasets
334 |     ```
335 | 
336 |     === "Optimizing prompts for a ReAct agent"
337 |         This is a minimal but fully runnable example of setting up a `dspy.ReAct` agent that answers questions via
338 |         search from Wikipedia and then optimizing it using `dspy.MIPROv2` in the cheap `light` mode on 500
339 |         question-answer pairs sampled from the `HotPotQA` dataset.
340 | 
341 |         ```python linenums="1"
342 |         import dspy
343 |         from dspy.datasets import HotPotQA
344 | 
345 |         dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
346 | 
347 |         def search_wikipedia(query: str) -> list[str]:
348 |             results = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")(query, k=3)
349 |             return [x["text"] for x in results]
350 | 
351 |         trainset = [x.with_inputs('question') for x in HotPotQA(train_seed=2024, train_size=500).train]
352 |         react = dspy.ReAct("question -> answer", tools=[search_wikipedia])
353 | 
354 |         tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto="light", num_threads=24)
355 |         optimized_react = tp.compile(react, trainset=trainset)
356 |         ```
357 | 
358 |         An informal run like this raises ReAct's score from 24% to 51%, by teaching `gpt-4o-mini` more about the specifics of the task.
359 | 
360 |     === "Optimizing prompts for RAG"
361 |         Given a retrieval index to `search`, your favorite `dspy.LM`, and a small `trainset` of questions and ground-truth responses, the following code snippet can optimize your RAG system with long outputs against the built-in `SemanticF1` metric, which is implemented as a DSPy module.
362 | 
363 |         ```python linenums="1"
364 |         class RAG(dspy.Module):
365 |             def __init__(self, num_docs=5):
366 |                 self.num_docs = num_docs
367 |                 self.respond = dspy.ChainOfThought("context, question -> response")
368 | 
369 |             def forward(self, question):
370 |                 context = search(question, k=self.num_docs)   # defined in tutorial linked below
371 |                 return self.respond(context=context, question=question)
372 | 
373 |         tp = dspy.MIPROv2(metric=dspy.evaluate.SemanticF1(decompositional=True), auto="medium", num_threads=24)
374 |         optimized_rag = tp.compile(RAG(), trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2)
375 |         ```
376 | 
377 |         For a complete RAG example that you can run, start this [tutorial](tutorials/rag/index.ipynb). It improves the quality of a RAG system over a subset of StackExchange communities by 10% relative gain.
378 | 
379 |     === "Optimizing weights for Classification"
380 |         <details><summary>Click to show dataset setup code.</summary>
381 | 
382 |         ```python linenums="1"
383 |         import random
384 |         from typing import Literal
385 | 
386 |         from datasets import load_dataset
387 | 
388 |         import dspy
389 |         from dspy.datasets import DataLoader
390 | 
391 |         # Load the Banking77 dataset.
392 |         CLASSES = load_dataset("PolyAI/banking77", split="train", trust_remote_code=True).features["label"].names
393 |         kwargs = {"fields": ("text", "label"), "input_keys": ("text",), "split": "train", "trust_remote_code": True}
394 | 
395 |         # Load the first 2000 examples from the dataset, and assign a hint to each *training* example.
396 |         trainset = [
397 |             dspy.Example(x, hint=CLASSES[x.label], label=CLASSES[x.label]).with_inputs("text", "hint")
398 |             for x in DataLoader().from_huggingface(dataset_name="PolyAI/banking77", **kwargs)[:2000]
399 |         ]
400 |         random.Random(0).shuffle(trainset)
401 |         ```
402 |         </details>
403 | 
404 |         ```python linenums="1"
405 |         import dspy
406 |         lm=dspy.LM('openai/gpt-4o-mini-2024-07-18')
407 | 
408 |         # Define the DSPy module for classification. It will use the hint at training time, if available.
409 |         signature = dspy.Signature("text, hint -> label").with_updated_fields("label", type_=Literal[tuple(CLASSES)])
410 |         classify = dspy.ChainOfThought(signature)
411 |         classify.set_lm(lm)
412 | 
413 |         # Optimize via BootstrapFinetune.
414 |         optimizer = dspy.BootstrapFinetune(metric=(lambda x, y, trace=None: x.label == y.label), num_threads=24)
415 |         optimized = optimizer.compile(classify, trainset=trainset)
416 | 
417 |         optimized(text="What does a pending cash withdrawal mean?")
418 |         
419 |         # For a complete fine-tuning tutorial, see: https://dspy.ai/tutorials/classification_finetuning/
420 |         ```
421 | 
422 |         **Possible Output (from the last line):**
423 |         ```text
424 |         Prediction(
425 |             reasoning='A pending cash withdrawal indicates that a request to withdraw cash has been initiated but has not yet been completed or processed. This status means that the transaction is still in progress and the funds have not yet been deducted from the account or made available to the user.',
426 |             label='pending_cash_withdrawal'
427 |         )
428 |         ```
429 | 
430 |         An informal run similar to this on DSPy 2.5.29 raises GPT-4o-mini's score 66% to 87%.
431 | 
432 | 
433 | ??? "What's an example of a DSPy optimizer? How do different optimizers work?"
434 | 
435 |     Take the `dspy.MIPROv2` optimizer as an example. First, MIPRO starts with the **bootstrapping stage**. It takes your program, which may be unoptimized at this point, and runs it many times across different inputs to collect traces of input/output behavior for each one of your modules. It filters these traces to keep only those that appear in trajectories scored highly by your metric. Second, MIPRO enters its **grounded proposal stage**. It previews your DSPy program's code, your data, and traces from running your program, and uses them to draft many potential instructions for every prompt in your program. Third, MIPRO launches the **discrete search stage**. It samples mini-batches from your training set, proposes a combination of instructions and traces to use for constructing every prompt in the pipeline, and evaluates the candidate program on the mini-batch. Using the resulting score, MIPRO updates a surrogate model that helps the proposals get better over time.
436 | 
437 |     One thing that makes DSPy optimizers so powerful is that they can be composed. You can run `dspy.MIPROv2` and use the produced program as an input to `dspy.MIPROv2` again or, say, to `dspy.BootstrapFinetune` to get better results. This is partly the essence of `dspy.BetterTogether`. Alternatively, you can run the optimizer and then extract the top-5 candidate programs and build a `dspy.Ensemble` of them. This allows you to scale _inference-time compute_ (e.g., ensembles) as well as DSPy's unique _pre-inference time compute_ (i.e., optimization budget) in highly systematic ways.
438 | 
439 | 
440 | 
441 | <!-- Future:
442 | BootstrapRS or MIPRO on ??? with a local SGLang LM
443 | BootstrapFS on MATH with a tiny LM like Llama-3.2 with Ollama (maybe with a big teacher) -->
444 | 
445 | 
446 | 
447 | ## 3) **DSPy's Ecosystem** advances open-source AI research.
448 | 
449 | Compared to monolithic LMs, DSPy's modular paradigm enables a large community to improve the compositional architectures, inference-time strategies, and optimizers for LM programs in an open, distributed way. This gives DSPy users more control, helps them iterate much faster, and allows their programs to get better over time by applying the latest optimizers or modules.
450 | 
451 | The DSPy research effort started at Stanford NLP in Feb 2022, building on what we had learned from developing early [compound LM systems](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/) like [ColBERT-QA](https://arxiv.org/abs/2007.00814), [Baleen](https://arxiv.org/abs/2101.00436), and [Hindsight](https://arxiv.org/abs/2110.07752). The first version was released as [DSP](https://arxiv.org/abs/2212.14024) in Dec 2022 and evolved by Oct 2023 into [DSPy](https://arxiv.org/abs/2310.03714). Thanks to [250 contributors](https://github.com/stanfordnlp/dspy/graphs/contributors), DSPy has introduced tens of thousands of people to building and optimizing modular LM programs.
452 | 
453 | Since then, DSPy's community has produced a large body of work on optimizers, like [MIPROv2](https://arxiv.org/abs/2406.11695), [BetterTogether](https://arxiv.org/abs/2407.10930), and [LeReT](https://arxiv.org/abs/2410.23214), on program architectures, like [STORM](https://arxiv.org/abs/2402.14207), [IReRa](https://arxiv.org/abs/2401.12178), and [DSPy Assertions](https://arxiv.org/abs/2312.13382), and on successful applications to new problems, like [PAPILLON](https://arxiv.org/abs/2410.17127), [PATH](https://arxiv.org/abs/2406.11706), [WangLab@MEDIQA](https://arxiv.org/abs/2404.14544), [UMD's Prompting Case Study](https://arxiv.org/abs/2406.06608), and [Haize's Red-Teaming Program](https://blog.haizelabs.com/posts/dspy/), in addition to many open-source projects, production applications, and other [use cases](community/use-cases.md).
```

roadmap.md
```
1 | ---
2 | draft: true
3 | ---
4 | 
5 | !!! warning "This document is from Aug 2024. Since then, DSPy 2.5 and 2.6 were released, DSPy has grown considerably, and 3.0 is approaching! Content below is highly outdated."
6 | 
7 | 
8 | 
9 | # Roadmap Sketch: DSPy 2.5+
10 | 
11 | It’s been a year since DSPy evolved out of Demonstrate–Search–Predict (DSP), whose research started at Stanford NLP all the way back in February 2022. Thanks to 200 wonderful contributors, DSPy has introduced tens of thousands of people to building modular LM programs and optimizing their prompts and weights automatically. In this time, DSPy has grown to 160,000 monthly downloads and 16,000 stars on GitHub, becoming synonymous with prompt optimization in many circles and inspiring at least a half-dozen cool new libraries.
12 | 
13 | This document is an initial sketch of DSPy’s public roadmap for the next few weeks and months, as we work on DSPy 2.5 and plan for DSPy 3.0. Suggestions and open-source contributors are more than welcome: just open an issue or submit a pull request regarding the roadmap.
14 | 
15 | 
16 | 
17 | ## Technical Objectives
18 | 
19 | The thesis of DSPy is that for LMs to be useful, we have to shift from ad-hoc prompting to new notions of programming LMs. Instead of relying on LMs gaining much more general or more compositional capabilities, we need to enable developers to iteratively explore their problems and build modular software that invokes LMs for well-scoped tasks. We need to enable that through modules and optimizers that isolate how they decompose their problems and describe their system's objectives from how their LMs are invoked or fine-tuned to maximize their objectives. DSPy's goal has been to develop (and to build the community and shared infrastructure for the collective development of) the abstractions, programming patterns, and optimizers toward this thesis.
20 | 
21 | To a first approximation, DSPy’s current user-facing language has the minimum number of appropriate abstractions that address the goals above: declarative signatures, define-by-run modules, and optimizers that can be composed quite powerfully. But there are several things we need to do better to realize our goals. The upcoming DSPy releases will have the following objectives.
22 | 
23 | 1. Polishing the core functionality.
24 | 2. Developing more accurate, lower-cost optimizers.
25 | 3. Building end-to-end tutorials from DSPy’s ML workflow to deployment.
26 | 4. Shifting towards more interactive optimization & tracking.
27 | 
28 | 
29 | 
30 | ## Team & Organization
31 | 
32 | DSPy is fairly unusual in its technical objectives, contributors, and audience. Though DSPy takes inspiration from PyTorch, a library for building and optimizing DNNs, there is one major difference: PyTorch was introduced well after DNNs were mature ML concepts, but DSPy seeks to establish and advance core LM Programs research: the framework is propelled by constant academic research from programming abstractions (like the original **Demonstrate–Search–Predict** concepts, DSPy **Signatures**, or **LM Assertions**) to NLP systems (like **STORM**, **PATH**, and **IReRa**) to prompt optimizers (like **MIPRO**) and RL (like **BetterTogether**), among many other related directions.
33 | 
34 | This research all composes into a concrete, practical library, thanks to dozens of industry contributors, many of whom are deploying apps in production using DSPy. Because of this, DSPy reaches not only of grad students and ML engineers, but also many non-ML engineers, from early adopter SWEs to hobbyists exploring new ways of using LMs. The following team, with help from many folks in the OSS community, is working towards the objectives in this Roadmap.
35 | 
36 | **Project Lead:** Omar Khattab (Stanford & Databricks)
37 | 
38 | **Project Mentors:** Chris Potts (Stanford), Matei Zaharia (UC Berkeley & Databricks), Heather Miller (CMU & Two Sigma)
39 | 
40 | **Core Library:** Arnav Singhvi (Databricks & Stanford), Herumb Shandilya (Stanford), Hanna Moazam (Databricks), Sri Vardhamanan (Dashworks), Cyrus Nouroozi (Zenbase), Amir Mehr (Zenbase), Kyle Caverly (Modular), with special thanks to Keshav Santhanam (Stanford), Thomas Ahle (Normal Computing), Connor Shorten (Weaviate)
41 | 
42 | **Prompt Optimization:** Krista Opsahl-Ong (Stanford), Michael Ryan (Stanford), Josh Purtell (Basis), with special thanks to Eric Zhang (Stanford)
43 | 
44 | **Finetuning & RL:** Dilara Soylu (Stanford), Isaac Miller (Anyscale), Karel D'Oosterlinck (Ghent), with special thanks to Paridhi Masehswari (Stanford)
45 | 
46 | **PL Abstractions:** Shangyin Tan (UC Berkeley), Manish Shetty (UC Berkeley), Peter Zhong (CMU)
47 | 
48 | **Applications:** Jasper Xian (Waterloo), Saron Samuel (Stanford), Alberto Mancarella (Stanford), Faraz Khoubsirat (Waterloo), Saiful Haq (IIT-B), Ashutosh Sharma (UIUC)
49 | 
50 | 
51 | 
52 | ## 1) Polishing the core functionality.
53 | 
54 | Over the next month, polishing is the main objective and likely the one to have the highest ROI on the experience of the average user. Conceptually, DSPy has an extremely small core. It’s nothing but (1) LMs, (2) Signatures & Modules, (3) Optimizers, and (4) Assertions. These concepts and their implementations evolved organically over the past couple of years. We are working now to consolidate what we’ve learned and refactor internally so that things “just work” out of the box for new users, who may not know all the tips-and-tricks just yet.
55 | 
56 | More concretely:
57 | 
58 | 1. We want to increase the quality of zero-shot, off-the-shelf DSPy programs, i.e. those not yet compiled on custom data.
59 | 2. Wherever possible, DSPy should delegate lower-level internal complexity (like managing LMs and structured generation) to emerging lower-level libraries. When required, we may fork smaller libraries out of DSPy to support infrastructure pieces as their own projects.
60 | 3. DSPy should internally be more modular and we need higher compatibility between internal components. Specifically, we need more deeper and more native investment in (i) typed multi-field constraints, (ii) assertions, (iii) observability and experimental tracking, (iv) deployment of artifacts and related concerns like streaming and async, and (v) fine-tuning and serving open models.
61 | 
62 | 
63 | ### On LMs
64 | 
65 | As of DSPy 2.4, the library has approximately 20,000 lines of code and roughly another 10,000 lines of code for tests, examples, and documentation. Some of these are clearly necessary (e.g., DSPy optimizers) but others exist only because the LM space lacks the building blocks we need under the hood. Luckily, for LM interfaces, a very strong library now exists: LiteLLM, a library that unifies interfaces to various LM and embedding providers. We expect to reduce around 6000 LoCs of support for custom LMs and retrieval models by shifting a lot of that to LiteLLM.
66 | 
67 | Objectives in this space include improved caching, saving/loading of LMs, support for streaming and async LM requests. Work here is currently led by Hanna Moazam and Sri Vardhamanan, building on a foundation by Cyrus Nouroozi, Amir Mehr, Kyle Caverly, and others.
68 | 
69 | 
70 | ### On Signatures & Modules
71 | 
72 | Traditionally, LMs offer text-in-text-out interfaces. Toward modular programming, DSPy introduced signatures for the first time (as DSP Templates in Jan 2023) as a way to structure the inputs and outputs of LM interactions. Standard prompts conflate interface (“what should the LM do?”) with implementation (“how do we tell it to do that?”). DSPy signatures isolate the former so we can infer and learn the latter from data — in the context of a bigger program. Today in the LM landscape, notions of "structured outputs" have evolved dramatically, thanks to constrained decoding and other improvements, and have become mainstream. What may be called "structured inputs" remains is yet to become mainstream outside of DSPy, but is as crucial.
73 | 
74 | Objectives in this space include refining the abstractions and implementations first-class notion of LM Adapters in DSPy, as translators that sits between signatures and LM interfaces. While Optimizers adjust prompts through interactions with a user-supplied metric and data, Adapters are more concerned with building up interactions with LMs to account for, e.g. (i) non-plaintext LM interfaces like chat APIs, structured outputs, function calling, and multi-modal APIs, (ii) languages beyond English or other forms of higher-level specialization. This has been explored in DSPy on and off in various forms, but we have started working on more fundamental approaches to this problem that will offer tangible improvements to most use-cases. Work here is currently led by Omar Khattab.
75 | 
76 | 
77 | ### On Finetuning & Serving
78 | 
79 | In February 2023, DSPy introduced the notion of compiling to optimize the weights of an LM program. (To understand just how long ago that was in AI terms, this was before the Alpaca training project at Stanford had even started and a month before the first GPT-4 was released.) Since then, we have shown in October 2023 and, much more expansively, in July 2024, that the fine-tuning flavor of DSPy can deliver large gains for small LMs, especially when composed with prompt optimization.
80 | 
81 | Overall, though, most DSPy users in practice explore prompt optimization and not weight optimization and most of our examples do the same. The primary reason for a lot of this is infrastructure. Fine-tuning in the DSPy flavor is more than just training a model: ultimately, we need to bootstrap training data for several different modules in a program, train multiple models and handle model selection, and then load and plug in those models into the program's modules. Doing this robustly at the level of abstraction DSPy offers requires a level of resource management that is not generally supported by external existing tools. Major efforts in this regard are currently led by Dilara Soylu and Isaac Miller.
82 | 
83 | 
84 | ### On Optimizers & Assertions
85 | 
86 | This is a naturally major direction in the course of polishing. We will share more thoughts here after making more progress on the three angles above.
87 | 
88 | 
89 | 
90 | ## 2) Developing more accurate, lower-cost optimizers.
91 | 
92 | A very large fraction of the research in DSPy focuses on optimizing the prompts and the weights of LM programs. In December 2022, we introduced the algorithm and abstractions behind BootstrapFewShot (as Demonstrate in DSP) and several of its variants. In February 2023, we introduced the core version of what later became BootstrapFinetune. In August 2023, we introduced new variations of both of these. In December 2023, we introduced the first couple of instruction optimizers into DSPy, CA-OPRO and early versions of MIPRO. These were again upgraded in March 2024. Fast forward to June and July 2024, we released MIPROv2 for prompt optimization and BetterTogether for fine-tuning the weights of LM programs.
93 | 
94 | We have been working towards a number of stronger optimizers. While we cannot share the internal details of research on new optimizers yet, we can outline the goals. A DSPy optimizer can be characterized via three angles:
95 | 
96 | 1. Quality: How much quality can it deliver from various LMs? How effective does it need the zero-shot program to be in order to work well?
97 | 2. Cost: How many labeled (and unlabeled) inputs does it need? How many invocations of the program does it need? How expensive is the resulting optimized program at inference time?
98 | 3. Robustness: How well can it generalize to different unseen data points or distributions? How sensitive is it to mistakes of the metric or labels?
99 | 
100 | Over the next six months, our goal is to dramatically improve each angle of these _when the other two are held constant_.  Concretely, there are three directions here.
101 | 
102 | - Benchmarking: A key prerequisite here is work on benchmarking. On the team, Michael Ryan and Shangyin Tan are leading these efforts. More soon.
103 | 
104 | - Quality: The goal here is optimizers that extract, on average, 20% more on representative tasks than MIPROv2 and BetterTogether, under the usual conditions — like a few hundred inputs with labels and a good metric starting from a decent zero-shot program. Various efforts here are led by Dilara Soylu, Michael Ryan, Josh Purtell, Krista Opsahl-Ong, and Isaac Miller.
105 | 
106 | - Efficiency: The goal here is optimizers that match the current best scores from MIPROv2 and BetterTogether but under 1-2 challenges like: (i) starting from only 10-20 inputs with labels, (ii) starting with a weak zero-shot program that scores 0%, (iii) where significant misalignment exists between train/validation and test, or (iii) where the user supplies no metric but provides a very small number of output judgments.
107 | 
108 | 
109 | 
110 | ## 3) Building end-to-end tutorials from DSPy’s ML workflow to deployment.
111 | 
112 | Using DSPy well for solving a new task is just doing good machine learning with LMs, but teaching this is hard. On the one hand, it's an iterative process: you make some initial choices, which will be sub-optimal, and then you refine them incrementally. It's highly exploratory: it's often the case that no one knows yet how to best solve a problem in a DSPy-esque way. One the other hand, DSPy offers many emerging lessons from several years of building LM systems, in which the design space, the data regime, and many other factors are new both to ML experts and to the very large fraction of users that have no ML experience.
113 | 
114 | Though current docs do address [a bunch of this](learn/index.md) in isolated ways, one thing we've learned is that we should separate teaching the core DSPy language (which is ultimately pretty small) from teaching the emerging ML workflow that works well in a DSPy-esque setting. As a natural extension of this, we need to place more emphasis on steps prior and after to the explicit coding in DSPy, from data collection to deployment that serves and monitors the optimized DSPy program in practice. This is just starting but efforts will be ramping up led by Omar Khattab, Isaac Miller, and Herumb Shandilya.
115 | 
116 | 
117 | ## 4) Shifting towards more interactive optimization & tracking.
118 | 
119 | Right now, a DSPy user has a few ways to observe and tweak the process of optimization. They can study the prompts before, during, and after optimization methods like `inspect_history`, built-in logging, and/or the metadata returned by optimizers. Similarly, they can rely on `program.save` and `program.load` to potentially adjust the optimized prompts by hand. Alternatively, they can use one of the many powerful observability integrations — like from Phoenix Arize, LangWatch, or Weights & Biases Weave — to observe _in real time_ the process of optimization (e.g., scores, stack traces, successful & failed traces, and candidate prompts). DSPy encourages iterative engineering by adjusting the program, data, or metrics across optimization runs. For example, some optimizers allow “checkpointing” — e.g., if you optimize with BootstrapFewShotWithRandomSearch for 10 iterations then increase to 15 iterations, the first 10 will be loaded from cache.
120 | 
121 | While these can accomplish a lot of goals, there are two limitations that future versions of DSPy will seek to address.
122 | 
123 | 1. In general, DSPy’s (i) observability, (ii) experimental tracking, (iii) cost management, and (iii) deployment of programs should become first-class concerns via integration with tools like MLFlow. We will share more plans addressing this for DSPy 2.6 in the next 1-2 months.
124 | 
125 | 2. DSPy 3.0 will introduce new optimizers that prioritize ad-hoc, human-in-the-loop feedback. This is perhaps the only substantial paradigm shift we see as necessary in the foreseeable future in DSPy. It involves various research questions at the level of the abstractions, UI/HCI, and ML, so it is a longer-term goal that we will share more about in the next 3-4 month.
126 | 
127 | 
```

api/index.md
```
1 | # API Reference
2 | 
3 | Welcome to the DSPy API reference documentation. This section provides detailed information about DSPy's classes, modules, and functions.
```

community/community-resources.md
```
1 | # Resources
2 | 
3 | This is the list of tutorials and blog posts on DSPy. If you would like to add your own tutorial, please make a PR.
4 | 
5 | ## A Few Blogs & Videos on using DSPy
6 | 
7 | ### Blogs
8 | 
9 | | **Name** | **Link** |
10 | |---|---|
11 | | **Why I bet on DSPy** | [Blog](https://blog.isaacbmiller.com/posts/dspy) |
12 | | **Not Your Average Prompt Engineering** | [Blog](https://jina.ai/news/dspy-not-your-average-prompt-engineering/) |
13 | | **Why I'm excited about DSPy** | [Blog](https://substack.stephen.so/p/why-im-excited-about-dspy) |
14 | | **Achieving GPT-4 Performance at Lower Cost** | [Link](https://gradient.ai/blog/achieving-gpt-4-level-performance-at-lower-cost-using-dspy) |
15 | | **Prompt engineering is a task best left to AI models** | [Link](https://www.theregister.com/2024/02/22/prompt_engineering_ai_models/) |
16 | | **What makes DSPy a valuable framework for developing complex language model pipelines?** | [Link](https://medium.com/@sujathamudadla1213/what-makes-dspy-a-valuable-framework-for-developing-complex-language-model-pipelines-edfa5b4bcf9b) |
17 | | **DSPy: A new framework to program your foundation models just by prompting** | [Link](https://www.linkedin.com/pulse/dspy-new-framework-program-your-foundation-models-just-prompting-lli4c/) |
18 | | **Intro to DSPy: Goodbye Prompting, Hello Programming** | [Link](https://medium.com/towards-data-science/intro-to-dspy-goodbye-prompting-hello-programming-4ca1c6ce3eb9) |
19 | | **DSPyGen: Revolutionizing AI** | [Link](https://www.linkedin.com/pulse/launch-alert-dspygen-20242252-revolutionizing-ai-sean-chatman--g9f1c/) |
20 | | **Building an AI Assistant with DSPy** | [Link](https://www.linkedin.com/pulse/building-ai-assistant-dspy-valliappa-lakshmanan-vgnsc/) |
21 | | **Building Self-improving Agents in Production with DSPy** | [Link](https://relevanceai.com/blog/building-self-improving-agentic-systems-in-production-with-dspy) |
22 | 
23 | ### Videos
24 | 
25 | | **Name** | **Link** |
26 | |---|---|
27 | | **DSPy Explained! (60K views)** | [Link](https://www.youtube.com/watch?v=41EfOY0Ldkc) |
28 | | **DSPy Intro from Sephora (25K views)** | [Link](https://www.youtube.com/watch?v=D2HurSldDkE) |
29 | | **Structured Outputs with DSPy** | [Link](https://www.youtube.com/watch?v=tVw3CwrN5-8) |
30 | | **DSPy and ColBERT - Weaviate Podcast** | [Link](https://www.youtube.com/watch?v=CDung1LnLbY) |
31 | | **SBTB23 DSPy** | [Link](https://www.youtube.com/watch?v=Dt3H2ninoeY) |
32 | | **Optimization with DSPy and LangChain** | [Link](https://www.youtube.com/watch?v=4EXOmWeqXRc) |
33 | | **Automated Prompt Engineering + Visualization** | [Link](https://www.youtube.com/watch?v=eAZ2LtJ6D5k) |
34 | | **Transforming LM Calls into Pipelines** | [Link](https://www.youtube.com/watch?v=NoaDWKHdkHg) |
35 | | **NeurIPS Hacker Cup: DSPy for Code Gen** | [Link](https://www.youtube.com/watch?v=gpe-rtJN8z8) |
36 | | **MIPRO and DSPy - Weaviate Podcast** | [Link](https://www.youtube.com/watch?v=skMH3DOV_UQ) |
37 | | **Getting Started with RAG in DSPy** | [Link](https://www.youtube.com/watch?v=CEuUG4Umfxs) |
38 | | **Adding Depth to DSPy Programs** | [Link](https://www.youtube.com/watch?v=0c7Ksd6BG88) |
39 | | **Programming Foundation Models with DSPy** | [Link](https://www.youtube.com/watch?v=Y94tw4eDHW0) |
40 | | **DSPy End-to-End: SF Meetup** | [Link](https://www.youtube.com/watch?v=Y81DoFmt-2U) |
41 | | **Monitoring & Tracing DSPy with Langtrace** | [Link](https://langtrace.ai/blog/announcing-dspy-support-in-langtrace) |
42 | | **Teaching chat models to solve chess puzzles using DSPy + Finetuning** | [Link](https://raw.sh/posts/chess_puzzles) |
43 | | **Build Self-Improving AI Agents with DSPy (No Code)** | [Link](https://www.youtube.com/watch?v=UY8OsMlV21Y) |
44 | | **DSPy 3.0 and DSPy at Databricks** | [Link](https://www.youtube.com/watch?v=grIuzesOwwU) |
45 | | **Context Engineering with DSPy** | [Link](https://www.youtube.com/watch?v=1I9PoXzvWcs) |
46 | 
47 | ### Slides
48 | 
49 | | **Name** | **Link** |
50 | |---|---|
51 | | **Context Engineering with DSPy** | [Link](https://docs.google.com/presentation/d/1ydssF387l1LsJ14z41_HUqsJwU77tKZJNGnAWPsw-1I/edit?usp=sharing) |
52 | 
53 | ### Podcasts
54 | 
55 | Weaviate has a directory of 10 amazing notebooks and 6 podcasts!
56 | Huge shoutout to them for the massive support ❤️. See the [Weaviate DSPy directory](https://weaviate.io/developers/weaviate/more-resources/dspy).
57 | 
58 | This list represents a curated selection of DSPy resources. We continuously add new content as it becomes available in the community.
59 | 
60 | Credit: Some of these resources were originally compiled in the [Awesome DSPy](https://github.com/ganarajpr/awesome-dspy/tree/master) repo.
```

community/how-to-contribute.md
```
1 | # Contributing
2 | 
3 | DSPy is an actively growing project and community, and we welcome your contributions and involvement! Please read the
4 | [contributing guide](https://github.com/stanfordnlp/dspy/blob/main/CONTRIBUTING.md) for how to contribute to DSPy.
```

community/use-cases.md
```
1 | # Use Cases
2 | 
3 | We often get questions like “How are people using DSPy in practice?”, both in production and for research. This list was created to collect a few pointers and to encourage others in the community to add their own work below.
4 | 
5 | This list is continuously growing. We regularly add new use cases and welcome community contributions. If you would like to add your product or research to this list, please submit a PR.
6 | 
7 | ## A Few Company Use Cases
8 | 
9 | | **Name** | **Use Cases** |
10 | |---|---|
11 | | **[JetBlue](https://www.jetblue.com/)** | Multiple chatbot use cases. [Blog](https://www.databricks.com/blog/optimizing-databricks-llm-pipelines-dspy) |
12 | | **[Replit](https://replit.com/)** | Synthesize diffs using code LLMs using a DSPy pipeline. [Blog](https://blog.replit.com/code-repair) |
13 | | **[Databricks](https://www.databricks.com/)** | Research, products, and customer solutions around LM Judges, RAG, classification, and other applications. [Blog](https://www.databricks.com/blog/dspy-databricks), [Blog II](https://www.databricks.com/customers/ddi) |
14 | | **[Sephora](https://www.sephora.com/)** | Undisclosed agent usecases; perspectives shared in [DAIS Session](https://www.youtube.com/watch?v=D2HurSldDkE). |
15 | | **[Zoro UK](https://www.zoro.co.uk/)** | E-commerce applications around structured shopping. [Portkey Session](https://www.youtube.com/watch?v=_vGKSc1tekE) |
16 | | **[VMware](https://www.vmware.com/)** | RAG and other prompt optimization applications. [Interview in The Register.](https://www.theregister.com/2024/02/22/prompt_engineering_ai_models/) [Business Insider.](https://www.businessinsider.com/chaptgpt-large-language-model-ai-prompt-engineering-automated-optimizer-2024-3) |
17 | | **[Haize Labs](https://www.haizelabs.com/)** | Automated red-teaming for LLMs. [Blog](https://blog.haizelabs.com/posts/dspy/) |
18 | | **[Plastic Labs](https://www.plasticlabs.ai/)** | R&D pipelines for Honcho. [Blog](https://blog.plasticlabs.ai/blog/User-State-is-State-of-the-Art) |
19 | | **[PingCAP](https://pingcap.com/)** | Building a knowledge graph. [Article](https://www.pingcap.com/article/building-a-graphrag-from-wikipedia-page-using-dspy-openai-and-tidb-vector-database/) |
20 | | **[Salomatic](https://langtrace.ai/blog/case-study-how-salomatic-used-langtrace-to-build-a-reliable-medical-report-generation-system)** | Enriching medical reports using DSPy. [Blog](https://langtrace.ai/blog/case-study-how-salomatic-used-langtrace-to-build-a-reliable-medical-report-generation-system) |
21 | | **[Truelaw](https://www.youtube.com/watch?v=O0F3RAWZNfM)** | How Truelaw builds bespoke LLM pipelines for law firms using DSPy. [Podcast](https://www.youtube.com/watch?v=O0F3RAWZNfM) |
22 | | **[STChealth](https://stchealth.com/)** | Using DSPy for entity resolution including human-readable rationale for decisions. |
23 | | **[Moody's](https://www.moodys.com/)** | Leveraging DSPy to optimize RAG systems, LLM-as-a-Judge, and agentic systems for financial workflows. |
24 | | **[Normal Computing](https://www.normalcomputing.com/)** | Translate specs from chip companies from English to intermediate formal languages |
25 | | **[Procure.FYI](https://www.procure.fyi/)** | Process messy, publicly available technology spending and pricing data via DSPy. |
26 | | **[RadiantLogic](https://www.radiantlogic.com/)** | AI Data Assistant. DSPy is used for the agent that routes the query, the context extraction module, the text-to-sql conversion engine, and the table summarization module. |
27 | | **[Raia](https://raiahealth.com/)** | Using DSPy for AI-powered Personal Healthcare Agents. |
28 | | **[Hyperlint](https://hyperlint.com)** | Uses DSPy to generate technical documentation. DSPy helps to fetch relevant information and synthesize that into tutorials. |
29 | | **[Starops](https://staropshq.com/) & [Saya](https://heysaya.ai/)** | Building research documents given a user's corpus. Generate prompts to create more articles from example articles. |
30 | | **[Tessel AI](https://tesselai.com/)** | Enhancing human-machine interaction with data use cases. |
31 | | **[Dicer.ai](https://dicer.ai/)** | Uses DSPy for marketing AI to get the most from their paid ads. |
32 | | **[Howie](https://howie.ai)** | Using DSPy to automate meeting scheduling through email. |
33 | | **[Isoform.ai](https://isoform.ai)** | Building custom integrations using DSPy. |
34 | | **[Trampoline AI](https://trampoline.ai)** | Uses DSPy to power their data-augmentation and LM pipelines. |
35 | | **[Pretrain](https://pretrain.com)** | Uses DSPy to automatically optimize AI performance towards user-defined tasks based on uploaded examples. |
36 | | **[Spindle AI](https://spindle.ai)** | Turns natural-language constrained optimization problems into solvable mathematical programs whose candidate solutions are scenarios. |
37 | | **[Infinitus](https://www.infinitus.ai/product/ai-agents/)** | Leverages DSPy to build and optimize healthcare AI agents |
38 | 
39 | This list represents companies that have publicly shared their use cases or have provided permission to be included. It reflects a selection of the many industry applications of DSPy currently in production.
40 | 
41 | 
42 | ## A Few Papers Using DSPy
43 | 
44 | | **Name** | **Description** |
45 | |---|---|
46 | | **[STORM](https://arxiv.org/abs/2402.14207)** | Writing Wikipedia-like Articles From Scratch. |
47 | | **[PATH](https://arxiv.org/abs/2406.11706)** | Prompts as Auto-Optimized Training Hyperparameters: Training Best-in-Class IR Models from Scratch with 10 Gold Labels |
48 | | **[WangLab @ MEDIQA](https://arxiv.org/abs/2404.14544)** | UofT's winning system at MEDIQA, outperforms the next best system by 20 points |
49 | | **[UMD's Suicide Detection System](https://arxiv.org/abs/2406.06608)** | Outperforms 20-hour expert human prompt engineering by 40% |
50 | | **[IReRa](https://arxiv.org/abs/2401.12178)** | Infer-Retrieve-Rank: Extreme Classification with > 10,000 Labels |
51 | | **[Unreasonably Effective Eccentric Prompts](https://arxiv.org/abs/2402.10949v2)** | General Prompt Optimization |
52 | | **[Palimpzest](https://arxiv.org/abs/2405.14696)** | A Declarative System for Optimizing AI Workloads |
53 | | **[AI Agents that Matter](https://arxiv.org/abs/2407.01502v1)** | Agent Efficiency Optimization |
54 | | **[EDEN](https://arxiv.org/abs/2406.17982v1)** | Empathetic Dialogues for English Learning: Uses adaptive empathetic feedback to improve student grit |
55 | | **[ECG-Chat](https://arxiv.org/pdf/2408.08849)** | Uses DSPy with GraphRAG for medical report generation |
56 | | **[DSPy Assertions](https://arxiv.org/abs/2312.13382)** | Various applications of imposing hard and soft constraints on LM outputs |
57 | | **[DSPy Guardrails](https://boxiyu.github.io/assets/pdf/DSPy_Guardrails.pdf)** | Reduce the attack success rate of CodeAttack, decreasing from 75% to 5% |
58 | | **[Co-STORM](https://arxiv.org/pdf/2408.15232)** | Collaborative STORM: Generate Wikipedia-like articles through collaborative discourse among users and multiple LM agents |
59 | | **[MedVAL](https://arxiv.org/abs/2507.03152)** | Expert-level validation of AI-generated medical text with scalable language models |
60 | | **[Neural @ ArchEHR-QA 2025](https://aclanthology.org/2025.bionlp-share.13.pdf)** | Runner up method at 2025 BioNLP Shared Task Workshop
61 | 
62 | This list is regularly updated with new research publications using DSPy.
63 | 
64 | ## A Few Repositories (or other OSS examples) using DSPy
65 | 
66 | | **Name** | **Description/Link** |
67 | |---|---|
68 | | **Stanford CS 224U Homework** | [Github](https://github.com/cgpotts/cs224u/blob/main/hw_openqa.ipynb) |
69 | | **STORM Report Generation (10,000 GitHub stars)** | [Github](https://github.com/stanford-oval/storm) |
70 | | **DSPy Redteaming** | [Github](https://github.com/haizelabs/dspy-redteam) |
71 | | **DSPy Theory of Mind** |  [Github](https://github.com/plastic-labs/dspy-opentom) |
72 | | **Indic cross-lingual Natural Language Inference** |  [Github](https://github.com/saifulhaq95/DSPy-Indic/blob/main/indicxlni.ipynb) |
73 | | **Optimizing LM for Text2SQL using DSPy** | [Github](https://github.com/jjovalle99/DSPy-Text2SQL) |
74 | | **DSPy PII Masking Demo by Eric Ness** | [Colab](https://colab.research.google.com/drive/1KZR1sGTp_RLWUJPAiK1FKPKI-Qn9neUm?usp=sharing) |
75 | | **DSPy on BIG-Bench Hard Example** |  [Github](https://drchrislevy.github.io/posts/dspy/dspy.html) |
76 | | **Building a chess playing agent using DSPy** |  [Github](https://medium.com/thoughts-on-machine-learning/building-a-chess-playing-agent-using-dspy-9b87c868f71e) |
77 | | **Ittia Research Fact Checking** | [Github](https://github.com/ittia-research/check) |
78 | | **Strategic Debate via Tree-of-Thought** | [Github](https://github.com/zbambergerNLP/strategic-debate-tot) |
79 | | **Sanskrit to English Translation App**| [Github](https://github.com/ganarajpr/sanskrit-translator-dspy) |
80 | | **DSPy for extracting features from PDFs on arXiv**| [Github](https://github.com/S1M0N38/dspy-arxiv) |
81 | | **DSPygen: DSPy in Ruby on Rails**| [Github](https://github.com/seanchatmangpt/dspygen) |
82 | | **DSPy Inspector**| [Github](https://github.com/Neoxelox/dspy-inspector) |
83 | | **DSPy with FastAPI**| [Github](https://github.com/diicellman/dspy-rag-fastapi) |
84 | | **DSPy for Indian Languages**| [Github](https://github.com/saifulhaq95/DSPy-Indic) |
85 | | **Hurricane: Blog Posts with Generative Feedback Loops!**| [Github](https://github.com/weaviate-tutorials/Hurricane) |
86 | | **RAG example using DSPy, Gradio, FastAPI, and Ollama**| [Github](https://github.com/diicellman/dspy-gradio-rag) |
87 | | **Synthetic Data Generation**| [Github](https://colab.research.google.com/drive/1CweVOu0qhTC0yOfW5QkLDRIKuAuWJKEr?usp=sharing) |
88 | | **Self Discover**| [Github](https://colab.research.google.com/drive/1GkAQKmw1XQgg5UNzzy8OncRe79V6pADB?usp=sharing) |
89 | | **MedVAL**| [Github](https://github.com/StanfordMIMI/MedVAL) |
90 | 
91 | This list showcases some of the open-source projects and repositories using DSPy, with many more examples available in the community.
92 | 
93 | ## A Few Providers, Integrations, and related Blog Releases
94 | 
95 | | **Name** | **Link** |
96 | |---|---|
97 | | **Databricks** | [Link](https://www.databricks.com/blog/dspy-databricks) |
98 | | **Zenbase** | [Link](https://zenbase.ai/) |
99 | | **LangWatch** | [Link](https://langwatch.ai/blog/introducing-dspy-visualizer) |
100 | | **Gradient** | [Link](https://gradient.ai/blog/achieving-gpt-4-level-performance-at-lower-cost-using-dspy) |
101 | | **Snowflake** | [Link](https://medium.com/snowflake/dspy-snowflake-140d6d947d73) |
102 | | **Langchain** | [Link](https://python.langchain.com/v0.2/docs/integrations/providers/dspy/) |
103 | | **Weaviate** | [Link](https://weaviate.io/blog/dspy-optimizers) |
104 | | **Qdrant** | [Link](https://qdrant.tech/documentation/frameworks/dspy/) |
105 | | **Weights & Biases Weave** | [Link](https://weave-docs.wandb.ai/guides/integrations/dspy/) |
106 | | **Milvus** | [Link](https://milvus.io/docs/integrate_with_dspy.md) |
107 | | **Neo4j** | [Link](https://neo4j.com/labs/genai-ecosystem/dspy/) |
108 | | **Lightning AI** | [Link](https://lightning.ai/lightning-ai/studios/dspy-programming-with-foundation-models) |
109 | | **Haystack** | [Link](https://towardsdatascience.com/automating-prompt-engineering-with-dspy-and-haystack-926a637a3f43) |
110 | | **Arize** | [Link](https://docs.arize.com/phoenix/tracing/integrations-tracing/dspy) |
111 | | **LlamaIndex** | [Link](https://github.com/stanfordnlp/dspy/blob/main/examples/llamaindex/dspy_llamaindex_rag.ipynb) |
112 | | **Langtrace** | [Link](https://docs.langtrace.ai/supported-integrations/llm-frameworks/dspy) |
113 | | **Langfuse** | [Link](https://langfuse.com/docs/integrations/dspy) |
114 | | **OpenLIT** | [Link](https://docs.openlit.io/latest/integrations/dspy) |
115 | | **Relevance AI** | [Link](https://relevanceai.com/blog/dspy-programming---not-prompting---language-models) |
116 | 
117 | Credit: Some of these resources were originally compiled in the [Awesome DSPy](https://github.com/ganarajpr/awesome-dspy/tree/master) repo.
```

js/runllm-widget.js
```
1 | document.addEventListener("DOMContentLoaded", function () {
2 |     var script = document.createElement("script");
3 |     script.defer = true;
4 |     script.type = "module";
5 |     script.id = "runllm-widget-script";
6 |     script.src =
7 |       "https://widget.runllm.com";
8 |     script.setAttribute("runllm-name", "DSPy");
9 |     script.setAttribute("runllm-preset", "mkdocs");
10 |     script.setAttribute("runllm-server-address", "https://api.runllm.com");
11 |     script.setAttribute("runllm-assistant-id", "132");
12 |     script.setAttribute("runllm-position", "BOTTOM_RIGHT");
13 |     script.setAttribute("runllm-keyboard-shortcut", "Mod+j");
14 |     script.setAttribute(
15 |       "runllm-slack-community-url",
16 |       ""
17 |     );
18 |   
19 |     document.head.appendChild(script);
20 |   });
```

learn/index.md
```
1 | ---
2 | sidebar_position: 1
3 | ---
4 | 
5 | # Learning DSPy: An Overview
6 | 
7 | DSPy exposes a very small API that you can learn quickly. However, building a new AI system is a more open-ended journey of iterative development, in which you compose the tools and design patterns of DSPy to optimize for _your_ objectives. The three stages of building AI systems in DSPy are:
8 | 
9 | 1) **DSPy Programming.** This is about defining your task, its constraints, exploring a few examples, and using that to inform your initial pipeline design.
10 | 
11 | 2) **DSPy Evaluation.** Once your system starts working, this is the stage where you collect an initial development set, define your DSPy metric, and use these to iterate on your system more systematically.
12 | 
13 | 3) **DSPy Optimization.** Once you have a way to evaluate your system, you use DSPy optimizers to tune the prompts or weights in your program.
14 | 
15 | We suggest learning and applying DSPy in this order. For example, it's unproductive to launch optimization runs using a poorly designed program or a bad metric.
```

production/index.md
```
1 | # Using DSPy in Production
2 | 
3 | <div class="grid cards" style="text-align: left;" markdown>
4 | 
5 | - :material-earth:{ .lg .middle } __Real-World Use Cases__
6 | 
7 |     ---
8 | 
9 |     DSPy is deployed in production by many enterprises and startups. Explore real-world case studies.
10 | 
11 |     [:octicons-arrow-right-24: Use Cases](../community/use-cases.md)
12 | 
13 | - :material-magnify-expand:{ .lg .middle } __Monitoring & Observability__
14 | 
15 |     ---
16 | 
17 |     Monitor your DSPy programs using **MLflow Tracing**, based on OpenTelemetry.
18 | 
19 |     [:octicons-arrow-right-24: Set Up Observability](../tutorials/observability/index.md#tracing)
20 | 
21 | - :material-ab-testing: __Reproducibility__
22 | 
23 |     ---
24 | 
25 |     Log programs, metrics, configs, and environments for full reproducibility with DSPy's native MLflow integration.
26 | 
27 |     [:octicons-arrow-right-24: MLflow Integration](https://mlflow.org/docs/latest/llms/dspy/index.html)
28 | 
29 | - :material-rocket-launch: __Deployment__
30 | 
31 |     ---
32 | 
33 |     When it's time to productionize, deploy your application easily with DSPy's integration with MLflow Model Serving.
34 | 
35 |     [:octicons-arrow-right-24: Deployment Guide](../tutorials/deployment/index.md)
36 | 
37 | - :material-arrow-up-right-bold: __Scalability__
38 | 
39 |     ---
40 | 
41 |     DSPy is designed with thread-safety in mind and offers native asynchronous execution support for high-throughput environments.
42 | 
43 |     [:octicons-arrow-right-24: Async Program](../api/utils/asyncify.md)
44 | 
45 | - :material-alert-rhombus: __Guardrails & Controllability__
46 | 
47 |     ---
48 | 
49 |     DSPy's **Signatures**, **Modules**, and **Optimizers** help you control and guide LM outputs.
50 | 
51 |     [:octicons-arrow-right-24: Learn Signature](../learn/programming/signatures.md)
52 | 
53 | </div>
```

static/.nojekyll
```
```

stylesheets/extra.css
```
1 | /* Custom styles for logo */
2 | .md-logo, .md-logo img {
3 |     width: auto !important;
4 |     height: 1.5rem !important;
5 |     padding: 0 !important;
6 |     margin: 0 !important;
7 | }
8 | 
9 | .md-logo img {
10 |     object-fit: contain !important;
11 | }
12 | 
13 | /* Adjust header to accommodate logo */
14 | .md-header__inner {
15 |     padding: 0.5rem !important;
16 | }
17 | 
18 | /* Responsive adjustments */
19 | @media screen and (min-width: 76.25em) {
20 |     .md-logo {
21 |         height: 1.8rem !important; /* Reduced from 2.2rem */
22 |     }
23 |     .md-logo img {
24 |         height: 1.8rem !important; /* Reduced from 2.2rem */
25 |     }
26 | }
27 | 
28 | /* Dark mode specific adjustments */
29 | [data-md-color-scheme="slate"] .md-logo img {
30 |     filter: brightness(0.9);
31 | }
32 | 
33 | /* Default max-width */
34 | .md-content {
35 |     max-width: 980px;
36 |     margin: 0 auto;
37 | }
38 | 
39 | 
40 | /* Adjust width when a sidebar or TOC is present */
41 | @media (min-width: 1700px) {
42 |     /* Increase width when there's enough space (like on desktop or tablet landscape) */
43 |     .md-content {
44 |         min-width: 980px;
45 |     }
46 | }
47 | 
48 | 
49 | /* Justified text for main content */
50 | .md-content__inner p {
51 |     text-align: justify;
52 | }
53 | 
54 | /* Left-aligned text for grid cards */
55 | .md-content__inner .grid.cards p {
56 |     text-align: left;
57 | }
58 | 
59 | /* Base styling for the output area */
60 | .jp-Cell-outputWrapper .jp-OutputArea-output pre {
61 |     max-height: 300px;
62 |     overflow-y: auto;
63 |     padding: 10px 15px;
64 |     border-radius: 5px;
65 |     font-family: monospace;
66 |     font-size: 0.9em;
67 | }
68 | 
69 | /* Light mode specific styling */
70 | :root {
71 |     --output-bg-light: #fafafa;
72 |     --output-border-light: #ddd;
73 |     --output-text-light: #333;
74 | }
75 | 
76 | body[data-md-color-scheme="default"] .jp-Cell-outputWrapper .jp-OutputArea-output pre {
77 |     background-color: var(--output-bg-light);
78 |     border: 1px solid var(--output-border-light);
79 |     color: var(--output-text-light);
80 |     box-shadow: 0px 1px 4px rgba(0, 0, 0, 0.1);
81 | }
82 | 
83 | /* Dark mode specific styling */
84 | :root {
85 |     --output-bg-dark: #2e2e2e;
86 |     --output-border-dark: #555;
87 |     --output-text-dark: #e0e0e0;
88 | }
89 | 
90 | body[data-md-color-scheme="slate"] .jp-Cell-outputWrapper .jp-OutputArea-output pre {
91 |     background-color: var(--output-bg-dark);
92 |     border: 1px solid var(--output-border-dark);
93 |     color: var(--output-text-dark);
94 |     box-shadow: 0px 1px 4px rgba(255, 255, 255, 0.1);
95 | }
96 | 
97 | 
98 | 
99 | /* Set a fixed width for the sidebar */
100 | .md-sidebar {
101 |     width: 235px;
102 | }
103 | 
104 | /* Adjust search bar position */
105 | .md-search {
106 |     margin-left: auto;
107 |     padding-right: 0;
108 | }
109 | 
110 | /* If you need to adjust the width of the search bar */
111 | .md-search__inner {
112 |     width: 13rem;
113 | }
114 | 
115 | /* Adjust repository button position and alignment */
116 | .md-header__source {
117 |     margin-left: 1rem;
118 |     margin-right: 0;
119 |     text-align: right;  /* Keep right alignment for container */
120 | }
121 | 
122 | .md-header__source .md-source {
123 |     justify-content: flex-start;  /* Change to flex-start to align text to left */
124 |     width: auto;  /* Allow element to shrink to content */
125 |     min-width: 0;  /* Remove minimum width constraint */
126 | }
127 | 
128 | .md-header__source .md-source__icon {
129 |     order: 2;  /* Keep icon on the right */
130 |     margin-left: 0.5rem;
131 |     margin-right: 0;
132 | }
133 | 
134 | .md-header__source .md-source__repository {
135 |     order: 1;  /* Keep text on the left */
136 |     text-align: left;  /* Ensure text is left-aligned */
137 | }
138 | 
139 | h2.doc-heading {
140 |     font-size: 1rem;
141 |     font-weight: 700;
142 | }
143 | 
144 | /* Add more spacing between API sections */
145 | .doc-heading {
146 |     margin-top: 1em;
147 |     border-top: 1px solid var(--md-default-fg-color--lightest);
148 |     font-size: 0.85rem;
149 | }
150 | 
151 | /* Make method names more prominent */
152 | .doc-method, .doc-function {
153 |     background-color: var(--md-code-bg-color);
154 |     padding: 0.1em;
155 |     margin: 0.5em 0;
156 |     border-radius: 4px;
157 | }
158 | 
159 | /* Make class documentation stand out */
160 | .doc-class {
161 |     padding: 1em;
162 |     margin: 1em 0;
163 |     border-left: 4px solid var(--md-primary-fg-color);
164 |     background-color: var(--md-code-bg-color);
165 | }
166 | 
167 | /* Style for type labels */
168 | .doc-label {
169 |     font-size: 0.8em;
170 |     padding: 0.2em 0.6em;
171 |     border-radius: 4px;
172 |     background-color: var(--md-code-bg-color);
173 |     display: inline-block;
174 |     margin: 0.2em 0;
175 |     font-weight: 400;
176 |     text-transform: none;  /* Prevent uppercase transformation */
177 |     color: var(--md-code-fg-color);
178 | }
179 | 
180 | /* Add indentation and visual cues for nested navigation items */
181 | .md-nav__item .md-nav__item {
182 |     padding-left: 0.3rem;
183 |     border-left: 1px solid var(--md-primary-fg-color--light);
184 |     margin-left: 0.2rem;
185 | }
186 | 
187 | /* Add some spacing between items */
188 | .md-nav__item {
189 |     margin: 0.3em 0;  /* Reduced from 0.4em */
190 | }
191 | 
192 | /* Optional: add hover effect */
193 | .md-nav__item .md-nav__item:hover {
194 |     border-left-color: var(--md-primary-fg-color);
195 | }
196 | 
197 | 
198 | /* Enhance code examples in documentation */
199 | .highlight {
200 |     background-color: #f8f9fa;
201 |     border: 1px solid #e9ecef;
202 |     border-radius: 6px;
203 |     margin: 1.5em 0;
204 |     padding: 1em;
205 |     box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
206 | }
207 | 
208 | /* Dark mode adjustments */
209 | [data-md-color-scheme="slate"] .highlight {
210 |     background-color: #2b2b2b;
211 |     border-color: #404040;
212 | }
213 | 
214 | /* Add subtle left border for visual interest */
215 | .highlight pre {
216 |     margin: 0;
217 | }
218 | 
219 | /* Ensure code is readable */
220 | .highlight code {
221 |     font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
222 |     font-size: 0.75em;
223 | }
224 | 
225 | .highlight .linenos {
226 |     font-size: 0.75em;
227 | }
228 | 
229 | /* Copy button styling */
230 | .highlight .md-clipboard {
231 |     color: var(--md-default-fg-color--lighter);
232 | }
233 | 
234 | .highlight .md-clipboard:hover {
235 |     color: var(--md-accent-fg-color);
236 | }
237 | 
```

tutorials/index.md
```
1 | Welcome to DSPy tutorials! We've organized our tutorials into three main categories to help you get started:
2 | 
3 | - **Build AI Programs with DSPy**: These hands-on tutorials guide you through building production-ready AI
4 |   applications. From implementing RAG systems to creating intelligent agents, each tutorial demonstrates
5 |   practical use cases. You'll also learn how to leverage DSPy optimizers to enhance your program's performance.
6 | 
7 | - **Optimize AI Programs with DSPy Optimizers**: These tutorials deep dive into DSPy's optimization capabilities. While
8 |   lighter on programming concepts, they focus on how to systematically improve your AI programs using DSPy
9 |   optimizers, and showcase how DSPy optimizers help improve the quality automatically.
10 | 
11 | - **DSPy Core Development**: These tutorials cover essential DSPy features and best practices. Learn how to implement
12 |   key functionalities like streaming, caching, deployment, and monitoring in your DSPy applications.
13 | 
14 | 
15 | - Build AI Programs with DSPy
16 |     - [Managing Conversation History](conversation_history/index.md)
17 |     - [Building AI Agents with DSPy](customer_service_agent/index.ipynb)
18 |     - [Building AI Applications by Customizing DSPy Modules](custom_module/index.ipynb)
19 |     - [Retrieval-Augmented Generation (RAG)](rag/index.ipynb)
20 |     - [Building RAG as Agent](agents/index.ipynb)
21 |     - [Entity Extraction](entity_extraction/index.ipynb)
22 |     - [Classification](classification/index.md)
23 |     - [Multi-Hop RAG](multihop_search/index.ipynb)
24 |     - [Privacy-Conscious Delegation](papillon/index.md)
25 |     - [Program Of Thought](program_of_thought/index.ipynb)
26 |     - [Image Generation Prompt iteration](image_generation_prompting/index.ipynb)
27 |     - [Audio](audio/index.ipynb)
28 | 
29 | 
30 | - Optimize AI Programs with DSPy
31 |     - [Math Reasoning](math/index.ipynb)
32 |     - [Classification Finetuning](classification_finetuning/index.ipynb)
33 |     - [Advanced Tool Use](tool_use/index.ipynb)
34 |     - [Finetuning Agents](games/index.ipynb)
35 | 
36 | 
37 | - Reflective Prompt Evolution with dspy.GEPA:
38 |     - [Overview](gepa_ai_program/index.md)
39 |     - [GEPA for AIME](gepa_aime/index.ipynb)
40 |     - [GEPA for PAPILLON](gepa_papillon/index.ipynb)
41 |     - [GEPA for Enterprise classification task](gepa_facilitysupportanalyzer/index.ipynb)
42 | 
43 | 
44 | - Tools, Development, and Deployment
45 |     - [Use MCP in DSPy](mcp/index.md)
46 |     - [Output Refinement](output_refinement/best-of-n-and-refine.md)
47 |     - [Saving and Loading](saving/index.md)
48 |     - [Cache](cache/index.md)
49 |     - [Deployment](deployment/index.md)
50 |     - [Debugging & Observability](observability/index.md)
51 |     - [Tracking DSPy Optimizers](optimizer_tracking/index.md)
52 |     - [Streaming](streaming/index.md)
53 |     - [Async](async/index.md)
54 | 
55 | 
```

api/adapters/Adapter.md
```
1 | # dspy.Adapter
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Adapter
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - format
11 |             - format_assistant_message_content
12 |             - format_conversation_history
13 |             - format_demos
14 |             - format_field_description
15 |             - format_field_structure
16 |             - format_task_description
17 |             - format_user_message_content
18 |             - parse
19 |         show_source: true
20 |         show_root_heading: true
21 |         heading_level: 2
22 |         docstring_style: google
23 |         show_root_full_path: true
24 |         show_object_full_path: false
25 |         separate_signature: false
26 |         inherited_members: true
27 | :::
28 | <!-- END_API_REF -->
```

api/adapters/ChatAdapter.md
```
1 | # dspy.ChatAdapter
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.ChatAdapter
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - format
11 |             - format_assistant_message_content
12 |             - format_conversation_history
13 |             - format_demos
14 |             - format_field_description
15 |             - format_field_structure
16 |             - format_field_with_value
17 |             - format_finetune_data
18 |             - format_task_description
19 |             - format_user_message_content
20 |             - parse
21 |             - user_message_output_requirements
22 |         show_source: true
23 |         show_root_heading: true
24 |         heading_level: 2
25 |         docstring_style: google
26 |         show_root_full_path: true
27 |         show_object_full_path: false
28 |         separate_signature: false
29 |         inherited_members: true
30 | :::
31 | <!-- END_API_REF -->
```

api/adapters/JSONAdapter.md
```
1 | # dspy.JSONAdapter
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.JSONAdapter
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - format
11 |             - format_assistant_message_content
12 |             - format_conversation_history
13 |             - format_demos
14 |             - format_field_description
15 |             - format_field_structure
16 |             - format_field_with_value
17 |             - format_finetune_data
18 |             - format_task_description
19 |             - format_user_message_content
20 |             - parse
21 |             - user_message_output_requirements
22 |         show_source: true
23 |         show_root_heading: true
24 |         heading_level: 2
25 |         docstring_style: google
26 |         show_root_full_path: true
27 |         show_object_full_path: false
28 |         separate_signature: false
29 |         inherited_members: true
30 | :::
31 | <!-- END_API_REF -->
```

api/adapters/TwoStepAdapter.md
```
1 | # dspy.TwoStepAdapter
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.TwoStepAdapter
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - format
11 |             - format_assistant_message_content
12 |             - format_conversation_history
13 |             - format_demos
14 |             - format_field_description
15 |             - format_field_structure
16 |             - format_task_description
17 |             - format_user_message_content
18 |             - parse
19 |         show_source: true
20 |         show_root_heading: true
21 |         heading_level: 2
22 |         docstring_style: google
23 |         show_root_full_path: true
24 |         show_object_full_path: false
25 |         separate_signature: false
26 |         inherited_members: true
27 | :::
28 | <!-- END_API_REF -->
```

api/evaluation/CompleteAndGrounded.md
```
1 | # dspy.evaluate.CompleteAndGrounded
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.evaluate.CompleteAndGrounded
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - batch
11 |             - deepcopy
12 |             - dump_state
13 |             - forward
14 |             - get_lm
15 |             - inspect_history
16 |             - load
17 |             - load_state
18 |             - map_named_predictors
19 |             - named_parameters
20 |             - named_predictors
21 |             - named_sub_modules
22 |             - parameters
23 |             - predictors
24 |             - reset_copy
25 |             - save
26 |             - set_lm
27 |         show_source: true
28 |         show_root_heading: true
29 |         heading_level: 2
30 |         docstring_style: google
31 |         show_root_full_path: true
32 |         show_object_full_path: false
33 |         separate_signature: false
34 |         inherited_members: true
35 | :::
36 | <!-- END_API_REF -->
```

api/evaluation/Evaluate.md
```
1 | # dspy.Evaluate
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Evaluate
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |         show_source: true
10 |         show_root_heading: true
11 |         heading_level: 2
12 |         docstring_style: google
13 |         show_root_full_path: true
14 |         show_object_full_path: false
15 |         separate_signature: false
16 |         inherited_members: true
17 | :::
18 | <!-- END_API_REF -->
```

api/evaluation/EvaluationResult.md
```
1 | # dspy.evaluate.EvaluationResult
2 | 
3 | 
4 | <!-- START_API_REF -->
5 | ::: dspy.evaluate.EvaluationResult
6 |     handler: python
7 |     options:
8 |         members:
9 |             - copy
10 |             - from_completions
11 |             - get
12 |             - get_lm_usage
13 |             - inputs
14 |             - items
15 |             - keys
16 |             - labels
17 |             - set_lm_usage
18 |             - toDict
19 |             - values
20 |             - with_inputs
21 |             - without
22 |         show_source: true
23 |         show_root_heading: true
24 |         heading_level: 2
25 |         docstring_style: google
26 |         show_root_full_path: true
27 |         show_object_full_path: false
28 |         separate_signature: false
29 |         inherited_members: true
30 | :::
31 | <!-- END_API_REF -->
```

api/evaluation/SemanticF1.md
```
1 | # dspy.evaluate.SemanticF1
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.evaluate.SemanticF1
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - batch
11 |             - deepcopy
12 |             - dump_state
13 |             - forward
14 |             - get_lm
15 |             - inspect_history
16 |             - load
17 |             - load_state
18 |             - map_named_predictors
19 |             - named_parameters
20 |             - named_predictors
21 |             - named_sub_modules
22 |             - parameters
23 |             - predictors
24 |             - reset_copy
25 |             - save
26 |             - set_lm
27 |         show_source: true
28 |         show_root_heading: true
29 |         heading_level: 2
30 |         docstring_style: google
31 |         show_root_full_path: true
32 |         show_object_full_path: false
33 |         separate_signature: false
34 |         inherited_members: true
35 | :::
36 | <!-- END_API_REF -->
```

api/evaluation/answer_exact_match.md
```
1 | # dspy.evaluate.answer_exact_match
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.evaluate.answer_exact_match
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/evaluation/answer_passage_match.md
```
1 | # dspy.evaluate.answer_passage_match
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.evaluate.answer_passage_match
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/experimental/Citations.md
```
1 | # dspy.experimental.Citations
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.experimental.Citations
5 |     handler: python
6 |     options:
7 |         members:
8 |             - description
9 |             - extract_custom_type_from_annotation
10 |             - format
11 |             - from_dict_list
12 |             - is_streamable
13 |             - parse_lm_response
14 |             - parse_stream_chunk
15 |             - serialize_model
16 |             - validate_input
17 |         show_source: true
18 |         show_root_heading: true
19 |         heading_level: 2
20 |         docstring_style: google
21 |         show_root_full_path: true
22 |         show_object_full_path: false
23 |         separate_signature: false
24 |         inherited_members: true
25 | :::
26 | <!-- END_API_REF -->
```

api/experimental/Document.md
```
1 | # dspy.experimental.Document
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.experimental.Document
5 |     handler: python
6 |     options:
7 |         members:
8 |             - description
9 |             - extract_custom_type_from_annotation
10 |             - format
11 |             - is_streamable
12 |             - parse_lm_response
13 |             - parse_stream_chunk
14 |             - serialize_model
15 |             - validate_input
16 |         show_source: true
17 |         show_root_heading: true
18 |         heading_level: 2
19 |         docstring_style: google
20 |         show_root_full_path: true
21 |         show_object_full_path: false
22 |         separate_signature: false
23 |         inherited_members: true
24 | :::
25 | <!-- END_API_REF -->
```

api/models/Embedder.md
```
1 | # dspy.Embedder
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Embedder
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/models/LM.md
```
1 | # dspy.LM
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.LM
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - aforward
11 |             - copy
12 |             - dump_state
13 |             - finetune
14 |             - forward
15 |             - infer_provider
16 |             - inspect_history
17 |             - kill
18 |             - launch
19 |             - reinforce
20 |             - update_history
21 |         show_source: true
22 |         show_root_heading: true
23 |         heading_level: 2
24 |         docstring_style: google
25 |         show_root_full_path: true
26 |         show_object_full_path: false
27 |         separate_signature: false
28 |         inherited_members: true
29 | :::
30 | <!-- END_API_REF -->
```

api/modules/BestOfN.md
```
1 | # dspy.BestOfN
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.BestOfN
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - batch
11 |             - deepcopy
12 |             - dump_state
13 |             - forward
14 |             - get_lm
15 |             - inspect_history
16 |             - load
17 |             - load_state
18 |             - map_named_predictors
19 |             - named_parameters
20 |             - named_predictors
21 |             - named_sub_modules
22 |             - parameters
23 |             - predictors
24 |             - reset_copy
25 |             - save
26 |             - set_lm
27 |         show_source: true
28 |         show_root_heading: true
29 |         heading_level: 2
30 |         docstring_style: google
31 |         show_root_full_path: true
32 |         show_object_full_path: false
33 |         separate_signature: false
34 |         inherited_members: true
35 | :::
36 | <!-- END_API_REF -->
```

api/modules/ChainOfThought.md
```
1 | # dspy.ChainOfThought
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.ChainOfThought
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - aforward
11 |             - batch
12 |             - deepcopy
13 |             - dump_state
14 |             - forward
15 |             - get_lm
16 |             - inspect_history
17 |             - load
18 |             - load_state
19 |             - map_named_predictors
20 |             - named_parameters
21 |             - named_predictors
22 |             - named_sub_modules
23 |             - parameters
24 |             - predictors
25 |             - reset_copy
26 |             - save
27 |             - set_lm
28 |         show_source: true
29 |         show_root_heading: true
30 |         heading_level: 2
31 |         docstring_style: google
32 |         show_root_full_path: true
33 |         show_object_full_path: false
34 |         separate_signature: false
35 |         inherited_members: true
36 | :::
37 | <!-- END_API_REF -->
```

api/modules/CodeAct.md
```
1 | # dspy.CodeAct
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.CodeAct
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - batch
10 |             - deepcopy
11 |             - dump_state
12 |             - get_lm
13 |             - inspect_history
14 |             - load
15 |             - load_state
16 |             - map_named_predictors
17 |             - named_parameters
18 |             - named_predictors
19 |             - named_sub_modules
20 |             - parameters
21 |             - predictors
22 |             - reset_copy
23 |             - save
24 |             - set_lm
25 |         show_source: true
26 |         show_root_heading: true
27 |         heading_level: 2
28 |         docstring_style: google
29 |         show_root_full_path: true
30 |         show_object_full_path: false
31 |         separate_signature: false
32 |         inherited_members: true
33 | <!-- END_API_REF -->
34 | 
35 | # CodeAct
36 | 
37 | CodeAct is a DSPy module that combines code generation with tool execution to solve problems. It generates Python code snippets that use provided tools and the Python standard library to accomplish tasks.
38 | 
39 | ## Basic Usage
40 | 
41 | Here's a simple example of using CodeAct:
42 | 
43 | ```python
44 | import dspy
45 | from dspy.predict import CodeAct
46 | 
47 | # Define a simple tool function
48 | def factorial(n: int) -> int:
49 |     """Calculate the factorial of a number."""
50 |     if n == 1:
51 |         return 1
52 |     return n * factorial(n-1)
53 | 
54 | # Create a CodeAct instance
55 | act = CodeAct("n->factorial_result", tools=[factorial])
56 | 
57 | # Use the CodeAct instance
58 | result = act(n=5)
59 | print(result) # Will calculate factorial(5) = 120
60 | ```
61 | 
62 | ## How It Works
63 | 
64 | CodeAct operates in an iterative manner:
65 | 
66 | 1. Takes input parameters and available tools
67 | 2. Generates Python code snippets that use these tools
68 | 3. Executes the code using a Python sandbox
69 | 4. Collects the output and determines if the task is complete
70 | 5. Answer the original question based on the collected information
71 | 
72 | ## ⚠️ Limitations
73 | 
74 | ### Only accepts pure functions as tools (no callable objects)
75 | 
76 | The following example does not work due to the usage of a callable object.
77 | 
78 | ```python
79 | # ❌ NG
80 | class Add():
81 |     def __call__(self, a: int, b: int):
82 |         return a + b
83 | 
84 | dspy.CodeAct("question -> answer", tools=[Add()])
85 | ```
86 | 
87 | ### External libraries cannot be used
88 | 
89 | The following example does not work due to the usage of the external library `numpy`.
90 | 
91 | ```python
92 | # ❌ NG
93 | import numpy as np
94 | 
95 | def exp(i: int):
96 |     return np.exp(i)
97 | 
98 | dspy.CodeAct("question -> answer", tools=[exp])
99 | ```
100 | 
101 | ### All dependent functions need to be passed to `CodeAct`
102 | 
103 | Functions that depend on other functions or classes not passed to `CodeAct` cannot be used. The following example does not work because the tool functions depend on other functions or classes that are not passed to `CodeAct`, such as `Profile` or `secret_function`.
104 | 
105 | ```python
106 | # ❌ NG
107 | from pydantic import BaseModel
108 | 
109 | class Profile(BaseModel):
110 |     name: str
111 |     age: int
112 |     
113 | def age(profile: Profile):
114 |     return 
115 | 
116 | def parent_function():
117 |     print("Hi!")
118 | 
119 | def child_function():
120 |     parent_function()
121 | 
122 | dspy.CodeAct("question -> answer", tools=[age, child_function])
123 | ```
124 | 
125 | Instead, the following example works since all necessary tool functions are passed to `CodeAct`:
126 | 
127 | ```python
128 | # ✅ OK
129 | 
130 | def parent_function():
131 |     print("Hi!")
132 | 
133 | def child_function():
134 |     parent_function()
135 | 
136 | dspy.CodeAct("question -> answer", tools=[parent_function, child_function])
137 | ```
```

api/modules/Module.md
```
1 | # dspy.Module
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Module
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - batch
11 |             - deepcopy
12 |             - dump_state
13 |             - get_lm
14 |             - inspect_history
15 |             - load
16 |             - load_state
17 |             - map_named_predictors
18 |             - named_parameters
19 |             - named_predictors
20 |             - named_sub_modules
21 |             - parameters
22 |             - predictors
23 |             - reset_copy
24 |             - save
25 |             - set_lm
26 |         show_source: true
27 |         show_root_heading: true
28 |         heading_level: 2
29 |         docstring_style: google
30 |         show_root_full_path: true
31 |         show_object_full_path: false
32 |         separate_signature: false
33 |         inherited_members: true
34 | :::
35 | <!-- END_API_REF -->
```

api/modules/MultiChainComparison.md
```
1 | # dspy.MultiChainComparison
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.MultiChainComparison
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - batch
11 |             - deepcopy
12 |             - dump_state
13 |             - forward
14 |             - get_lm
15 |             - inspect_history
16 |             - load
17 |             - load_state
18 |             - map_named_predictors
19 |             - named_parameters
20 |             - named_predictors
21 |             - named_sub_modules
22 |             - parameters
23 |             - predictors
24 |             - reset_copy
25 |             - save
26 |             - set_lm
27 |         show_source: true
28 |         show_root_heading: true
29 |         heading_level: 2
30 |         docstring_style: google
31 |         show_root_full_path: true
32 |         show_object_full_path: false
33 |         separate_signature: false
34 |         inherited_members: true
35 | :::
36 | <!-- END_API_REF -->
```

api/modules/Parallel.md
```
1 | # dspy.Parallel
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Parallel
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - forward
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/modules/Predict.md
```
1 | # dspy.Predict
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Predict
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - aforward
11 |             - batch
12 |             - deepcopy
13 |             - dump_state
14 |             - forward
15 |             - get_config
16 |             - get_lm
17 |             - inspect_history
18 |             - load
19 |             - load_state
20 |             - map_named_predictors
21 |             - named_parameters
22 |             - named_predictors
23 |             - named_sub_modules
24 |             - parameters
25 |             - predictors
26 |             - reset
27 |             - reset_copy
28 |             - save
29 |             - set_lm
30 |             - update_config
31 |         show_source: true
32 |         show_root_heading: true
33 |         heading_level: 2
34 |         docstring_style: google
35 |         show_root_full_path: true
36 |         show_object_full_path: false
37 |         separate_signature: false
38 |         inherited_members: true
39 | :::
40 | <!-- END_API_REF -->
```

api/modules/ProgramOfThought.md
```
1 | # dspy.ProgramOfThought
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.ProgramOfThought
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - batch
11 |             - deepcopy
12 |             - dump_state
13 |             - forward
14 |             - get_lm
15 |             - inspect_history
16 |             - load
17 |             - load_state
18 |             - map_named_predictors
19 |             - named_parameters
20 |             - named_predictors
21 |             - named_sub_modules
22 |             - parameters
23 |             - predictors
24 |             - reset_copy
25 |             - save
26 |             - set_lm
27 |         show_source: true
28 |         show_root_heading: true
29 |         heading_level: 2
30 |         docstring_style: google
31 |         show_root_full_path: true
32 |         show_object_full_path: false
33 |         separate_signature: false
34 |         inherited_members: true
35 | :::
36 | <!-- END_API_REF -->
```

api/modules/ReAct.md
```
1 | # dspy.ReAct
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.ReAct
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - aforward
11 |             - batch
12 |             - deepcopy
13 |             - dump_state
14 |             - forward
15 |             - get_lm
16 |             - inspect_history
17 |             - load
18 |             - load_state
19 |             - map_named_predictors
20 |             - named_parameters
21 |             - named_predictors
22 |             - named_sub_modules
23 |             - parameters
24 |             - predictors
25 |             - reset_copy
26 |             - save
27 |             - set_lm
28 |             - truncate_trajectory
29 |         show_source: true
30 |         show_root_heading: true
31 |         heading_level: 2
32 |         docstring_style: google
33 |         show_root_full_path: true
34 |         show_object_full_path: false
35 |         separate_signature: false
36 |         inherited_members: true
37 | :::
38 | <!-- END_API_REF -->
```

api/modules/Refine.md
```
1 | # dspy.Refine
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Refine
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - batch
11 |             - deepcopy
12 |             - dump_state
13 |             - forward
14 |             - get_lm
15 |             - inspect_history
16 |             - load
17 |             - load_state
18 |             - map_named_predictors
19 |             - named_parameters
20 |             - named_predictors
21 |             - named_sub_modules
22 |             - parameters
23 |             - predictors
24 |             - reset_copy
25 |             - save
26 |             - set_lm
27 |         show_source: true
28 |         show_root_heading: true
29 |         heading_level: 2
30 |         docstring_style: google
31 |         show_root_full_path: true
32 |         show_object_full_path: false
33 |         separate_signature: false
34 |         inherited_members: true
35 | :::
36 | <!-- END_API_REF -->
```

api/optimizers/BetterTogether.md
```
1 | # dspy.BetterTogether
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.BetterTogether
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - get_params
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/optimizers/BootstrapFewShot.md
```
1 | # dspy.BootstrapFewShot
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.BootstrapFewShot
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - get_params
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/optimizers/BootstrapFewShotWithRandomSearch.md
```
1 | # dspy.BootstrapFewShotWithRandomSearch
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.BootstrapFewShotWithRandomSearch
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - get_params
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/optimizers/BootstrapFinetune.md
```
1 | # dspy.BootstrapFinetune
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.BootstrapFinetune
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - convert_to_lm_dict
10 |             - finetune_lms
11 |             - get_params
12 |         show_source: true
13 |         show_root_heading: true
14 |         heading_level: 2
15 |         docstring_style: google
16 |         show_root_full_path: true
17 |         show_object_full_path: false
18 |         separate_signature: false
19 |         inherited_members: true
20 | :::
21 | <!-- END_API_REF -->
```

api/optimizers/BootstrapRS.md
```
1 | # dspy.BootstrapRS
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.BootstrapRS
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - get_params
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/optimizers/COPRO.md
```
1 | # dspy.COPRO
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.COPRO
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - get_params
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/optimizers/Ensemble.md
```
1 | # dspy.Ensemble
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Ensemble
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - get_params
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/optimizers/InferRules.md
```
1 | # dspy.InferRules
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.InferRules
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - evaluate_program
10 |             - format_examples
11 |             - get_params
12 |             - get_predictor_demos
13 |             - induce_natural_language_rules
14 |             - update_program_instructions
15 |         show_source: true
16 |         show_root_heading: true
17 |         heading_level: 2
18 |         docstring_style: google
19 |         show_root_full_path: true
20 |         show_object_full_path: false
21 |         separate_signature: false
22 |         inherited_members: true
23 | :::
24 | <!-- END_API_REF -->
```

api/optimizers/KNN.md
```
1 | # dspy.KNN
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.KNN
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |         show_source: true
10 |         show_root_heading: true
11 |         heading_level: 2
12 |         docstring_style: google
13 |         show_root_full_path: true
14 |         show_object_full_path: false
15 |         separate_signature: false
16 |         inherited_members: true
17 | :::
18 | <!-- END_API_REF -->
```

api/optimizers/KNNFewShot.md
```
1 | # dspy.KNNFewShot
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.KNNFewShot
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - get_params
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/optimizers/LabeledFewShot.md
```
1 | # dspy.LabeledFewShot
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.LabeledFewShot
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - get_params
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/optimizers/MIPROv2.md
```
1 | # dspy.MIPROv2
2 | 
3 | `MIPROv2` (<u>M</u>ultiprompt <u>I</u>nstruction <u>PR</u>oposal <u>O</u>ptimizer Version 2) is an prompt optimizer capable of optimizing both instructions and few-shot examples jointly. It does this by bootstrapping few-shot example candidates, proposing instructions grounded in different dynamics of the task, and finding an optimized combination of these options using Bayesian Optimization. It can be used for optimizing few-shot examples & instructions jointly, or just instructions for 0-shot optimization.
4 | 
5 | <!-- START_API_REF -->
6 | ::: dspy.MIPROv2
7 |     handler: python
8 |     options:
9 |         members:
10 |             - compile
11 |             - get_params
12 |         show_source: true
13 |         show_root_heading: true
14 |         heading_level: 2
15 |         docstring_style: google
16 |         show_root_full_path: true
17 |         show_object_full_path: false
18 |         separate_signature: false
19 |         inherited_members: true
20 | :::
21 | <!-- END_API_REF -->
22 | 
23 | ## Example Usage
24 | 
25 | The program below shows optimizing a math program with MIPROv2
26 | 
27 | ```python
28 | import dspy
29 | from dspy.datasets.gsm8k import GSM8K, gsm8k_metric
30 | 
31 | # Import the optimizer
32 | from dspy.teleprompt import MIPROv2
33 | 
34 | # Initialize the LM
35 | lm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY')
36 | dspy.configure(lm=lm)
37 | 
38 | # Initialize optimizer
39 | teleprompter = MIPROv2(
40 |     metric=gsm8k_metric,
41 |     auto="medium", # Can choose between light, medium, and heavy optimization runs
42 | )
43 | 
44 | # Optimize program
45 | print(f"Optimizing program with MIPROv2...")
46 | gsm8k = GSM8K()
47 | optimized_program = teleprompter.compile(
48 |     dspy.ChainOfThought("question -> answer"),
49 |     trainset=gsm8k.train,
50 | )
51 | 
52 | # Save optimize program for future use
53 | optimized_program.save(f"optimized.json")
54 | ```
55 | 
56 | ## How `MIPROv2` works
57 | 
58 | At a high level, `MIPROv2` works by creating both few-shot examples and new instructions for each predictor in your LM program, and then searching over these using Bayesian Optimization to find the best combination of these variables for your program.  If you want a visual explanation check out this [twitter thread](https://x.com/michaelryan207/status/1804189184988713065).
59 | 
60 | These steps are broken down in more detail below:
61 | 
62 | 1) **Bootstrap Few-Shot Examples**: Randomly samples examples from your training set, and run them through your LM program. If the output from the program is correct for this example, it is kept as a valid few-shot example candidate. Otherwise, we try another example until we've curated the specified amount of few-shot example candidates. This step creates `num_candidates` sets of `max_bootstrapped_demos` bootstrapped examples and `max_labeled_demos` basic examples sampled from the training set.
63 | 
64 | 2) **Propose Instruction Candidates**. The instruction proposer includes (1) a generated summary of properties of the training dataset, (2) a generated summary of your LM program's code and the specific predictor that an instruction is being generated for, (3) the previously bootstrapped few-shot examples to show reference inputs / outputs for a given predictor and (4) a randomly sampled tip for generation (i.e. "be creative", "be concise", etc.) to help explore the feature space of potential instructions.  This context is provided to a `prompt_model` which writes high quality instruction candidates.
65 | 
66 | 3) **Find an Optimized Combination of Few-Shot Examples & Instructions**. Finally, we use Bayesian Optimization to choose which combinations of instructions and demonstrations work best for each predictor in our program. This works by running a series of `num_trials` trials, where a new set of prompts are evaluated over our validation set at each trial. The new set of prompts are only evaluated on a minibatch of size `minibatch_size` at each trial (when `minibatch`=`True`). The best averaging set of prompts is then evaluated on the full validation set every `minibatch_full_eval_steps`. At the end of the optimization process, the LM program with the set of prompts that performed best on the full validation set is returned.
67 | 
68 | For those interested in more details, more information on `MIPROv2` along with a study on `MIPROv2` compared with other DSPy optimizers can be found in [this paper](https://arxiv.org/abs/2406.11695).
```

api/optimizers/SIMBA.md
```
1 | # dspy.SIMBA
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.SIMBA
5 |     handler: python
6 |     options:
7 |         members:
8 |             - compile
9 |             - get_params
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | <!-- END_API_REF -->
19 | 
20 | ## Example Usage
21 | 
22 | ```python
23 | optimizer = dspy.SIMBA(metric=your_metric)
24 | optimized_program = optimizer.compile(your_program, trainset=your_trainset)
25 | 
26 | # Save optimize program for future use
27 | optimized_program.save(f"optimized.json")
28 | ```
29 | 
30 | ## How `SIMBA` works
31 | SIMBA (Stochastic Introspective Mini-Batch Ascent) is a DSPy optimizer that uses the LLM to analyze its own performance and generate improvement rules. It samples mini-batches, identifies challenging examples with high output variability, then either creates self-reflective rules or adds successful examples as demonstrations. See [this great blog post](https://blog.mariusvach.com/posts/dspy-simba) from [Marius](https://x.com/rasmus1610) for more details.
```

api/primitives/Audio.md
```
1 | # dspy.Audio
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Audio
5 |     handler: python
6 |     options:
7 |         members:
8 |             - description
9 |             - extract_custom_type_from_annotation
10 |             - format
11 |             - from_array
12 |             - from_file
13 |             - from_url
14 |             - is_streamable
15 |             - parse_lm_response
16 |             - parse_stream_chunk
17 |             - serialize_model
18 |             - validate_input
19 |         show_source: true
20 |         show_root_heading: true
21 |         heading_level: 2
22 |         docstring_style: google
23 |         show_root_full_path: true
24 |         show_object_full_path: false
25 |         separate_signature: false
26 |         inherited_members: true
27 | :::
28 | <!-- END_API_REF -->
```

api/primitives/Code.md
```
1 | # dspy.Code
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Code
5 |     handler: python
6 |     options:
7 |         members:
8 |             - description
9 |             - extract_custom_type_from_annotation
10 |             - format
11 |             - is_streamable
12 |             - parse_lm_response
13 |             - parse_stream_chunk
14 |             - serialize_model
15 |             - validate_input
16 |         show_source: true
17 |         show_root_heading: true
18 |         heading_level: 2
19 |         docstring_style: google
20 |         show_root_full_path: true
21 |         show_object_full_path: false
22 |         separate_signature: false
23 |         inherited_members: true
24 | :::
25 | <!-- END_API_REF -->
```

api/primitives/Example.md
```
1 | # dspy.Example
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Example
5 |     handler: python
6 |     options:
7 |         members:
8 |             - copy
9 |             - get
10 |             - inputs
11 |             - items
12 |             - keys
13 |             - labels
14 |             - toDict
15 |             - values
16 |             - with_inputs
17 |             - without
18 |         show_source: true
19 |         show_root_heading: true
20 |         heading_level: 2
21 |         docstring_style: google
22 |         show_root_full_path: true
23 |         show_object_full_path: false
24 |         separate_signature: false
25 |         inherited_members: true
26 | :::
27 | <!-- END_API_REF -->
```

api/primitives/History.md
```
1 | # dspy.History
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.History
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/primitives/Image.md
```
1 | # dspy.Image
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Image
5 |     handler: python
6 |     options:
7 |         members:
8 |             - description
9 |             - extract_custom_type_from_annotation
10 |             - format
11 |             - from_PIL
12 |             - from_file
13 |             - from_url
14 |             - is_streamable
15 |             - parse_lm_response
16 |             - parse_stream_chunk
17 |             - serialize_model
18 |         show_source: true
19 |         show_root_heading: true
20 |         heading_level: 2
21 |         docstring_style: google
22 |         show_root_full_path: true
23 |         show_object_full_path: false
24 |         separate_signature: false
25 |         inherited_members: true
26 | :::
27 | <!-- END_API_REF -->
```

api/primitives/Prediction.md
```
1 | # dspy.Prediction
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Prediction
5 |     handler: python
6 |     options:
7 |         members:
8 |             - copy
9 |             - from_completions
10 |             - get
11 |             - get_lm_usage
12 |             - inputs
13 |             - items
14 |             - keys
15 |             - labels
16 |             - set_lm_usage
17 |             - toDict
18 |             - values
19 |             - with_inputs
20 |             - without
21 |         show_source: true
22 |         show_root_heading: true
23 |         heading_level: 2
24 |         docstring_style: google
25 |         show_root_full_path: true
26 |         show_object_full_path: false
27 |         separate_signature: false
28 |         inherited_members: true
29 | :::
30 | <!-- END_API_REF -->
```

api/primitives/Tool.md
```
1 | # dspy.Tool
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Tool
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - acall
10 |             - description
11 |             - extract_custom_type_from_annotation
12 |             - format
13 |             - format_as_litellm_function_call
14 |             - from_langchain
15 |             - from_mcp_tool
16 |             - is_streamable
17 |             - parse_lm_response
18 |             - parse_stream_chunk
19 |             - serialize_model
20 |         show_source: true
21 |         show_root_heading: true
22 |         heading_level: 2
23 |         docstring_style: google
24 |         show_root_full_path: true
25 |         show_object_full_path: false
26 |         separate_signature: false
27 |         inherited_members: true
28 | :::
29 | <!-- END_API_REF -->
```

api/primitives/ToolCalls.md
```
1 | # dspy.ToolCalls
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.ToolCalls
5 |     handler: python
6 |     options:
7 |         members:
8 |             - description
9 |             - extract_custom_type_from_annotation
10 |             - format
11 |             - from_dict_list
12 |             - is_streamable
13 |             - parse_lm_response
14 |             - parse_stream_chunk
15 |             - serialize_model
16 |             - validate_input
17 |         show_source: true
18 |         show_root_heading: true
19 |         heading_level: 2
20 |         docstring_style: google
21 |         show_root_full_path: true
22 |         show_object_full_path: false
23 |         separate_signature: false
24 |         inherited_members: true
25 | :::
26 | <!-- END_API_REF -->
```

api/signatures/InputField.md
```
1 | # dspy.InputField
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.InputField
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/signatures/OutputField.md
```
1 | # dspy.OutputField
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.OutputField
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/signatures/Signature.md
```
1 | # dspy.Signature
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Signature
5 |     handler: python
6 |     options:
7 |         members:
8 |             - append
9 |             - delete
10 |             - dump_state
11 |             - equals
12 |             - insert
13 |             - load_state
14 |             - prepend
15 |             - with_instructions
16 |             - with_updated_fields
17 |         show_source: true
18 |         show_root_heading: true
19 |         heading_level: 2
20 |         docstring_style: google
21 |         show_root_full_path: true
22 |         show_object_full_path: false
23 |         separate_signature: false
24 |         inherited_members: true
25 | :::
26 | <!-- END_API_REF -->
```

api/tools/ColBERTv2.md
```
1 | # dspy.ColBERTv2
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.ColBERTv2
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |         show_source: true
10 |         show_root_heading: true
11 |         heading_level: 2
12 |         docstring_style: google
13 |         show_root_full_path: true
14 |         show_object_full_path: false
15 |         separate_signature: false
16 |         inherited_members: true
17 | :::
18 | <!-- END_API_REF -->
```

api/tools/Embeddings.md
```
1 | # dspy.retrievers.Embeddings
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.Embeddings
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - forward
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/tools/PythonInterpreter.md
```
1 | # dspy.PythonInterpreter
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.PythonInterpreter
5 |     handler: python
6 |     options:
7 |         members:
8 |             - __call__
9 |             - execute
10 |             - shutdown
11 |         show_source: true
12 |         show_root_heading: true
13 |         heading_level: 2
14 |         docstring_style: google
15 |         show_root_full_path: true
16 |         show_object_full_path: false
17 |         separate_signature: false
18 |         inherited_members: true
19 | :::
20 | <!-- END_API_REF -->
```

api/utils/StatusMessage.md
```
1 | # dspy.streaming.StatusMessage
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.streaming.StatusMessage
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/utils/StatusMessageProvider.md
```
1 | # dspy.streaming.StatusMessageProvider
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.streaming.StatusMessageProvider
5 |     handler: python
6 |     options:
7 |         members:
8 |             - lm_end_status_message
9 |             - lm_start_status_message
10 |             - module_end_status_message
11 |             - module_start_status_message
12 |             - tool_end_status_message
13 |             - tool_start_status_message
14 |         show_source: true
15 |         show_root_heading: true
16 |         heading_level: 2
17 |         docstring_style: google
18 |         show_root_full_path: true
19 |         show_object_full_path: false
20 |         separate_signature: false
21 |         inherited_members: true
22 | :::
23 | <!-- END_API_REF -->
```

api/utils/StreamListener.md
```
1 | # dspy.streaming.StreamListener
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.streaming.StreamListener
5 |     handler: python
6 |     options:
7 |         members:
8 |             - flush
9 |             - receive
10 |         show_source: true
11 |         show_root_heading: true
12 |         heading_level: 2
13 |         docstring_style: google
14 |         show_root_full_path: true
15 |         show_object_full_path: false
16 |         separate_signature: false
17 |         inherited_members: true
18 | :::
19 | <!-- END_API_REF -->
```

api/utils/asyncify.md
```
1 | # dspy.asyncify
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.asyncify
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/utils/configure_cache.md
```
1 | # dspy.configure_cache
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.configure_cache
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/utils/disable_litellm_logging.md
```
1 | # dspy.disable_litellm_logging
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.disable_litellm_logging
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/utils/disable_logging.md
```
1 | # dspy.disable_logging
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.disable_logging
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/utils/enable_litellm_logging.md
```
1 | # dspy.enable_litellm_logging
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.enable_litellm_logging
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/utils/enable_logging.md
```
1 | # dspy.enable_logging
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.enable_logging
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/utils/inspect_history.md
```
1 | # dspy.inspect_history
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.inspect_history
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/utils/load.md
```
1 | # dspy.load
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.load
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

api/utils/streamify.md
```
1 | # dspy.streamify
2 | 
3 | <!-- START_API_REF -->
4 | ::: dspy.streamify
5 |     handler: python
6 |     options:
7 |         show_source: true
8 |         show_root_heading: true
9 |         heading_level: 2
10 |         docstring_style: google
11 |         show_root_full_path: true
12 |         show_object_full_path: false
13 |         separate_signature: false
14 |         inherited_members: true
15 | :::
16 | <!-- END_API_REF -->
```

deep-dive/data-handling/built-in-datasets.md
```
1 | ---
2 | sidebar_position: 2
3 | ---
4 | 
5 | !!! warning "This page is outdated and may not be fully accurate in DSPy 2.5"
6 | 
7 | # Utilizing Built-in Datasets
8 | 
9 | It's easy to use your own data in DSPy: a dataset is just a list of `Example` objects. Using DSPy well involves being able to find and re-purpose existing datasets for your own pipelines in new ways; DSPy makes this a particularly powerful strategy.
10 | 
11 | For convenience, DSPy currently also provides support for the following dataset out of the box:
12 | 
13 | * **HotPotQA** (multi-hop question answering)
14 | * **GSM8k** (math questions)
15 | * **Color** (basic dataset of colors)
16 | 
17 | 
18 | ## Loading HotPotQA
19 | 
20 | HotPotQA is which is a collection of question-answer pairs.
21 | 
22 | ```python
23 | from dspy.datasets import HotPotQA
24 | 
25 | dataset = HotPotQA(train_seed=1, train_size=5, eval_seed=2023, dev_size=50, test_size=0)
26 | 
27 | print(dataset.train)
28 | ```
29 | **Output:**
30 | ```text
31 | [Example({'question': 'At My Window was released by which American singer-songwriter?', 'answer': 'John Townes Van Zandt'}) (input_keys=None),
32 |  Example({'question': 'which  American actor was Candace Kita  guest starred with ', 'answer': 'Bill Murray'}) (input_keys=None),
33 |  Example({'question': 'Which of these publications was most recently published, Who Put the Bomp or Self?', 'answer': 'Self'}) (input_keys=None),
34 |  Example({'question': 'The Victorians - Their Story In Pictures is a documentary series written by an author born in what year?', 'answer': '1950'}) (input_keys=None),
35 |  Example({'question': 'Which magazine has published articles by Scott Shaw, Tae Kwon Do Times or Southwest Art?', 'answer': 'Tae Kwon Do Times'}) (input_keys=None)]
36 | ```
37 | 
38 | We just loaded trainset (5 examples) and devset (50 examples). Each example in our training set contains just a question and its (human-annotated) answer. As you can see, it is loaded as a list of `Example` objects. However, one thing to note is that it doesn't set the input keys implicitly, so that is something that we'll need to do!!
39 | 
40 | ```python
41 | trainset = [x.with_inputs('question') for x in dataset.train]
42 | devset = [x.with_inputs('question') for x in dataset.dev]
43 | 
44 | print(trainset)
45 | ```
46 | **Output:**
47 | ```text
48 | [Example({'question': 'At My Window was released by which American singer-songwriter?', 'answer': 'John Townes Van Zandt'}) (input_keys={'question'}),
49 |  Example({'question': 'which  American actor was Candace Kita  guest starred with ', 'answer': 'Bill Murray'}) (input_keys={'question'}),
50 |  Example({'question': 'Which of these publications was most recently published, Who Put the Bomp or Self?', 'answer': 'Self'}) (input_keys={'question'}),
51 |  Example({'question': 'The Victorians - Their Story In Pictures is a documentary series written by an author born in what year?', 'answer': '1950'}) (input_keys={'question'}),
52 |  Example({'question': 'Which magazine has published articles by Scott Shaw, Tae Kwon Do Times or Southwest Art?', 'answer': 'Tae Kwon Do Times'}) (input_keys={'question'})]
53 | ```
54 | 
55 | DSPy typically requires very minimal labeling. Whereas your pipeline may involve six or seven complex steps, you only need labels for the initial question and the final answer. DSPy will bootstrap any intermediate labels needed to support your pipeline. If you change your pipeline in any way, the data bootstrapped will change accordingly!
56 | 
57 | 
58 | ## Advanced: Inside DSPy's `Dataset` class (Optional)
59 | 
60 | We've seen how you can use `HotPotQA` dataset class and load the HotPotQA dataset, but how does it actually work? The `HotPotQA` class inherits from the `Dataset` class, which takes care of the conversion of the data loaded from a source into train-test-dev split, all of which are *list of examples*. In the `HotPotQA` class, you only implement the `__init__` method, where you populate the splits from HuggingFace into the variables `_train`, `_test` and `_dev`. The rest of the process is handled by methods in the `Dataset` class.
61 | 
62 | ![Dataset Loading Process in HotPotQA Class](./img/data-loading.png)
63 | 
64 | But how do the methods of the `Dataset` class convert the data from HuggingFace? Let's take a deep breath and think step by step...pun intended. In example above, we can see the splits accessed by `.train`, `.dev` and `.test` methods, so let's take a look at the implementation of the `train()` method:
65 | 
66 | ```python
67 | @property
68 | def train(self):
69 |     if not hasattr(self, '_train_'):
70 |         self._train_ = self._shuffle_and_sample('train', self._train, self.train_size, self.train_seed)
71 | 
72 |     return self._train_
73 | ```
74 | 
75 | As you can see, the `train()` method serves as a property, not a regular method. Within this property, it first checks if the `_train_` attribute exists. If not, it calls the `_shuffle_and_sample()` method to process the `self._train` where the HuggingFace dataset is loaded. Let's see the  `_shuffle_and_sample()` method:
76 | 
77 | ```python
78 | def _shuffle_and_sample(self, split, data, size, seed=0):
79 |     data = list(data)
80 |     base_rng = random.Random(seed)
81 | 
82 |     if self.do_shuffle:
83 |         base_rng.shuffle(data)
84 | 
85 |     data = data[:size]
86 |     output = []
87 | 
88 |     for example in data:
89 |         output.append(Example(**example, dspy_uuid=str(uuid.uuid4()), dspy_split=split))
90 |     
91 |         return output
92 | ```
93 | 
94 | The `_shuffle_and_sample()` method does two things:
95 | 
96 | * It shuffles the data if `self.do_shuffle` is True.
97 | * It then takes a sample of size `size` from the shuffled data.
98 | * It then loops through the sampled data and converts each element in `data` into an `Example` object. The `Example` along with example data also contains a unique ID, and the split name.
99 | 
100 | Converting the raw examples into `Example` objects allows the Dataset class to process them in a standardized way later. For example, the collate method, which is used by the PyTorch DataLoader, expects each item to be an `Example`.
101 | 
102 | To summarize, the `Dataset` class handles all the necessary data processing and provides a simple API to access the different splits. This differentiates from the dataset classes like HotpotQA which require only definitions on how to load the raw data.
```

deep-dive/data-handling/examples.md
```
1 | ---
2 | sidebar_position: 1
3 | ---
4 | 
5 | !!! warning "This page is outdated and may not be fully accurate in DSPy 2.5"
6 | 
7 | # Examples in DSPy
8 | 
9 | Working in DSPy involves training sets, development sets, and test sets. This is like traditional ML, but you usually need far fewer labels (or zero labels) to use DSPy effectively.
10 | 
11 | The core data type for data in DSPy is `Example`. You will use **Examples** to represent items in your training set and test set. 
12 | 
13 | DSPy **Examples** are similar to Python `dict`s but have a few useful utilities. Your DSPy modules will return values of the type `Prediction`, which is a special sub-class of `Example`.
14 | 
15 | ## Creating an `Example`
16 | 
17 | When you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type `Example`:
18 | 
19 | ```python
20 | qa_pair = dspy.Example(question="This is a question?", answer="This is an answer.")
21 | 
22 | print(qa_pair)
23 | print(qa_pair.question)
24 | print(qa_pair.answer)
25 | ```
26 | **Output:**
27 | ```text
28 | Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys=None)
29 | This is a question?
30 | This is an answer.
31 | ```
32 | 
33 | Examples can have any field keys and any value types, though usually values are strings.
34 | 
35 | ```text
36 | object = Example(field1=value1, field2=value2, field3=value3, ...)
37 | ```
38 | 
39 | ## Specifying Input Keys
40 | 
41 | In traditional ML, there are separated "inputs" and "labels".
42 | 
43 | In DSPy, the `Example` objects have a `with_inputs()` method, which can mark specific fields as inputs. (The rest are just metadata or labels.)
44 | 
45 | ```python
46 | # Single Input.
47 | print(qa_pair.with_inputs("question"))
48 | 
49 | # Multiple Inputs; be careful about marking your labels as inputs unless you mean it.
50 | print(qa_pair.with_inputs("question", "answer"))
51 | ```
52 | 
53 | This flexibility allows for customized tailoring of the `Example` object for different DSPy scenarios.
54 | 
55 | When you call `with_inputs()`, you get a new copy of the example. The original object is kept unchanged.
56 | 
57 | 
58 | ## Element Access and Updation
59 | 
60 | Values can be accessed using the `.`(dot) operator. You can access the value of key `name` in defined object `Example(name="John Doe", job="sleep")` through `object.name`. 
61 | 
62 | To access or exclude certain keys, use `inputs()` and `labels()` methods to return new Example objects containing only input or non-input keys, respectively.
63 | 
64 | ```python
65 | article_summary = dspy.Example(article= "This is an article.", summary= "This is a summary.").with_inputs("article")
66 | 
67 | input_key_only = article_summary.inputs()
68 | non_input_key_only = article_summary.labels()
69 | 
70 | print("Example object with Input fields only:", input_key_only)
71 | print("Example object with Non-Input fields only:", non_input_key_only)
72 | ```
73 | 
74 | **Output**
75 | ```
76 | Example object with Input fields only: Example({'article': 'This is an article.'}) (input_keys=None)
77 | Example object with Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)
78 | ```
79 | 
80 | To exclude keys, use `without()`:
81 | 
82 | ```python
83 | article_summary = dspy.Example(context="This is an article.", question="This is a question?", answer="This is an answer.", rationale= "This is a rationale.").with_inputs("context", "question")
84 | 
85 | print("Example object without answer & rationale keys:", article_summary.without("answer", "rationale"))
86 | ```
87 | 
88 | **Output**
89 | ```
90 | Example object without answer & rationale keys: Example({'context': 'This is an article.', 'question': 'This is a question?'}) (input_keys=None)
91 | ```
92 | 
93 | Updating values is simply assigning a new value using the `.` operator.
94 | 
95 | ```python
96 | article_summary.context = "new context"
97 | ```
98 | 
99 | ## Iterating over Example
100 | 
101 | Iteration in the `Example` class also functions like a dictionary, supporting methods like `keys()`, `values()`, etc: 
102 | 
103 | ```python
104 | for k, v in article_summary.items():
105 |     print(f"{k} = {v}")
106 | ```
107 | 
108 | **Output**
109 | 
110 | ```text
111 | context = This is an article.
112 | question = This is a question?
113 | answer = This is an answer.
114 | rationale = This is a rationale.
115 | ```
```

deep-dive/data-handling/loading-custom-data.md
```
1 | ---
2 | sidebar_position: 3
3 | ---
4 | 
5 | !!! warning "This page is outdated and may not be fully accurate in DSPy 2.5"
6 | 
7 | # Creating a Custom Dataset
8 | 
9 | We've seen how to work with with `Example` objects and use the `HotPotQA` class to load the HuggingFace HotPotQA dataset as a list of `Example` objects. But in production, such structured datasets are rare. Instead, you'll find yourself working on a custom dataset and might question: how do I create my own dataset or what format should it be?
10 | 
11 | In DSPy, your dataset is a list of `Examples`, which we can accomplish in two ways:
12 | 
13 | * **Recommended: The Pythonic Way:** Using native python utility and logic.
14 | * **Advanced: Using DSPy's `Dataset` class**
15 | 
16 | ## Recommended: The Pythonic Way
17 | 
18 | To create a list of `Example` objects, we can simply load data from the source and formulate it into a Python list. Let's load an example CSV `sample.csv` that contains 3 fields: (**context**, **question** and **summary**) via Pandas. From there, we can construct our data list.
19 | 
20 | ```python
21 | import pandas as pd
22 | 
23 | df = pd.read_csv("sample.csv")
24 | print(df.shape)
25 | ```
26 | **Output:**
27 | ```text
28 | (1000, 3)
29 | ```
30 | ```python
31 | dataset = []
32 | 
33 | for context, question, answer in df.values:
34 |     dataset.append(dspy.Example(context=context, question=question, answer=answer).with_inputs("context", "question"))
35 | 
36 | print(dataset[:3])
37 | ```
38 | **Output:**
39 | ```python
40 | [Example({'context': nan, 'question': 'Which is a species of fish? Tope or Rope', 'answer': 'Tope'}) (input_keys={'question', 'context'}),
41 |  Example({'context': nan, 'question': 'Why can camels survive for long without water?', 'answer': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.'}) (input_keys={'question', 'context'}),
42 |  Example({'context': nan, 'question': "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?", 'answer': 'The name of the third daughter is Alice'}) (input_keys={'question', 'context'})]
43 | ```
44 | 
45 | While this is fairly simple, let's take a look at how loading datasets would look in DSPy - via the DSPythonic way!
46 | 
47 | ## Advanced: Using DSPy's `Dataset` class (Optional)
48 | 
49 | Let's take advantage of the `Dataset` class we defined in the previous article to accomplish the preprocessing: 
50 | 
51 | * Load data from CSV to a dataframe.
52 | * Split the data to train, dev and test splits.
53 | * Populate `_train`, `_dev` and `_test` class attributes. Note that these attributes should be a list of dictionary, or an iterator over mapping like HuggingFace Dataset, to make it work.
54 | 
55 | This is all done through the `__init__` method, which is the only method we have to implement.
56 | 
57 | ```python
58 | import pandas as pd
59 | from dspy.datasets.dataset import Dataset
60 | 
61 | class CSVDataset(Dataset):
62 |     def __init__(self, file_path, *args, **kwargs) -> None:
63 |         super().__init__(*args, **kwargs)
64 |         
65 |         df = pd.read_csv(file_path)
66 |         self._train = df.iloc[0:700].to_dict(orient='records')
67 | 
68 |         self._dev = df.iloc[700:].to_dict(orient='records')
69 | 
70 | dataset = CSVDataset("sample.csv")
71 | print(dataset.train[:3])
72 | ```
73 | **Output:**
74 | ```text
75 | [Example({'context': nan, 'question': 'Which is a species of fish? Tope or Rope', 'answer': 'Tope'}) (input_keys={'question', 'context'}),
76 |  Example({'context': nan, 'question': 'Why can camels survive for long without water?', 'answer': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.'}) (input_keys={'question', 'context'}),
77 |  Example({'context': nan, 'question': "Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?", 'answer': 'The name of the third daughter is Alice'}) (input_keys={'question', 'context'})]
78 | ```
79 | 
80 | Let's understand the code step by step:
81 | 
82 | * It inherits the base `Dataset` class from DSPy. This inherits all the useful data loading/processing functionality.
83 | * We load the data in CSV into a DataFrame.
84 | * We get the **train** split i.e first 700 rows in the DataFrame and convert it to lists of dicts using `to_dict(orient='records')` method and is then assigned to `self._train`.
85 | * We get the **dev** split i.e first 300 rows in the DataFrame and convert it to lists of dicts using `to_dict(orient='records')` method and is then assigned to `self._dev`.
86 | 
87 | Using the Dataset base class now makes loading custom datasets incredibly easy and avoids having to write all that boilerplate code ourselves for every new dataset.
88 | 
89 | !!! caution
90 | 
91 |     We did not populate `_test` attribute in the above code, which is fine and won't cause any unnecessary error as such. However it'll give you an error if you try to access the test split.
92 | 
93 |     ```python
94 |     dataset.test[:5]
95 |     ```
96 |     ****
97 |     ```text
98 |     ---------------------------------------------------------------------------
99 |     AttributeError                            Traceback (most recent call last)
100 |     <ipython-input-59-5202f6de3c7b> in <cell line: 1>()
101 |     ----> 1 dataset.test[:5]
102 | 
103 |     /usr/local/lib/python3.10/dist-packages/dspy/datasets/dataset.py in test(self)
104 |         51     def test(self):
105 |         52         if not hasattr(self, '_test_'):
106 |     ---> 53             self._test_ = self._shuffle_and_sample('test', self._test, self.test_size, self.test_seed)
107 |         54 
108 |         55         return self._test_
109 | 
110 |     AttributeError: 'CSVDataset' object has no attribute '_test'
111 |     ```
112 | 
113 |     To prevent that you'll just need to make sure `_test` is not `None` and populated with the appropriate data.
114 | 
115 | You can override the methods in `Dataset` class to customize your class even more. 
116 | 
117 | In summary, the Dataset base class provides a simplistic way to load and preprocess custom datasets with minimal code!
```

learn/evaluation/data.md
```
1 | ---
2 | sidebar_position: 5
3 | ---
4 | 
5 | # Data
6 | 
7 | DSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets. For each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.
8 | 
9 | 
10 | ## DSPy `Example` objects
11 | 
12 | The core data type for data in DSPy is `Example`. You will use **Examples** to represent items in your training set and test set. 
13 | 
14 | DSPy **Examples** are similar to Python `dict`s but have a few useful utilities. Your DSPy modules will return values of the type `Prediction`, which is a special sub-class of `Example`.
15 | 
16 | When you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type `Example`:
17 | 
18 | ```python
19 | qa_pair = dspy.Example(question="This is a question?", answer="This is an answer.")
20 | 
21 | print(qa_pair)
22 | print(qa_pair.question)
23 | print(qa_pair.answer)
24 | ```
25 | **Output:**
26 | ```text
27 | Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys=None)
28 | This is a question?
29 | This is an answer.
30 | ```
31 | 
32 | Examples can have any field keys and any value types, though usually values are strings.
33 | 
34 | ```text
35 | object = Example(field1=value1, field2=value2, field3=value3, ...)
36 | ```
37 | 
38 | You can now express your training set for example as:
39 | 
40 | ```python
41 | trainset = [dspy.Example(report="LONG REPORT 1", summary="short summary 1"), ...]
42 | ```
43 | 
44 | 
45 | ### Specifying Input Keys
46 | 
47 | In traditional ML, there are separated "inputs" and "labels".
48 | 
49 | In DSPy, the `Example` objects have a `with_inputs()` method, which can mark specific fields as inputs. (The rest are just metadata or labels.)
50 | 
51 | ```python
52 | # Single Input.
53 | print(qa_pair.with_inputs("question"))
54 | 
55 | # Multiple Inputs; be careful about marking your labels as inputs unless you mean it.
56 | print(qa_pair.with_inputs("question", "answer"))
57 | ```
58 | 
59 | Values can be accessed using the `.`(dot) operator. You can access the value of key `name` in defined object `Example(name="John Doe", job="sleep")` through `object.name`. 
60 | 
61 | To access or exclude certain keys, use `inputs()` and `labels()` methods to return new Example objects containing only input or non-input keys, respectively.
62 | 
63 | ```python
64 | article_summary = dspy.Example(article= "This is an article.", summary= "This is a summary.").with_inputs("article")
65 | 
66 | input_key_only = article_summary.inputs()
67 | non_input_key_only = article_summary.labels()
68 | 
69 | print("Example object with Input fields only:", input_key_only)
70 | print("Example object with Non-Input fields only:", non_input_key_only)
71 | ```
72 | 
73 | **Output**
74 | ```
75 | Example object with Input fields only: Example({'article': 'This is an article.'}) (input_keys={'article'})
76 | Example object with Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)
77 | ```
78 | 
79 | <!-- ## Loading Dataset from sources
80 | 
81 | One of the most convenient way to import datasets in DSPy is by using `DataLoader`. The first step is to declare an object, this object can then be used to call utilities to load datasets in different formats:
82 | 
83 | ```python
84 | from dspy.datasets import DataLoader
85 | 
86 | dl = DataLoader()
87 | ```
88 | 
89 | For most dataset formats, it's quite straightforward you pass the file path to the corresponding method of the format and you'll get the list of `Example` for the dataset in return:
90 | 
91 | ```python
92 | import pandas as pd
93 | 
94 | csv_dataset = dl.from_csv(
95 |     "sample_dataset.csv",
96 |     fields=("instruction", "context", "response"),
97 |     input_keys=("instruction", "context")
98 | )
99 | 
100 | json_dataset = dl.from_json(
101 |     "sample_dataset.json",
102 |     fields=("instruction", "context", "response"),
103 |     input_keys=("instruction", "context")
104 | )
105 | 
106 | parquet_dataset = dl.from_parquet(
107 |     "sample_dataset.parquet",
108 |     fields=("instruction", "context", "response"),
109 |     input_keys=("instruction", "context")
110 | )
111 | 
112 | pandas_dataset = dl.from_pandas(
113 |     pd.read_csv("sample_dataset.csv"),    # DataFrame
114 |     fields=("instruction", "context", "response"),
115 |     input_keys=("instruction", "context")
116 | )
117 | ```
118 | 
119 | These are some formats that `DataLoader` supports to load from file directly. In the backend, most of these methods leverage the `load_dataset` method from `datasets` library to load these formats. But when working with text data you often use HuggingFace datasets, in order to import HF datasets in list of `Example` format we can use `from_huggingface` method:
120 | 
121 | ```python
122 | blog_alpaca = dl.from_huggingface(
123 |     "intertwine-expel/expel-blog",
124 |     input_keys=("title",)
125 | )
126 | ```
127 | 
128 | You can access the splits of the dataset by accessing the corresponding key:
129 | 
130 | ```python
131 | train_split = blog_alpaca['train']
132 | 
133 | # Since this is the only split in the dataset we can split this into 
134 | # train and test split ourselves by slicing or sampling 75 rows from the train
135 | # split for testing.
136 | testset = train_split[:75]
137 | trainset = train_split[75:]
138 | ```
139 | 
140 | The way you load a HuggingFace dataset using `load_dataset` is exactly how you load data it via `from_huggingface` as well. This includes passing specific splits, subsplits, read instructions, etc. For code snippets, you can refer to the [cheatsheet snippets](/cheatsheet/#dspy-dataloaders) for loading from HF. -->
```

learn/evaluation/metrics.md
```
1 | ---
2 | sidebar_position: 5
3 | ---
4 | 
5 | # Metrics
6 | 
7 | DSPy is a machine learning framework, so you must think about your **automatic metrics** for evaluation (to track your progress) and optimization (so DSPy can make your programs more effective).
8 | 
9 | 
10 | ## What is a metric and how do I define a metric for my task?
11 | 
12 | A metric is just a function that will take examples from your data and the output of your system and return a score that quantifies how good the output is. What makes outputs from your system good or bad? 
13 | 
14 | For simple tasks, this could be just "accuracy" or "exact match" or "F1 score". This may be the case for simple classification or short-form QA tasks.
15 | 
16 | However, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (quite possibly using AI feedback from LMs).
17 | 
18 | Getting this right on the first try is unlikely, but you should start with something simple and iterate. 
19 | 
20 | 
21 | ## Simple metrics
22 | 
23 | A DSPy metric is just a function in Python that takes `example` (e.g., from your training or dev set) and the output `pred` from your DSPy program, and outputs a `float` (or `int` or `bool`) score.
24 | 
25 | Your metric should also accept an optional third argument called `trace`. You can ignore this for a moment, but it will enable some powerful tricks if you want to use your metric for optimization.
26 | 
27 | Here's a simple example of a metric that's comparing `example.answer` and `pred.answer`. This particular metric will return a `bool`.
28 | 
29 | ```python
30 | def validate_answer(example, pred, trace=None):
31 |     return example.answer.lower() == pred.answer.lower()
32 | ```
33 | 
34 | Some people find these utilities (built-in) convenient:
35 | 
36 | - `dspy.evaluate.metrics.answer_exact_match`
37 | - `dspy.evaluate.metrics.answer_passage_match`
38 | 
39 | Your metrics could be more complex, e.g. check for multiple properties. The metric below will return a `float` if `trace is None` (i.e., if it's used for evaluation or optimization), and will return a `bool` otherwise (i.e., if it's used to bootstrap demonstrations).
40 | 
41 | ```python
42 | def validate_context_and_answer(example, pred, trace=None):
43 |     # check the gold label and the predicted answer are the same
44 |     answer_match = example.answer.lower() == pred.answer.lower()
45 | 
46 |     # check the predicted answer comes from one of the retrieved contexts
47 |     context_match = any((pred.answer.lower() in c) for c in pred.context)
48 | 
49 |     if trace is None: # if we're doing evaluation or optimization
50 |         return (answer_match + context_match) / 2.0
51 |     else: # if we're doing bootstrapping, i.e. self-generating good demonstrations of each step
52 |         return answer_match and context_match
53 | ```
54 | 
55 | Defining a good metric is an iterative process, so doing some initial evaluations and looking at your data and outputs is key.
56 | 
57 | 
58 | ## Evaluation
59 | 
60 | Once you have a metric, you can run evaluations in a simple Python loop.
61 | 
62 | ```python
63 | scores = []
64 | for x in devset:
65 |     pred = program(**x.inputs())
66 |     score = metric(x, pred)
67 |     scores.append(score)
68 | ```
69 | 
70 | If you need some utilities, you can also use the built-in `Evaluate` utility. It can help with things like parallel evaluation (multiple threads) or showing you a sample of inputs/outputs and the metric scores.
71 | 
72 | ```python
73 | from dspy.evaluate import Evaluate
74 | 
75 | # Set up the evaluator, which can be re-used in your code.
76 | evaluator = Evaluate(devset=YOUR_DEVSET, num_threads=1, display_progress=True, display_table=5)
77 | 
78 | # Launch evaluation.
79 | evaluator(YOUR_PROGRAM, metric=YOUR_METRIC)
80 | ```
81 | 
82 | 
83 | ## Intermediate: Using AI feedback for your metric
84 | 
85 | For most applications, your system will output long-form outputs, so your metric should check multiple dimensions of the output using AI feedback from LMs.
86 | 
87 | This simple signature could come in handy.
88 | 
89 | ```python
90 | # Define the signature for automatic assessments.
91 | class Assess(dspy.Signature):
92 |     """Assess the quality of a tweet along the specified dimension."""
93 | 
94 |     assessed_text = dspy.InputField()
95 |     assessment_question = dspy.InputField()
96 |     assessment_answer: bool = dspy.OutputField()
97 | ```
98 | 
99 | For example, below is a simple metric that checks a generated tweet (1) answers a given question correctly and (2) whether it's also engaging. We also check that (3) `len(tweet) <= 280` characters.
100 | 
101 | ```python
102 | def metric(gold, pred, trace=None):
103 |     question, answer, tweet = gold.question, gold.answer, pred.output
104 | 
105 |     engaging = "Does the assessed text make for a self-contained, engaging tweet?"
106 |     correct = f"The text should answer `{question}` with `{answer}`. Does the assessed text contain this answer?"
107 |     
108 |     correct =  dspy.Predict(Assess)(assessed_text=tweet, assessment_question=correct)
109 |     engaging = dspy.Predict(Assess)(assessed_text=tweet, assessment_question=engaging)
110 | 
111 |     correct, engaging = [m.assessment_answer for m in [correct, engaging]]
112 |     score = (correct + engaging) if correct and (len(tweet) <= 280) else 0
113 | 
114 |     if trace is not None: return score >= 2
115 |     return score / 2.0
116 | ```
117 | 
118 | When compiling, `trace is not None`, and we want to be strict about judging things, so we will only return `True` if `score >= 2`. Otherwise, we return a score out of 1.0 (i.e., `score / 2.0`).
119 | 
120 | 
121 | ## Advanced: Using a DSPy program as your metric
122 | 
123 | If your metric is itself a DSPy program, one of the most powerful ways to iterate is to compile (optimize) your metric itself. That's usually easy because the output of the metric is usually a simple value (e.g., a score out of 5) so the metric's metric is easy to define and optimize by collecting a few examples.
124 | 
125 | 
126 | 
127 | ### Advanced: Accessing the `trace`
128 | 
129 | When your metric is used during evaluation runs, DSPy will not try to track the steps of your program.
130 | 
131 | But during compiling (optimization), DSPy will trace your LM calls. The trace will contain inputs/outputs to each DSPy predictor and you can leverage that to validate intermediate steps for optimization.
132 | 
133 | 
134 | ```python
135 | def validate_hops(example, pred, trace=None):
136 |     hops = [example.question] + [outputs.query for *_, outputs in trace if 'query' in outputs]
137 | 
138 |     if max([len(h) for h in hops]) > 100: return False
139 |     if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))): return False
140 | 
141 |     return True
142 | ```
```

learn/evaluation/overview.md
```
1 | ---
2 | sidebar_position: 1
3 | ---
4 | 
5 | # Evaluation in DSPy
6 | 
7 | Once you have an initial system, it's time to **collect an initial development set** so you can refine it more systematically. Even 20 input examples of your task can be useful, though 200 goes a long way. Depending on your _metric_, you either just need inputs and no labels at all, or you need inputs and the _final_ outputs of your system. (You almost never need labels for the intermediate steps in your program in DSPy.) You can probably find datasets that are adjacent to your task on, say, HuggingFace datasets or in a naturally occurring source like StackExchange. If there's data whose licenses are permissive enough, we suggest you use them. Otherwise, you can label a few examples by hand or start deploying a demo of your system and collect initial data that way.
8 | 
9 | Next, you should **define your DSPy metric**. What makes outputs from your system good or bad? Invest in defining metrics and improving them incrementally over time; it's hard to consistently improve what you aren't able to define. A metric is a function that takes examples from your data and takes the output of your system, and returns a score. For simple tasks, this could be just "accuracy", e.g. for simple classification or short-form QA tasks. For most applications, your system will produce long-form outputs, so your metric will be a smaller DSPy program that checks multiple properties of the output. Getting this right on the first try is unlikely: start with something simple and iterate.
10 | 
11 | Now that you have some data and a metric, run development evaluations on your pipeline designs to understand their tradeoffs. Look at the outputs and the metric scores. This will probably allow you to spot any major issues, and it will define a baseline for your next steps.
12 | 
13 | 
14 | ??? "If your metric is itself a DSPy program..."
15 |     If your metric is itself a DSPy program, a powerful way to iterate is to optimize your metric itself. That's usually easy because the output of the metric is usually a simple value (e.g., a score out of 5), so the metric's metric is easy to define and optimize by collecting a few examples.
16 | 
```

learn/optimization/optimizers.md
```
1 | ---
2 | sidebar_position: 1
3 | ---
4 | 
5 | # DSPy Optimizers (formerly Teleprompters)
6 | 
7 | 
8 | A **DSPy optimizer** is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy.
9 | 
10 | 
11 | A typical DSPy optimizer takes three things:
12 | 
13 | - Your **DSPy program**. This may be a single module (e.g., `dspy.Predict`) or a complex multi-module program.
14 | 
15 | - Your **metric**. This is a function that evaluates the output of your program, and assigns it a score (higher is better).
16 | 
17 | - A few **training inputs**. This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels).
18 | 
19 | If you happen to have a lot of data, DSPy can leverage that. But you can start small and get strong results.
20 | 
21 | **Note:** Formerly called teleprompters. We are making an official name update, which will be reflected throughout the library and documentation.
22 | 
23 | 
24 | ## What does a DSPy Optimizer tune? How does it tune them?
25 | 
26 | Different optimizers in DSPy will tune your program's quality by **synthesizing good few-shot examples** for every module, like `dspy.BootstrapRS`,<sup>[1](https://arxiv.org/abs/2310.03714)</sup> **proposing and intelligently exploring better natural-language instructions** for every prompt, like `dspy.MIPROv2`,<sup>[2](https://arxiv.org/abs/2406.11695)</sup> and `dspy.GEPA`,<sup>[3](https://arxiv.org/abs/2507.19457)</sup> and **building datasets for your modules and using them to finetune the LM weights** in your system, like `dspy.BootstrapFinetune`.<sup>[4](https://arxiv.org/abs/2407.10930)</sup>
27 | 
28 | ??? "What's an example of a DSPy optimizer? How do different optimizers work?"
29 | 
30 |     Take the `dspy.MIPROv2` optimizer as an example. First, MIPRO starts with the **bootstrapping stage**. It takes your program, which may be unoptimized at this point, and runs it many times across different inputs to collect traces of input/output behavior for each one of your modules. It filters these traces to keep only those that appear in trajectories scored highly by your metric. Second, MIPRO enters its **grounded proposal stage**. It previews your DSPy program's code, your data, and traces from running your program, and uses them to draft many potential instructions for every prompt in your program. Third, MIPRO launches the **discrete search stage**. It samples mini-batches from your training set, proposes a combination of instructions and traces to use for constructing every prompt in the pipeline, and evaluates the candidate program on the mini-batch. Using the resulting score, MIPRO updates a surrogate model that helps the proposals get better over time.
31 | 
32 |     One thing that makes DSPy optimizers so powerful is that they can be composed. You can run `dspy.MIPROv2` and use the produced program as an input to `dspy.MIPROv2` again or, say, to `dspy.BootstrapFinetune` to get better results. This is partly the essence of `dspy.BetterTogether`. Alternatively, you can run the optimizer and then extract the top-5 candidate programs and build a `dspy.Ensemble` of them. This allows you to scale _inference-time compute_ (e.g., ensembles) as well as DSPy's unique _pre-inference time compute_ (i.e., optimization budget) in highly systematic ways.
33 | 
34 | 
35 | 
36 | ## What DSPy Optimizers are currently available?
37 | 
38 | Optimizers can be accessed via `from dspy.teleprompt import *`.
39 | 
40 | ### Automatic Few-Shot Learning
41 | 
42 | These optimizers extend the signature by automatically generating and including **optimized** examples within the prompt sent to the model, implementing few-shot learning.
43 | 
44 | 1. [**`LabeledFewShot`**](../../api/optimizers/LabeledFewShot.md): Simply constructs few-shot examples (demos) from provided labeled input and output data points.  Requires `k` (number of examples for the prompt) and `trainset` to randomly select `k` examples from.
45 | 
46 | 2. [**`BootstrapFewShot`**](../../api/optimizers/BootstrapFewShot.md): Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program, along with labeled examples in `trainset`. Parameters include `max_labeled_demos` (the number of demonstrations randomly selected from the `trainset`) and `max_bootstrapped_demos` (the number of additional examples generated by the `teacher`). The bootstrapping process employs the metric to validate demonstrations, including only those that pass the metric in the "compiled" prompt. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.
47 | 
48 | 3. [**`BootstrapFewShotWithRandomSearch`**](../../api/optimizers/BootstrapFewShotWithRandomSearch.md): Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program over the optimization. Parameters mirror those of `BootstrapFewShot`, with the addition of `num_candidate_programs`, which specifies the number of random programs evaluated over the optimization, including candidates of the uncompiled program, `LabeledFewShot` optimized program, `BootstrapFewShot` compiled program with unshuffled examples and `num_candidate_programs` of `BootstrapFewShot` compiled programs with randomized example sets.
49 | 
50 | 4. [**`KNNFewShot`**](../../api/optimizers/KNNFewShot.md). Uses k-Nearest Neighbors algorithm to find the nearest training example demonstrations for a given input example. These nearest neighbor demonstrations are then used as the trainset for the BootstrapFewShot optimization process.
51 | 
52 | 
53 | ### Automatic Instruction Optimization
54 | 
55 | These optimizers produce optimal instructions for the prompt and, in the case of MIPROv2 can also optimize the set of few-shot demonstrations.
56 | 
57 | 5. [**`COPRO`**](../../api/optimizers/COPRO.md): Generates and refines new instructions for each step, and optimizes them with coordinate ascent (hill-climbing using the metric function and the `trainset`). Parameters include `depth` which is the number of iterations of prompt improvement the optimizer runs over.
58 | 
59 | 6. [**`MIPROv2`**](../../api/optimizers/MIPROv2.md): Generates instructions *and* few-shot examples in each step. The instruction generation is data-aware and demonstration-aware. Uses Bayesian Optimization to effectively search over the space of generation instructions/demonstrations across your modules.
60 | 
61 | 7. [**`SIMBA`**](../../api/optimizers/SIMBA.md)
62 | 
63 | 8. [**`GEPA`**](../../api/optimizers/GEPA/overview.md): Uses LM's to reflect on the DSPy program's trajectory, to identify what worked, what didn't and propose prompts addressing the gaps. Additionally, GEPA can leverage domain-specific textual feedback to rapidly improve the DSPy program. Detailed tutorials on using GEPA are available at [dspy.GEPA Tutorials](../../tutorials/gepa_ai_program/index.md).
64 | 
65 | ### Automatic Finetuning
66 | 
67 | This optimizer is used to fine-tune the underlying LLM(s).
68 | 
69 | 9. [**`BootstrapFinetune`**](/api/optimizers/BootstrapFinetune): Distills a prompt-based DSPy program into weight updates. The output is a DSPy program that has the same steps, but where each step is conducted by a finetuned model instead of a prompted LM. [See the classification fine-tuning tutorial](https://dspy.ai/tutorials/classification_finetuning/) for a complete example.
70 | 
71 | 
72 | ### Program Transformations
73 | 
74 | 10. [**`Ensemble`**](../../api/optimizers/Ensemble.md): Ensembles a set of DSPy programs and either uses the full set or randomly samples a subset into a single program.
75 | 
76 | 
77 | ## Which optimizer should I use?
78 | 
79 | Ultimately, finding the ‘right’ optimizer to use & the best configuration for your task will require experimentation. Success in DSPy is still an iterative process - getting the best performance on your task will require you to explore and iterate.  
80 | 
81 | That being said, here's the general guidance on getting started:
82 | 
83 | - If you have **very few examples** (around 10), start with `BootstrapFewShot`.
84 | - If you have **more data** (50 examples or more), try  `BootstrapFewShotWithRandomSearch`.
85 | - If you prefer to do **instruction optimization only** (i.e. you want to keep your prompt 0-shot), use `MIPROv2` [configured for 0-shot optimization](../../api/optimizers/MIPROv2.md). 
86 | - If you’re willing to use more inference calls to perform **longer optimization runs** (e.g. 40 trials or more), and have enough data (e.g. 200 examples or more to prevent overfitting) then try `MIPROv2`. 
87 | - If you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very **efficient program**, finetune a small LM for your task with `BootstrapFinetune`.
88 | 
89 | ## How do I use an optimizer?
90 | 
91 | They all share this general interface, with some differences in the keyword arguments (hyperparameters). A full list can be found in the [API reference](../../api/optimizers/BetterTogether.md).
92 | 
93 | Let's see this with the most common one, `BootstrapFewShotWithRandomSearch`.
94 | 
95 | ```python
96 | from dspy.teleprompt import BootstrapFewShotWithRandomSearch
97 | 
98 | # Set up the optimizer: we want to "bootstrap" (i.e., self-generate) 8-shot examples of your program's steps.
99 | # The optimizer will repeat this 10 times (plus some initial attempts) before selecting its best attempt on the devset.
100 | config = dict(max_bootstrapped_demos=4, max_labeled_demos=4, num_candidate_programs=10, num_threads=4)
101 | 
102 | teleprompter = BootstrapFewShotWithRandomSearch(metric=YOUR_METRIC_HERE, **config)
103 | optimized_program = teleprompter.compile(YOUR_PROGRAM_HERE, trainset=YOUR_TRAINSET_HERE)
104 | ```
105 | 
106 | 
107 | !!! info "Getting Started III: Optimizing the LM prompts or weights in DSPy programs"
108 |     A typical simple optimization run costs on the order of $2 USD and takes around ten minutes, but be careful when running optimizers with very large LMs or very large datasets.
109 |     Optimizer runs can cost as little as a few cents or up to tens of dollars, depending on your LM, dataset, and configuration.
110 |     
111 |     === "Optimizing prompts for a ReAct agent"
112 |         This is a minimal but fully runnable example of setting up a `dspy.ReAct` agent that answers questions via
113 |         search from Wikipedia and then optimizing it using `dspy.MIPROv2` in the cheap `light` mode on 500
114 |         question-answer pairs sampled from the `HotPotQA` dataset.
115 | 
116 |         ```python linenums="1"
117 |         import dspy
118 |         from dspy.datasets import HotPotQA
119 | 
120 |         dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))
121 | 
122 |         def search(query: str) -> list[str]:
123 |             """Retrieves abstracts from Wikipedia."""
124 |             results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)
125 |             return [x['text'] for x in results]
126 | 
127 |         trainset = [x.with_inputs('question') for x in HotPotQA(train_seed=2024, train_size=500).train]
128 |         react = dspy.ReAct("question -> answer", tools=[search])
129 | 
130 |         tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto="light", num_threads=24)
131 |         optimized_react = tp.compile(react, trainset=trainset)
132 |         ```
133 | 
134 |         An informal run similar to this on DSPy 2.5.29 raises ReAct's score from 24% to 51%.
135 | 
136 |     === "Optimizing prompts for RAG"
137 |         Given a retrieval index to `search`, your favorite `dspy.LM`, and a small `trainset` of questions and ground-truth responses, the following code snippet can optimize your RAG system with long outputs against the built-in `dspy.SemanticF1` metric, which is implemented as a DSPy module.
138 | 
139 |         ```python linenums="1"
140 |         class RAG(dspy.Module):
141 |             def __init__(self, num_docs=5):
142 |                 self.num_docs = num_docs
143 |                 self.respond = dspy.ChainOfThought('context, question -> response')
144 | 
145 |             def forward(self, question):
146 |                 context = search(question, k=self.num_docs)   # not defined in this snippet, see link above
147 |                 return self.respond(context=context, question=question)
148 | 
149 |         tp = dspy.MIPROv2(metric=dspy.SemanticF1(), auto="medium", num_threads=24)
150 |         optimized_rag = tp.compile(RAG(), trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2)
151 |         ```
152 | 
153 |         For a complete RAG example that you can run, start this [tutorial](../../tutorials/rag/index.ipynb). It improves the quality of a RAG system over a subset of StackExchange communities from 53% to 61%.
154 | 
155 |     === "Optimizing weights for Classification"
156 |         <details><summary>Click to show dataset setup code.</summary>
157 | 
158 |         ```python linenums="1"
159 |         import random
160 |         from typing import Literal
161 | 
162 |         from datasets import load_dataset
163 | 
164 |         import dspy
165 |         from dspy.datasets import DataLoader
166 | 
167 |         # Load the Banking77 dataset.
168 |         CLASSES = load_dataset("PolyAI/banking77", split="train", trust_remote_code=True).features["label"].names
169 |         kwargs = {"fields": ("text", "label"), "input_keys": ("text",), "split": "train", "trust_remote_code": True}
170 | 
171 |         # Load the first 2000 examples from the dataset, and assign a hint to each *training* example.
172 |         trainset = [
173 |             dspy.Example(x, hint=CLASSES[x.label], label=CLASSES[x.label]).with_inputs("text", "hint")
174 |             for x in DataLoader().from_huggingface(dataset_name="PolyAI/banking77", **kwargs)[:2000]
175 |         ]
176 |         random.Random(0).shuffle(trainset)
177 |         ```
178 |         </details>
179 | 
180 |         ```python linenums="1"
181 |         import dspy
182 |         lm=dspy.LM('openai/gpt-4o-mini-2024-07-18')
183 | 
184 |         # Define the DSPy module for classification. It will use the hint at training time, if available.
185 |         signature = dspy.Signature("text, hint -> label").with_updated_fields('label', type_=Literal[tuple(CLASSES)])
186 |         classify = dspy.ChainOfThought(signature)
187 |         classify.set_lm(lm)
188 | 
189 |         # Optimize via BootstrapFinetune.
190 |         optimizer = dspy.BootstrapFinetune(metric=(lambda x, y, trace=None: x.label == y.label), num_threads=24)
191 |         optimized = optimizer.compile(classify, trainset=trainset)
192 | 
193 |         optimized(text="What does a pending cash withdrawal mean?")
194 |         
195 |         # For a complete fine-tuning tutorial, see: https://dspy.ai/tutorials/classification_finetuning/
196 |         ```
197 | 
198 |         **Possible Output (from the last line):**
199 |         ```text
200 |         Prediction(
201 |             reasoning='A pending cash withdrawal indicates that a request to withdraw cash has been initiated but has not yet been completed or processed. This status means that the transaction is still in progress and the funds have not yet been deducted from the account or made available to the user.',
202 |             label='pending_cash_withdrawal'
203 |         )
204 |         ```
205 | 
206 |         An informal run similar to this on DSPy 2.5.29 raises GPT-4o-mini's score 66% to 87%.
207 | 
208 | 
209 | ## Saving and loading optimizer output
210 | 
211 | After running a program through an optimizer, it's useful to also save it. At a later point, a program can be loaded from a file and used for inference. For this, the `load` and `save` methods can be used.
212 | 
213 | ```python
214 | optimized_program.save(YOUR_SAVE_PATH)
215 | ```
216 | 
217 | The resulting file is in plain-text JSON format. It contains all the parameters and steps in the source program. You can always read it and see what the optimizer generated.
218 | 
219 | 
220 | To load a program from a file, you can instantiate an object from that class and then call the load method on it.
221 | 
222 | ```python
223 | loaded_program = YOUR_PROGRAM_CLASS()
224 | loaded_program.load(path=YOUR_SAVE_PATH)
225 | ```
226 | 
```

learn/optimization/overview.md
```
1 | ---
2 | sidebar_position: 1
3 | ---
4 | 
5 | 
6 | # Optimization in DSPy
7 | 
8 | Once you have a system and a way to evaluate it, you can use DSPy optimizers to tune the prompts or weights in your program. Now it's useful to expand your data collection effort into building a training set and a held-out test set, in addition to the development set you've been using for exploration. For the training set (and its subset, validation set), you can often get substantial value out of 30 examples, but aim for at least 300 examples. Some optimizers accept a `trainset` only. Others ask for a `trainset` and a `valset`. When splitting data for most prompt optimizers, we recommend an unusual split compared to deep neural networks: 20% for training, 80% for validation. This reverse allocation emphasizes stable validation, since prompt-based optimizers often overfit to small training sets. In contrast, the [dspy.GEPA](https://dspy.ai/tutorials/gepa_ai_program/) optimizer follows the more standard ML convention: Maximize the training set size, while keeping the validation set just large enough to reflect the distribution of the downstream tasks (test set).
9 | 
10 | After your first few optimization runs, you are either very happy with everything or you've made a lot of progress but you don't like something about the final program or the metric. At this point, go back to step 1 (Programming in DSPy) and revisit the major questions. Did you define your task well? Do you need to collect (or find online) more data for your problem? Do you want to update your metric? And do you want to use a more sophisticated optimizer? Do you need to consider advanced features like DSPy Assertions? Or, perhaps most importantly, do you want to add some more complexity or steps in your DSPy program itself? Do you want to use multiple optimizers in a sequence?
11 | 
12 | Iterative development is key. DSPy gives you the pieces to do that incrementally: iterating on your data, your program structure, your metric, and your optimization steps. Optimizing complex LM programs is an entirely new paradigm that only exists in DSPy at the time of writing (update: there are now numerous DSPy extension frameworks, so this part is no longer true :-), so naturally the norms around what to do are still emerging. If you need help, we recently created a [Discord server](https://discord.gg/XCGy2WDCQB) for the community.
13 | 
```

learn/programming/7-assertions.md
```
1 | # DSPy Assertions 
2 | 
3 | !!! warning "Assertions are deprecated and NOT supported. Please use the `dspy.Refine` module instead. (or dspy.Suggest)."
4 | 
5 | The content below is deprecated, and is scheduled to be removed.
6 | 
7 | ## Introduction
8 | 
9 | Language models (LMs) have transformed how we interact with machine learning, offering vast capabilities in natural language understanding and generation. However, ensuring these models adhere to domain-specific constraints remains a challenge. Despite the growth of techniques like fine-tuning or “prompt engineering”, these approaches are extremely tedious and rely on heavy, manual hand-waving to guide the LMs in adhering to specific constraints. Even DSPy's modularity of programming prompting pipelines lacks mechanisms to effectively and automatically enforce these constraints. 
10 | 
11 | To address this, we introduce DSPy Assertions, a feature within the DSPy framework designed to automate the enforcement of computational constraints on LMs. DSPy Assertions empower developers to guide LMs towards desired outcomes with minimal manual intervention, enhancing the reliability, predictability, and correctness of LM outputs.
12 | 
13 | ### dspy.Assert and dspy.Suggest API    
14 | 
15 | We introduce two primary constructs within DSPy Assertions:
16 | 
17 | - **`dspy.Assert`**:
18 |   - **Parameters**: 
19 |     - `constraint (bool)`: Outcome of Python-defined boolean validation check.
20 |     - `msg (Optional[str])`: User-defined error message providing feedback or correction guidance.
21 |     - `backtrack (Optional[module])`: Specifies target module for retry attempts upon constraint failure. The default backtracking module is the last module before the assertion.
22 |   - **Behavior**: Initiates retry  upon failure, dynamically adjusting the pipeline's execution. If failures persist, it halts execution and raises a `dspy.AssertionError`.
23 | 
24 | - **`dspy.Suggest`**:
25 |   - **Parameters**: Similar to `dspy.Assert`.
26 |   - **Behavior**: Encourages self-refinement through retries without enforcing hard stops. Logs failures after maximum backtracking attempts and continues execution.
27 | 
28 | - **dspy.Assert vs. Python Assertions**: Unlike conventional Python `assert` statements that terminate the program upon failure, `dspy.Assert` conducts a sophisticated retry mechanism, allowing the pipeline to adjust. 
29 | 
30 | Specifically, when a constraint is not met:
31 | 
32 | - Backtracking Mechanism: An under-the-hood backtracking is initiated, offering the model a chance to self-refine and proceed, which is done through signature modification.
33 | - Dynamic Signature Modification: internally modifying your DSPy program’s Signature by adding the following fields:
34 |     - Past Output: your model's past output that did not pass the validation_fn
35 |     - Instruction: your user-defined feedback message on what went wrong and what possibly to fix
36 | 
37 | If the error continues past the `max_backtracking_attempts`, then `dspy.Assert` will halt the pipeline execution, alerting you with an `dspy.AssertionError`. This ensures your program doesn't continue executing with “bad” LM behavior and immediately highlights sample failure outputs for user assessment.
38 | 
39 | - **dspy.Suggest vs. dspy.Assert**: `dspy.Suggest` on the other hand offers a softer approach. It maintains the same retry backtracking as `dspy.Assert` but instead serves as a gentle nudger. If the model outputs cannot pass the model constraints after the `max_backtracking_attempts`, `dspy.Suggest` will log the persistent failure and continue execution of the program on the rest of the data. This ensures the LM pipeline works in a "best-effort" manner without halting execution. 
40 | 
41 | - **`dspy.Suggest`** statements are best utilized as "helpers" during the evaluation phase, offering guidance and potential corrections without halting the pipeline.
42 | - **`dspy.Assert`** statements are recommended during the development stage as "checkers" to ensure the LM behaves as expected, providing a robust mechanism for identifying and addressing errors early in the development cycle.
43 | 
44 | 
45 | ## Use Case: Including Assertions in DSPy Programs
46 | 
47 | We start with using an example of a multi-hop QA SimplifiedBaleen pipeline as defined in the intro walkthrough. 
48 | 
49 | ```python
50 | class SimplifiedBaleen(dspy.Module):
51 |     def __init__(self, passages_per_hop=2, max_hops=2):
52 |         super().__init__()
53 | 
54 |         self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]
55 |         self.retrieve = dspy.Retrieve(k=passages_per_hop)
56 |         self.generate_answer = dspy.ChainOfThought(GenerateAnswer)
57 |         self.max_hops = max_hops
58 | 
59 |     def forward(self, question):
60 |         context = []
61 |         prev_queries = [question]
62 | 
63 |         for hop in range(self.max_hops):
64 |             query = self.generate_query[hop](context=context, question=question).query
65 |             prev_queries.append(query)
66 |             passages = self.retrieve(query).passages
67 |             context = deduplicate(context + passages)
68 |         
69 |         pred = self.generate_answer(context=context, question=question)
70 |         pred = dspy.Prediction(context=context, answer=pred.answer)
71 |         return pred
72 | 
73 | baleen = SimplifiedBaleen()
74 | 
75 | baleen(question = "Which award did Gary Zukav's first book receive?")
76 | ```
77 | 
78 | To include DSPy Assertions, we simply define our validation functions and declare our assertions following the respective model generation. 
79 | 
80 | For this use case, suppose we want to impose the following constraints:
81 |     1. Length - each query should be less than 100 characters
82 |     2. Uniqueness - each generated query should differ from previously-generated queries. 
83 |     
84 | We can define these validation checks as boolean functions:
85 | 
86 | ```python
87 | #simplistic boolean check for query length
88 | len(query) <= 100
89 | 
90 | #Python function for validating distinct queries
91 | def validate_query_distinction_local(previous_queries, query):
92 |     """check if query is distinct from previous queries"""
93 |     if previous_queries == []:
94 |         return True
95 |     if dspy.evaluate.answer_exact_match_str(query, previous_queries, frac=0.8):
96 |         return False
97 |     return True
98 | ```
99 | 
100 | We can declare these validation checks through `dspy.Suggest` statements (as we want to test the program in a best-effort demonstration). We want to keep these after the query generation `query = self.generate_query[hop](context=context, question=question).query`.
101 | 
102 | ```python
103 | dspy.Suggest(
104 |     len(query) <= 100,
105 |     "Query should be short and less than 100 characters",
106 |     target_module=self.generate_query
107 | )
108 | 
109 | dspy.Suggest(
110 |     validate_query_distinction_local(prev_queries, query),
111 |     "Query should be distinct from: "
112 |     + "; ".join(f"{i+1}) {q}" for i, q in enumerate(prev_queries)),
113 |     target_module=self.generate_query
114 | )
115 | ```
116 | 
117 | It is recommended to define a program with assertions separately than your original program if you are doing comparative evaluation for the effect of assertions. If not, feel free to set Assertions away!
118 | 
119 | Let's take a look at how the SimplifiedBaleen program will look with Assertions included:
120 | 
121 | ```python
122 | class SimplifiedBaleenAssertions(dspy.Module):
123 |     def __init__(self, passages_per_hop=2, max_hops=2):
124 |         super().__init__()
125 |         self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]
126 |         self.retrieve = dspy.Retrieve(k=passages_per_hop)
127 |         self.generate_answer = dspy.ChainOfThought(GenerateAnswer)
128 |         self.max_hops = max_hops
129 | 
130 |     def forward(self, question):
131 |         context = []
132 |         prev_queries = [question]
133 | 
134 |         for hop in range(self.max_hops):
135 |             query = self.generate_query[hop](context=context, question=question).query
136 | 
137 |             dspy.Suggest(
138 |                 len(query) <= 100,
139 |                 "Query should be short and less than 100 characters",
140 |                 target_module=self.generate_query
141 |             )
142 | 
143 |             dspy.Suggest(
144 |                 validate_query_distinction_local(prev_queries, query),
145 |                 "Query should be distinct from: "
146 |                 + "; ".join(f"{i+1}) {q}" for i, q in enumerate(prev_queries)),
147 |                 target_module=self.generate_query
148 |             )
149 | 
150 |             prev_queries.append(query)
151 |             passages = self.retrieve(query).passages
152 |             context = deduplicate(context + passages)
153 |         
154 |         if all_queries_distinct(prev_queries):
155 |             self.passed_suggestions += 1
156 | 
157 |         pred = self.generate_answer(context=context, question=question)
158 |         pred = dspy.Prediction(context=context, answer=pred.answer)
159 |         return pred
160 | ```
161 | 
162 | Now calling programs with DSPy Assertions requires one last step, and that is transforming the program to wrap it with internal assertions backtracking and Retry logic. 
163 | 
164 | ```python
165 | from dspy.primitives.assertions import assert_transform_module, backtrack_handler
166 | 
167 | baleen_with_assertions = assert_transform_module(SimplifiedBaleenAssertions(), backtrack_handler)
168 | 
169 | # backtrack_handler is parameterized over a few settings for the backtracking mechanism
170 | # To change the number of max retry attempts, you can do
171 | baleen_with_assertions_retry_once = assert_transform_module(SimplifiedBaleenAssertions(), 
172 |     functools.partial(backtrack_handler, max_backtracks=1))
173 | ```
174 | 
175 | Alternatively, you can also directly call `activate_assertions` on the program with `dspy.Assert/Suggest` statements using the default backtracking mechanism (`max_backtracks=2`):
176 | 
177 | ```python
178 | baleen_with_assertions = SimplifiedBaleenAssertions().activate_assertions()
179 | ```
180 | 
181 | Now let's take a look at the internal LM backtracking by inspecting the history of the LM query generations. Here we see that when a query fails to pass the validation check of being less than 100 characters, its internal `GenerateSearchQuery` signature is dynamically modified during the backtracking+Retry process to include the past query and the corresponding user-defined instruction: `"Query should be short and less than 100 characters"`.
182 | 
183 | 
184 | ```text
185 | Write a simple search query that will help answer a complex question.
186 | 
187 | ---
188 | 
189 | Follow the following format.
190 | 
191 | Context: may contain relevant facts
192 | 
193 | Question: ${question}
194 | 
195 | Reasoning: Let's think step by step in order to ${produce the query}. We ...
196 | 
197 | Query: ${query}
198 | 
199 | ---
200 | 
201 | Context:
202 | [1] «Kerry Condon | Kerry Condon (born 4 January 1983) is [...]»
203 | [2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was [...]»
204 | 
205 | Question: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of "Hamlet." ?
206 | 
207 | Reasoning: Let's think step by step in order to find the answer to this question. First, we need to identify the actress who played Ophelia in a Royal Shakespeare Company production of "Hamlet." Then, we need to find out if this actress also acted in the short film "The Shore."
208 | 
209 | Query: "actress who played Ophelia in Royal Shakespeare Company production of Hamlet" + "actress in short film The Shore"
210 | 
211 | 
212 | 
213 | Write a simple search query that will help answer a complex question.
214 | 
215 | ---
216 | 
217 | Follow the following format.
218 | 
219 | Context: may contain relevant facts
220 | 
221 | Question: ${question}
222 | 
223 | Past Query: past output with errors
224 | 
225 | Instructions: Some instructions you must satisfy
226 | 
227 | Query: ${query}
228 | 
229 | ---
230 | 
231 | Context:
232 | [1] «Kerry Condon | Kerry Condon (born 4 January 1983) is an Irish television and film actress, best known for her role as Octavia of the Julii in the HBO/BBC series "Rome," as Stacey Ehrmantraut in AMC's "Better Call Saul" and as the voice of F.R.I.D.A.Y. in various films in the Marvel Cinematic Universe. She is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of "Hamlet."»
233 | [2] «Corona Riccardo | Corona Riccardo (c. 1878October 15, 1917) was an Italian born American actress who had a brief Broadway stage career before leaving to become a wife and mother. Born in Naples she came to acting in 1894 playing a Mexican girl in a play at the Empire Theatre. Wilson Barrett engaged her for a role in his play "The Sign of the Cross" which he took on tour of the United States. Riccardo played the role of Ancaria and later played Berenice in the same play. Robert B. Mantell in 1898 who struck by her beauty also cast her in two Shakespeare plays, "Romeo and Juliet" and "Othello". Author Lewis Strang writing in 1899 said Riccardo was the most promising actress in America at the time. Towards the end of 1898 Mantell chose her for another Shakespeare part, Ophelia im Hamlet. Afterwards she was due to join Augustin Daly's Theatre Company but Daly died in 1899. In 1899 she gained her biggest fame by playing Iras in the first stage production of Ben-Hur.»
234 | 
235 | Question: Who acted in the shot film The Shore and is also the youngest actress ever to play Ophelia in a Royal Shakespeare Company production of "Hamlet." ?
236 | 
237 | Past Query: "actress who played Ophelia in Royal Shakespeare Company production of Hamlet" + "actress in short film The Shore"
238 | 
239 | Instructions: Query should be short and less than 100 characters
240 | 
241 | Query: "actress Ophelia RSC Hamlet" + "actress The Shore"
242 | 
243 | ```
244 | 
245 | 
246 | ## Assertion-Driven Optimizations
247 | 
248 | DSPy Assertions work with optimizations that DSPy offers, particularly with `BootstrapFewShotWithRandomSearch`, including the following settings:
249 | 
250 | - Compilation with Assertions
251 |     This includes assertion-driven example bootstrapping and counterexample bootstrapping during compilation. The teacher model for bootstrapping few-shot demonstrations can make use of DSPy Assertions to offer robust bootstrapped examples for the student model to learn from during inference. In this setting, the student model does not perform assertion aware optimizations (backtracking and retry) during inference.
252 | - Compilation + Inference with Assertions
253 |     -This includes assertion-driven optimizations in both compilation and inference. Now the teacher model offers assertion-driven examples but the student can further optimize with assertions of its own during inference time. 
254 | ```python
255 | teleprompter = BootstrapFewShotWithRandomSearch(
256 |     metric=validate_context_and_answer_and_hops,
257 |     max_bootstrapped_demos=max_bootstrapped_demos,
258 |     num_candidate_programs=6,
259 | )
260 | 
261 | #Compilation with Assertions
262 | compiled_with_assertions_baleen = teleprompter.compile(student = baleen, teacher = baleen_with_assertions, trainset = trainset, valset = devset)
263 | 
264 | #Compilation + Inference with Assertions
265 | compiled_baleen_with_assertions = teleprompter.compile(student=baleen_with_assertions, teacher = baleen_with_assertions, trainset=trainset, valset=devset)
266 | 
267 | ```
```

learn/programming/adapters.md
```
1 | # Understanding DSPy Adapters
2 | 
3 | ## What are Adapters?
4 | 
5 | Adapters are the bridge between `dspy.Predict` and the actual Language Model (LM). When you call a DSPy module, the
6 | adapter takes your signature, user inputs, and other attributes like `demos` (few-shot examples) and converts them
7 | into multi-turn messages that get sent to the LM.
8 | 
9 | The adapter system is responsible for:
10 | 
11 | - Translating DSPy signatures into system messages that define the task and request/response structure.
12 | - Formatting input data according to the request structure outlined in DSPy signatures.
13 | - Parsing LM responses back into structured DSPy outputs, such as `dspy.Prediction` instances.
14 | - Managing conversation history and function calls.
15 | - Converting pre-built DSPy types into LM prompt messages, e.g., `dspy.Tool`, `dspy.Image`, etc.
16 | 
17 | ## Configure Adapters
18 | 
19 | You can use `dspy.configure(adapter=...)` to choose the adapter for the entire Python process, or
20 | `with dspy.context(adapter=...):` to only affect a certain namespace.
21 | 
22 | If no adapter is specified in the DSPy workflow, each `dspy.Predict.__call__` defaults to using the `dspy.ChatAdapter`. Thus, the two code snippets below are equivalent:
23 | 
24 | ```python
25 | import dspy
26 | 
27 | dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
28 | 
29 | predict = dspy.Predict("question -> answer")
30 | result = predict(question="What is the capital of France?")
31 | ```
32 | 
33 | ```python
34 | import dspy
35 | 
36 | dspy.configure(
37 |     lm=dspy.LM("openai/gpt-4o-mini"),
38 |     adapter=dspy.ChatAdapter(),  # This is the default value
39 | )
40 | 
41 | predict = dspy.Predict("question -> answer")
42 | result = predict(question="What is the capital of France?")
43 | ```
44 | 
45 | ## Where Adapters Fit in the System
46 | 
47 | The flow works as follows:
48 | 
49 | 1. The user calls their DSPy agent, typically a `dspy.Module` with inputs.
50 | 2. The inner `dspy.Predict` is invoked to obtain the LM response.
51 | 3. `dspy.Predict` calls **Adapter.format()**, which converts its signature, inputs, and demos into multi-turn messages sent to the `dspy.LM`. `dspy.LM` is a thin wrapper around `litellm`, which communicates with the LM endpoint.
52 | 4. The LM receives the messages and generates a response.
53 | 5. **Adapter.parse()** converts the LM response into structured DSPy outputs, as specified in the signature.
54 | 6. The caller of `dspy.Predict` receives the parsed outputs.
55 | 
56 | You can explicitly call `Adapter.format()` to view the messages sent to the LM.
57 | 
58 | ```python
59 | # Simplified flow example
60 | signature = dspy.Signature("question -> answer")
61 | inputs = {"question": "What is 2+2?"}
62 | demos = [{"question": "What is 1+1?", "answer": "2"}]
63 | 
64 | adapter = dspy.ChatAdapter()
65 | print(adapter.format(signature, demos, inputs))
66 | ```
67 | 
68 | The output should resemble:
69 | 
70 | ```
71 | {'role': 'system', 'content': 'Your input fields are:\n1. `question` (str):\nYour output fields are:\n1. `answer` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## question ## ]]\n{question}\n\n[[ ## answer ## ]]\n{answer}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `question`, produce the fields `answer`.'}
72 | {'role': 'user', 'content': '[[ ## question ## ]]\nWhat is 1+1?'}
73 | {'role': 'assistant', 'content': '[[ ## answer ## ]]\n2\n\n[[ ## completed ## ]]\n'}
74 | {'role': 'user', 'content': '[[ ## question ## ]]\nWhat is 2+2?\n\nRespond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}
75 | ```
76 | 
77 | ## Types of Adapters
78 | 
79 | DSPy offers several adapter types, each tailored for specific use cases:
80 | 
81 | ### ChatAdapter
82 | 
83 | **ChatAdapter** is the default adapter and works with all language models. It uses a field-based format with special markers.
84 | 
85 | #### Format Structure
86 | 
87 | ChatAdapter uses `[[ ## field_name ## ]]` markers to delineate fields. For fields of non-primitive Python types, it includes the JSON schema of the type. Below, we use `dspy.inspect_history()` to display the formatted messages by `dspy.ChatAdapter` clearly.
88 | 
89 | ```python
90 | import dspy
91 | import pydantic
92 | 
93 | dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"), adapter=dspy.ChatAdapter())
94 | 
95 | 
96 | class ScienceNews(pydantic.BaseModel):
97 |     text: str
98 |     scientists_involved: list[str]
99 | 
100 | 
101 | class NewsQA(dspy.Signature):
102 |     """Get news about the given science field"""
103 | 
104 |     science_field: str = dspy.InputField()
105 |     year: int = dspy.InputField()
106 |     num_of_outputs: int = dspy.InputField()
107 |     news: list[ScienceNews] = dspy.OutputField(desc="science news")
108 | 
109 | 
110 | predict = dspy.Predict(NewsQA)
111 | predict(science_field="Computer Theory", year=2022, num_of_outputs=1)
112 | dspy.inspect_history()
113 | ```
114 | 
115 | The output looks like:
116 | 
117 | ```
118 | [2025-08-15T22:24:29.378666]
119 | 
120 | System message:
121 | 
122 | Your input fields are:
123 | 1. `science_field` (str):
124 | 2. `year` (int):
125 | 3. `num_of_outputs` (int):
126 | Your output fields are:
127 | 1. `news` (list[ScienceNews]): science news
128 | All interactions will be structured in the following way, with the appropriate values filled in.
129 | 
130 | [[ ## science_field ## ]]
131 | {science_field}
132 | 
133 | [[ ## year ## ]]
134 | {year}
135 | 
136 | [[ ## num_of_outputs ## ]]
137 | {num_of_outputs}
138 | 
139 | [[ ## news ## ]]
140 | {news}        # note: the value you produce must adhere to the JSON schema: {"type": "array", "$defs": {"ScienceNews": {"type": "object", "properties": {"scientists_involved": {"type": "array", "items": {"type": "string"}, "title": "Scientists Involved"}, "text": {"type": "string", "title": "Text"}}, "required": ["text", "scientists_involved"], "title": "ScienceNews"}}, "items": {"$ref": "#/$defs/ScienceNews"}}
141 | 
142 | [[ ## completed ## ]]
143 | In adhering to this structure, your objective is:
144 |         Get news about the given science field
145 | 
146 | 
147 | User message:
148 | 
149 | [[ ## science_field ## ]]
150 | Computer Theory
151 | 
152 | [[ ## year ## ]]
153 | 2022
154 | 
155 | [[ ## num_of_outputs ## ]]
156 | 1
157 | 
158 | Respond with the corresponding output fields, starting with the field `[[ ## news ## ]]` (must be formatted as a valid Python list[ScienceNews]), and then ending with the marker for `[[ ## completed ## ]]`.
159 | 
160 | 
161 | Response:
162 | 
163 | [[ ## news ## ]]
164 | [
165 |     {
166 |         "scientists_involved": ["John Doe", "Jane Smith"],
167 |         "text": "In 2022, researchers made significant advancements in quantum computing algorithms, demonstrating their potential to solve complex problems faster than classical computers. This breakthrough could revolutionize fields such as cryptography and optimization."
168 |     }
169 | ]
170 | 
171 | [[ ## completed ## ]]
172 | ```
173 | 
174 | !!! info "Practice: locate Signature information in the printed LM history"
175 | 
176 |     Try adjusting the signature, and observe how the changes are reflected in the printed LM message.
177 | 
178 | 
179 | Each field is preceded by a marker `[[ ## field_name ## ]]`. If an output field has non-primitive types, the instruction includes the type's JSON schema, and the output is formatted accordingly. Because the output field is structured as defined by ChatAdapter, it can be automatically parsed into structured data.
180 | 
181 | #### When to Use ChatAdapter
182 | 
183 | `ChatAdapter` offers the following advantages:
184 | 
185 | - **Universal compatibility**: Works with all language models, though smaller models may generate responses that do not match the required format.
186 | - **Fallback protection**: If `ChatAdapter` fails, it automatically retries with `JSONAdapter`.
187 | 
188 | In general, `ChatAdapter` is a reliable choice if you don't have specific requirements.
189 | 
190 | #### When Not to Use ChatAdapter
191 | 
192 | Avoid using `ChatAdapter` if you are:
193 | 
194 | - **Latency sensitive**: `ChatAdapter` includes more boilerplate output tokens compared to other adapters, so if you're building a system sensitive to latency, consider using a different adapter.
195 | 
196 | ### JSONAdapter
197 | 
198 | **JSONAdapter** prompts the LM to return JSON data containing all output fields as specified in the signature. It is effective for models that support structured output via the `response_format` parameter, leveraging native JSON generation capabilities for more reliable parsing.
199 | 
200 | #### Format Structure
201 | 
202 | The input part of the prompt formatted by `JSONAdapter` is similar to `ChatAdapter`, but the output part differs, as shown below:
203 | 
204 | ```python
205 | import dspy
206 | import pydantic
207 | 
208 | dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"), adapter=dspy.JSONAdapter())
209 | 
210 | 
211 | class ScienceNews(pydantic.BaseModel):
212 |     text: str
213 |     scientists_involved: list[str]
214 | 
215 | 
216 | class NewsQA(dspy.Signature):
217 |     """Get news about the given science field"""
218 | 
219 |     science_field: str = dspy.InputField()
220 |     year: int = dspy.InputField()
221 |     num_of_outputs: int = dspy.InputField()
222 |     news: list[ScienceNews] = dspy.OutputField(desc="science news")
223 | 
224 | 
225 | predict = dspy.Predict(NewsQA)
226 | predict(science_field="Computer Theory", year=2022, num_of_outputs=1)
227 | dspy.inspect_history()
228 | ```
229 | 
230 | ```
231 | System message:
232 | 
233 | Your input fields are:
234 | 1. `science_field` (str):
235 | 2. `year` (int):
236 | 3. `num_of_outputs` (int):
237 | Your output fields are:
238 | 1. `news` (list[ScienceNews]): science news
239 | All interactions will be structured in the following way, with the appropriate values filled in.
240 | 
241 | Inputs will have the following structure:
242 | 
243 | [[ ## science_field ## ]]
244 | {science_field}
245 | 
246 | [[ ## year ## ]]
247 | {year}
248 | 
249 | [[ ## num_of_outputs ## ]]
250 | {num_of_outputs}
251 | 
252 | Outputs will be a JSON object with the following fields.
253 | 
254 | {
255 |   "news": "{news}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"array\", \"$defs\": {\"ScienceNews\": {\"type\": \"object\", \"properties\": {\"scientists_involved\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"title\": \"Scientists Involved\"}, \"text\": {\"type\": \"string\", \"title\": \"Text\"}}, \"required\": [\"text\", \"scientists_involved\"], \"title\": \"ScienceNews\"}}, \"items\": {\"$ref\": \"#/$defs/ScienceNews\"}}"
256 | }
257 | In adhering to this structure, your objective is:
258 |         Get news about the given science field
259 | 
260 | 
261 | User message:
262 | 
263 | [[ ## science_field ## ]]
264 | Computer Theory
265 | 
266 | [[ ## year ## ]]
267 | 2022
268 | 
269 | [[ ## num_of_outputs ## ]]
270 | 1
271 | 
272 | Respond with a JSON object in the following order of fields: `news` (must be formatted as a valid Python list[ScienceNews]).
273 | 
274 | 
275 | Response:
276 | 
277 | {
278 |   "news": [
279 |     {
280 |       "text": "In 2022, researchers made significant advancements in quantum computing algorithms, demonstrating that quantum systems can outperform classical computers in specific tasks. This breakthrough could revolutionize fields such as cryptography and complex system simulations.",
281 |       "scientists_involved": [
282 |         "Dr. Alice Smith",
283 |         "Dr. Bob Johnson",
284 |         "Dr. Carol Lee"
285 |       ]
286 |     }
287 |   ]
288 | }
289 | ```
290 | 
291 | #### When to Use JSONAdapter
292 | 
293 | `JSONAdapter` is good at:
294 | 
295 | - **Structured output support**: When the model supports the `response_format` parameter.
296 | - **Low latency**: Minimal boilerplate in the LM response results in faster responses.
297 | 
298 | #### When Not to Use JSONAdapter
299 | 
300 | Avoid using `JSONAdapter` if you are:
301 | 
302 | - Using a model that does not natively support structured output, such as a small open-source model hosted on Ollama.
303 | 
304 | ## Summary
305 | 
306 | Adapters are a crucial component of DSPy that bridge the gap between structured DSPy signatures and language model APIs.
307 | Understanding when and how to use different adapters will help you build more reliable and efficient DSPy programs.
```

learn/programming/language_models.md
```
1 | ---
2 | sidebar_position: 2
3 | ---
4 | 
5 | # Language Models
6 | 
7 | The first step in any DSPy code is to set up your language model. For example, you can configure OpenAI's GPT-4o-mini as your default LM as follows.
8 | 
9 | ```python linenums="1"
10 | # Authenticate via `OPENAI_API_KEY` env: import os; os.environ['OPENAI_API_KEY'] = 'here'
11 | lm = dspy.LM('openai/gpt-4o-mini')
12 | dspy.configure(lm=lm)
13 | ```
14 | 
15 | !!! info "A few different LMs"
16 | 
17 |     === "OpenAI"
18 |         You can authenticate by setting the `OPENAI_API_KEY` env variable or passing `api_key` below.
19 | 
20 |         ```python linenums="1"
21 |         import dspy
22 |         lm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY')
23 |         dspy.configure(lm=lm)
24 |         ```
25 | 
26 |     === "Gemini (AI Studio)"
27 |         You can authenticate by setting the GEMINI_API_KEY env variable or passing `api_key` below.
28 | 
29 |         ```python linenums="1"
30 |         import dspy
31 |         lm = dspy.LM('gemini/gemini-2.5-pro-preview-03-25', api_key='GEMINI_API_KEY')
32 |         dspy.configure(lm=lm)
33 |         ```
34 | 
35 |     === "Anthropic"
36 |         You can authenticate by setting the ANTHROPIC_API_KEY env variable or passing `api_key` below.
37 | 
38 |         ```python linenums="1"
39 |         import dspy
40 |         lm = dspy.LM('anthropic/claude-3-opus-20240229', api_key='YOUR_ANTHROPIC_API_KEY')
41 |         dspy.configure(lm=lm)
42 |         ```
43 | 
44 |     === "Databricks"
45 |         If you're on the Databricks platform, authentication is automatic via their SDK. If not, you can set the env variables `DATABRICKS_API_KEY` and `DATABRICKS_API_BASE`, or pass `api_key` and `api_base` below.
46 | 
47 |         ```python linenums="1"
48 |         import dspy
49 |         lm = dspy.LM('databricks/databricks-meta-llama-3-1-70b-instruct')
50 |         dspy.configure(lm=lm)
51 |         ```
52 | 
53 |     === "Local LMs on a GPU server"
54 |           First, install [SGLang](https://sgl-project.github.io/start/install.html) and launch its server with your LM.
55 | 
56 |           ```bash
57 |           > pip install "sglang[all]"
58 |           > pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/ 
59 | 
60 |           > CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server --port 7501 --model-path meta-llama/Meta-Llama-3-8B-Instruct
61 |           ```
62 | 
63 |           Then, connect to it from your DSPy code as an OpenAI-compatible endpoint.
64 | 
65 |           ```python linenums="1"
66 |           lm = dspy.LM("openai/meta-llama/Meta-Llama-3-8B-Instruct",
67 |                            api_base="http://localhost:7501/v1",  # ensure this points to your port
68 |                            api_key="", model_type='chat')
69 |           dspy.configure(lm=lm)
70 |           ```
71 | 
72 |     === "Local LMs on your laptop"
73 |           First, install [Ollama](https://github.com/ollama/ollama) and launch its server with your LM.
74 | 
75 |           ```bash
76 |           > curl -fsSL https://ollama.ai/install.sh | sh
77 |           > ollama run llama3.2:1b
78 |           ```
79 | 
80 |           Then, connect to it from your DSPy code.
81 | 
82 |         ```python linenums="1"
83 |         import dspy
84 |         lm = dspy.LM('ollama_chat/llama3.2', api_base='http://localhost:11434', api_key='')
85 |         dspy.configure(lm=lm)
86 |         ```
87 | 
88 |     === "Other providers"
89 |         In DSPy, you can use any of the dozens of [LLM providers supported by LiteLLM](https://docs.litellm.ai/docs/providers). Simply follow their instructions for which `{PROVIDER}_API_KEY` to set and how to write pass the `{provider_name}/{model_name}` to the constructor. 
90 | 
91 |         Some examples:
92 | 
93 |         - `anyscale/mistralai/Mistral-7B-Instruct-v0.1`, with `ANYSCALE_API_KEY`
94 |         - `together_ai/togethercomputer/llama-2-70b-chat`, with `TOGETHERAI_API_KEY`
95 |         - `sagemaker/<your-endpoint-name>`, with `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_REGION_NAME`
96 |         - `azure/<your_deployment_name>`, with `AZURE_API_KEY`, `AZURE_API_BASE`, `AZURE_API_VERSION`, and the optional `AZURE_AD_TOKEN` and `AZURE_API_TYPE` as environment variables. If you are initiating external models without setting environment variables, use the following:
97 |         `lm = dspy.LM('azure/<your_deployment_name>', api_key = 'AZURE_API_KEY' , api_base = 'AZURE_API_BASE', api_version = 'AZURE_API_VERSION')`
98 | 
99 | 
100 |         
101 |         If your provider offers an OpenAI-compatible endpoint, just add an `openai/` prefix to your full model name.
102 | 
103 |         ```python linenums="1"
104 |         import dspy
105 |         lm = dspy.LM('openai/your-model-name', api_key='PROVIDER_API_KEY', api_base='YOUR_PROVIDER_URL')
106 |         dspy.configure(lm=lm)
107 |         ```
108 | If you run into errors, please refer to the [LiteLLM Docs](https://docs.litellm.ai/docs/providers) to verify if you are using the same variable names/following the right procedure.
109 | 
110 | ## Calling the LM directly.
111 | 
112 | It's easy to call the `lm` you configured above directly. This gives you a unified API and lets you benefit from utilities like automatic caching.
113 | 
114 | ```python linenums="1"       
115 | lm("Say this is a test!", temperature=0.7)  # => ['This is a test!']
116 | lm(messages=[{"role": "user", "content": "Say this is a test!"}])  # => ['This is a test!']
117 | ``` 
118 | 
119 | ## Using the LM with DSPy modules.
120 | 
121 | Idiomatic DSPy involves using _modules_, which we discuss in the next guide.
122 | 
123 | ```python linenums="1" 
124 | # Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).
125 | qa = dspy.ChainOfThought('question -> answer')
126 | 
127 | # Run with the default LM configured with `dspy.configure` above.
128 | response = qa(question="How many floors are in the castle David Gregory inherited?")
129 | print(response.answer)
130 | ```
131 | **Possible Output:**
132 | ```text
133 | The castle David Gregory inherited has 7 floors.
134 | ```
135 | 
136 | ## Using multiple LMs.
137 | 
138 | You can change the default LM globally with `dspy.configure` or change it inside a block of code with `dspy.context`.
139 | 
140 | !!! tip
141 |     Using `dspy.configure` and `dspy.context` is thread-safe!
142 | 
143 | 
144 | ```python linenums="1" 
145 | dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))
146 | response = qa(question="How many floors are in the castle David Gregory inherited?")
147 | print('GPT-4o-mini:', response.answer)
148 | 
149 | with dspy.context(lm=dspy.LM('openai/gpt-3.5-turbo')):
150 |     response = qa(question="How many floors are in the castle David Gregory inherited?")
151 |     print('GPT-3.5-turbo:', response.answer)
152 | ```
153 | **Possible Output:**
154 | ```text
155 | GPT-4o-mini: The number of floors in the castle David Gregory inherited cannot be determined with the information provided.
156 | GPT-3.5-turbo: The castle David Gregory inherited has 7 floors.
157 | ```
158 | 
159 | ## Configuring LM generation.
160 | 
161 | For any LM, you can configure any of the following attributes at initialization or in each subsequent call.
162 | 
163 | ```python linenums="1" 
164 | gpt_4o_mini = dspy.LM('openai/gpt-4o-mini', temperature=0.9, max_tokens=3000, stop=None, cache=False)
165 | ```
166 | 
167 | By default LMs in DSPy are cached. If you repeat the same call, you will get the same outputs. But you can turn off caching by setting `cache=False`.
168 | 
169 | If you want to keep caching enabled but force a new request (for example, to obtain diverse outputs),
170 | pass a unique `rollout_id` and set a non-zero `temperature` in your call. DSPy hashes both the inputs
171 | and the `rollout_id` when looking up a cache entry, so different values force a new LM request while
172 | still caching future calls with the same inputs and `rollout_id`. The ID is also recorded in
173 | `lm.history`, which makes it easy to track or compare different rollouts during experiments. Changing
174 | only the `rollout_id` while keeping `temperature=0` will not affect the LM's output.
175 | 
176 | ```python linenums="1"
177 | lm("Say this is a test!", rollout_id=1, temperature=1.0)
178 | ```
179 | 
180 | You can pass these LM kwargs directly to DSPy modules as well. Supplying them at
181 | initialization sets the defaults for every call:
182 | 
183 | ```python linenums="1"
184 | predict = dspy.Predict("question -> answer", rollout_id=1, temperature=1.0)
185 | ```
186 | 
187 | To override them for a single invocation, provide a ``config`` dictionary when
188 | calling the module:
189 | 
190 | ```python linenums="1"
191 | predict = dspy.Predict("question -> answer")
192 | predict(question="What is 1 + 52?", config={"rollout_id": 5, "temperature": 1.0})
193 | ```
194 | 
195 | In both cases, ``rollout_id`` is forwarded to the underlying LM, affects
196 | its caching behavior, and is stored alongside each response so you can
197 | replay or analyze specific rollouts later.
198 | 
199 | 
200 | ## Inspecting output and usage metadata.
201 | 
202 | Every LM object maintains the history of its interactions, including inputs, outputs, token usage (and $$$ cost), and metadata.
203 | 
204 | ```python linenums="1" 
205 | len(lm.history)  # e.g., 3 calls to the LM
206 | 
207 | lm.history[-1].keys()  # access the last call to the LM, with all metadata
208 | ```
209 | 
210 | **Output:**
211 | ```text
212 | dict_keys(['prompt', 'messages', 'kwargs', 'response', 'outputs', 'usage', 'cost', 'timestamp', 'uuid', 'model', 'response_model', 'model_type])
213 | ```
214 | 
215 | ## Using the Responses API
216 | 
217 | By default, DSPy calls language models (LMs) using LiteLLM's [Chat Completions API](https://docs.litellm.ai/docs/completion), which is suitable for most standard models and tasks. However, some advanced models, such as OpenAI's reasoning models (e.g., `gpt-5` or other future models), may offer improved quality or additional features when accessed via the [Responses API](https://docs.litellm.ai/docs/response_api), which is supported in DSPy.
218 | 
219 | **When should you use the Responses API?**
220 | 
221 | - If you are working with models that support or require the `responses` endpoint (such as OpenAI's reasoning models).
222 | - When you want to leverage enhanced reasoning, multi-turn, or richer output capabilities provided by certain models.
223 | 
224 | **How to enable the Responses API in DSPy:**
225 | 
226 | To enable the Responses API, just set `model_type="responses"` when creating the `dspy.LM` instance.
227 | 
228 | ```python
229 | import dspy
230 | 
231 | # Configure DSPy to use the Responses API for your language model
232 | dspy.settings.configure(
233 |     lm=dspy.LM(
234 |         "openai/gpt-5-mini",
235 |         model_type="responses",
236 |         temperature=1.0,
237 |         max_tokens=16000,
238 |     ),
239 | )
240 | ```
241 | 
242 | Please note that not all models or providers support the Responses API, check [LiteLLM's documentation](https://docs.litellm.ai/docs/response_api) for more details.
243 | 
244 | 
245 | ## Advanced: Building custom LMs and writing your own Adapters.
246 | 
247 | Though rarely needed, you can write custom LMs by inheriting from `dspy.BaseLM`. Another advanced layer in the DSPy ecosystem is that of _adapters_, which sit between DSPy signatures and LMs. A future version of this guide will discuss these advanced features, though you likely don't need them.
248 | 
```

learn/programming/modules.md
```
1 | ---
2 | sidebar_position: 3
3 | ---
4 | 
5 | # Modules
6 | 
7 | A **DSPy module** is a building block for programs that use LMs.
8 | 
9 | - Each built-in module abstracts a **prompting technique** (like chain of thought or ReAct). Crucially, they are generalized to handle any signature.
10 | 
11 | - A DSPy module has **learnable parameters** (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.
12 | 
13 | - Multiple modules can be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.
14 | 
15 | 
16 | ## How do I use a built-in module, like `dspy.Predict` or `dspy.ChainOfThought`?
17 | 
18 | Let's start with the most fundamental module, `dspy.Predict`. Internally, all other DSPy modules are built using `dspy.Predict`. We'll assume you are already at least a little familiar with [DSPy signatures](signatures.md), which are declarative specs for defining the behavior of any module we use in DSPy.
19 | 
20 | To use a module, we first **declare** it by giving it a signature. Then we **call** the module with the input arguments, and extract the output fields!
21 | 
22 | ```python
23 | sentence = "it's a charming and often affecting journey."  # example from the SST-2 dataset.
24 | 
25 | # 1) Declare with a signature.
26 | classify = dspy.Predict('sentence -> sentiment: bool')
27 | 
28 | # 2) Call with input argument(s). 
29 | response = classify(sentence=sentence)
30 | 
31 | # 3) Access the output.
32 | print(response.sentiment)
33 | ```
34 | **Output:**
35 | ```text
36 | True
37 | ```
38 | 
39 | When we declare a module, we can pass configuration keys to it.
40 | 
41 | Below, we'll pass `n=5` to request five completions. We can also pass `temperature` or `max_len`, etc.
42 | 
43 | Let's use `dspy.ChainOfThought`. In many cases, simply swapping `dspy.ChainOfThought` in place of `dspy.Predict` improves quality.
44 | 
45 | ```python
46 | question = "What's something great about the ColBERT retrieval model?"
47 | 
48 | # 1) Declare with a signature, and pass some config.
49 | classify = dspy.ChainOfThought('question -> answer', n=5)
50 | 
51 | # 2) Call with input argument.
52 | response = classify(question=question)
53 | 
54 | # 3) Access the outputs.
55 | response.completions.answer
56 | ```
57 | **Possible Output:**
58 | ```text
59 | ['One great thing about the ColBERT retrieval model is its superior efficiency and effectiveness compared to other models.',
60 |  'Its ability to efficiently retrieve relevant information from large document collections.',
61 |  'One great thing about the ColBERT retrieval model is its superior performance compared to other models and its efficient use of pre-trained language models.',
62 |  'One great thing about the ColBERT retrieval model is its superior efficiency and accuracy compared to other models.',
63 |  'One great thing about the ColBERT retrieval model is its ability to incorporate user feedback and support complex queries.']
64 | ```
65 | 
66 | Let's discuss the output object here. The `dspy.ChainOfThought` module will generally inject a `reasoning` before the output field(s) of your signature.
67 | 
68 | Let's inspect the (first) reasoning and answer!
69 | 
70 | ```python
71 | print(f"Reasoning: {response.reasoning}")
72 | print(f"Answer: {response.answer}")
73 | ```
74 | **Possible Output:**
75 | ```text
76 | Reasoning: We can consider the fact that ColBERT has shown to outperform other state-of-the-art retrieval models in terms of efficiency and effectiveness. It uses contextualized embeddings and performs document retrieval in a way that is both accurate and scalable.
77 | Answer: One great thing about the ColBERT retrieval model is its superior efficiency and effectiveness compared to other models.
78 | ```
79 | 
80 | This is accessible whether we request one or many completions.
81 | 
82 | We can also access the different completions as a list of `Prediction`s or as several lists, one for each field.
83 | 
84 | ```python
85 | response.completions[3].reasoning == response.completions.reasoning[3]
86 | ```
87 | **Output:**
88 | ```text
89 | True
90 | ```
91 | 
92 | 
93 | ## What other DSPy modules are there? How can I use them?
94 | 
95 | The others are very similar. They mainly change the internal behavior with which your signature is implemented!
96 | 
97 | 1. **`dspy.Predict`**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).
98 | 
99 | 2. **`dspy.ChainOfThought`**: Teaches the LM to think step-by-step before committing to the signature's response.
100 | 
101 | 3. **`dspy.ProgramOfThought`**: Teaches the LM to output code, whose execution results will dictate the response.
102 | 
103 | 4. **`dspy.ReAct`**: An agent that can use tools to implement the given signature.
104 | 
105 | 5. **`dspy.MultiChainComparison`**: Can compare multiple outputs from `ChainOfThought` to produce a final prediction.
106 | 
107 | 
108 | We also have some function-style modules:
109 | 
110 | 6. **`dspy.majority`**: Can do basic voting to return the most popular response from a set of predictions.
111 | 
112 | 
113 | !!! info "A few examples of DSPy modules on simple tasks."
114 |     Try the examples below after configuring your `lm`. Adjust the fields to explore what tasks your LM can do well out of the box.
115 | 
116 |     === "Math"
117 | 
118 |         ```python linenums="1"
119 |         math = dspy.ChainOfThought("question -> answer: float")
120 |         math(question="Two dice are tossed. What is the probability that the sum equals two?")
121 |         ```
122 |         
123 |         **Possible Output:**
124 |         ```text
125 |         Prediction(
126 |             reasoning='When two dice are tossed, each die has 6 faces, resulting in a total of 6 x 6 = 36 possible outcomes. The sum of the numbers on the two dice equals two only when both dice show a 1. This is just one specific outcome: (1, 1). Therefore, there is only 1 favorable outcome. The probability of the sum being two is the number of favorable outcomes divided by the total number of possible outcomes, which is 1/36.',
127 |             answer=0.0277776
128 |         )
129 |         ```
130 | 
131 |     === "Retrieval-Augmented Generation"
132 | 
133 |         ```python linenums="1"       
134 |         def search(query: str) -> list[str]:
135 |             """Retrieves abstracts from Wikipedia."""
136 |             results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)
137 |             return [x['text'] for x in results]
138 |         
139 |         rag = dspy.ChainOfThought('context, question -> response')
140 | 
141 |         question = "What's the name of the castle that David Gregory inherited?"
142 |         rag(context=search(question), question=question)
143 |         ```
144 |         
145 |         **Possible Output:**
146 |         ```text
147 |         Prediction(
148 |             reasoning='The context provides information about David Gregory, a Scottish physician and inventor. It specifically mentions that he inherited Kinnairdy Castle in 1664. This detail directly answers the question about the name of the castle that David Gregory inherited.',
149 |             response='Kinnairdy Castle'
150 |         )
151 |         ```
152 | 
153 |     === "Classification"
154 | 
155 |         ```python linenums="1"
156 |         from typing import Literal
157 | 
158 |         class Classify(dspy.Signature):
159 |             """Classify sentiment of a given sentence."""
160 |             
161 |             sentence: str = dspy.InputField()
162 |             sentiment: Literal['positive', 'negative', 'neutral'] = dspy.OutputField()
163 |             confidence: float = dspy.OutputField()
164 | 
165 |         classify = dspy.Predict(Classify)
166 |         classify(sentence="This book was super fun to read, though not the last chapter.")
167 |         ```
168 |         
169 |         **Possible Output:**
170 | 
171 |         ```text
172 |         Prediction(
173 |             sentiment='positive',
174 |             confidence=0.75
175 |         )
176 |         ```
177 | 
178 |     === "Information Extraction"
179 | 
180 |         ```python linenums="1"        
181 |         text = "Apple Inc. announced its latest iPhone 14 today. The CEO, Tim Cook, highlighted its new features in a press release."
182 | 
183 |         module = dspy.Predict("text -> title, headings: list[str], entities_and_metadata: list[dict[str, str]]")
184 |         response = module(text=text)
185 | 
186 |         print(response.title)
187 |         print(response.headings)
188 |         print(response.entities_and_metadata)
189 |         ```
190 |         
191 |         **Possible Output:**
192 |         ```text
193 |         Apple Unveils iPhone 14
194 |         ['Introduction', 'Key Features', "CEO's Statement"]
195 |         [{'entity': 'Apple Inc.', 'type': 'Organization'}, {'entity': 'iPhone 14', 'type': 'Product'}, {'entity': 'Tim Cook', 'type': 'Person'}]
196 |         ```
197 | 
198 |     === "Agents"
199 | 
200 |         ```python linenums="1"       
201 |         def evaluate_math(expression: str) -> float:
202 |             return dspy.PythonInterpreter({}).execute(expression)
203 | 
204 |         def search_wikipedia(query: str) -> str:
205 |             results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)
206 |             return [x['text'] for x in results]
207 | 
208 |         react = dspy.ReAct("question -> answer: float", tools=[evaluate_math, search_wikipedia])
209 | 
210 |         pred = react(question="What is 9362158 divided by the year of birth of David Gregory of Kinnairdy castle?")
211 |         print(pred.answer)
212 |         ```
213 |         
214 |         **Possible Output:**
215 | 
216 |         ```text
217 |         5761.328
218 |         ```
219 | 
220 | 
221 | ## How do I compose multiple modules into a bigger program?
222 | 
223 | DSPy is just Python code that uses modules in any control flow you like, with a little magic internally at `compile` time to trace your LM calls. What this means is that, you can just call the modules freely.
224 | 
225 | See tutorials like [multi-hop search](https://dspy.ai/tutorials/multihop_search/), whose module is reproduced below as an example.
226 | 
227 | ```python linenums="1"        
228 | class Hop(dspy.Module):
229 |     def __init__(self, num_docs=10, num_hops=4):
230 |         self.num_docs, self.num_hops = num_docs, num_hops
231 |         self.generate_query = dspy.ChainOfThought('claim, notes -> query')
232 |         self.append_notes = dspy.ChainOfThought('claim, notes, context -> new_notes: list[str], titles: list[str]')
233 | 
234 |     def forward(self, claim: str) -> list[str]:
235 |         notes = []
236 |         titles = []
237 | 
238 |         for _ in range(self.num_hops):
239 |             query = self.generate_query(claim=claim, notes=notes).query
240 |             context = search(query, k=self.num_docs)
241 |             prediction = self.append_notes(claim=claim, notes=notes, context=context)
242 |             notes.extend(prediction.new_notes)
243 |             titles.extend(prediction.titles)
244 |         
245 |         return dspy.Prediction(notes=notes, titles=list(set(titles)))
246 | ```
247 | 
248 | Then you can create a instance of the custom module class `Hop`, then invoke it by the `__call__` method:
249 | 
250 | ```
251 | hop = Hop()
252 | print(hop(claim="Stephen Curry is the best 3 pointer shooter ever in the human history"))
253 | ```
254 | 
255 | ## How do I track LM usage?
256 | 
257 | !!! note "Version Requirement"
258 |     LM usage tracking is available in DSPy version 2.6.16 and later.
259 | 
260 | DSPy provides built-in tracking of language model usage across all module calls. To enable tracking:
261 | 
262 | ```python
263 | dspy.settings.configure(track_usage=True)
264 | ```
265 | 
266 | Once enabled, you can access usage statistics from any `dspy.Prediction` object:
267 | 
268 | ```python
269 | usage = prediction_instance.get_lm_usage()
270 | ```
271 | 
272 | The usage data is returned as a dictionary that maps each language model name to its usage statistics. Here's a complete example:
273 | 
274 | ```python
275 | import dspy
276 | 
277 | # Configure DSPy with tracking enabled
278 | dspy.settings.configure(
279 |     lm=dspy.LM("openai/gpt-4o-mini", cache=False),
280 |     track_usage=True
281 | )
282 | 
283 | # Define a simple program that makes multiple LM calls
284 | class MyProgram(dspy.Module):
285 |     def __init__(self):
286 |         self.predict1 = dspy.ChainOfThought("question -> answer")
287 |         self.predict2 = dspy.ChainOfThought("question, answer -> score")
288 | 
289 |     def __call__(self, question: str) -> str:
290 |         answer = self.predict1(question=question)
291 |         score = self.predict2(question=question, answer=answer)
292 |         return score
293 | 
294 | # Run the program and check usage
295 | program = MyProgram()
296 | output = program(question="What is the capital of France?")
297 | print(output.get_lm_usage())
298 | ```
299 | 
300 | This will output usage statistics like:
301 | 
302 | ```python
303 | {
304 |     'openai/gpt-4o-mini': {
305 |         'completion_tokens': 61,
306 |         'prompt_tokens': 260,
307 |         'total_tokens': 321,
308 |         'completion_tokens_details': {
309 |             'accepted_prediction_tokens': 0,
310 |             'audio_tokens': 0,
311 |             'reasoning_tokens': 0,
312 |             'rejected_prediction_tokens': 0,
313 |             'text_tokens': None
314 |         },
315 |         'prompt_tokens_details': {
316 |             'audio_tokens': 0,
317 |             'cached_tokens': 0,
318 |             'text_tokens': None,
319 |             'image_tokens': None
320 |         }
321 |     }
322 | }
323 | ```
324 | 
325 | When using DSPy's caching features (either in-memory or on-disk via litellm), cached responses won't count toward usage statistics. For example:
326 | 
327 | ```python
328 | # Enable caching
329 | dspy.settings.configure(
330 |     lm=dspy.LM("openai/gpt-4o-mini", cache=True),
331 |     track_usage=True
332 | )
333 | 
334 | program = MyProgram()
335 | 
336 | # First call - will show usage statistics
337 | output = program(question="What is the capital of Zambia?")
338 | print(output.get_lm_usage())  # Shows token usage
339 | 
340 | # Second call - same question, will use cache
341 | output = program(question="What is the capital of Zambia?")
342 | print(output.get_lm_usage())  # Shows empty dict: {}
343 | ```
```

learn/programming/overview.md
```
1 | ---
2 | sidebar_position: 1
3 | ---
4 | 
5 | # Programming in DSPy
6 | 
7 | DSPy is a bet on _writing code instead of strings_. In other words, building the right control flow is crucial. Start by **defining your task**. What are the inputs to your system and what should your system produce as output? Is it a chatbot over your data or perhaps a code assistant? Or maybe a system for translation, for highlighting snippets from search results, or for generating reports with citations?
8 | 
9 | Next, **define your initial pipeline**. Can your DSPy program just be a single module or do you need to break it down into a few steps? Do you need retrieval or other tools, like a calculator or a calendar API? Is there a typical workflow for solving your problem in multiple well-scoped steps, or do you want more open-ended tool use with agents for your task? Think about these but start simple, perhaps with just a single `dspy.ChainOfThought` module, then add complexity incrementally based on observations.
10 | 
11 | As you do this, **craft and try a handful of examples** of the inputs to your program. Consider using a powerful LM at this point, or a couple of different LMs, just to understand what's possible. Record interesting (both easy and hard) examples you try. This will be useful when you are doing evaluation and optimization later.
12 | 
13 | 
14 | ??? "Beyond encouraging good design patterns, how does DSPy help here?"
15 | 
16 |     Conventional prompts couple your fundamental system architecture with incidental choices not portable to new LMs, objectives, or pipelines. A conventional prompt asks the LM to take some inputs and produce some outputs of certain types (a _signature_), formats the inputs in certain ways and requests outputs in a form it can parse accurately (an _adapter_), asks the LM to apply certain strategies like "thinking step by step" or using tools (a _module_'s logic), and relies on substantial trial-and-error to discover the right way to ask each LM to do this (a form of manual _optimization_).
17 |     
18 |     DSPy separates these concerns and automates the lower-level ones until you need to consider them. This allow you to write much shorter code, with much higher portability. For example, if you write a program using DSPy modules, you can swap the LM or its adapter without changing the rest of your logic. Or you can exchange one _module_, like `dspy.ChainOfThought`, with another, like `dspy.ProgramOfThought`, without modifying your signatures. When you're ready to use optimizers, the same program can have its prompts optimized or its LM weights fine-tuned.
```

learn/programming/signatures.md
```
1 | ---
2 | sidebar_position: 2
3 | ---
4 | 
5 | # Signatures
6 | 
7 | When we assign tasks to LMs in DSPy, we specify the behavior we need as a Signature.
8 | 
9 | **A signature is a declarative specification of input/output behavior of a DSPy module.** Signatures allow you to tell the LM _what_ it needs to do, rather than specify _how_ we should ask the LM to do it.
10 | 
11 | You're probably familiar with function signatures, which specify the input and output arguments and their types. DSPy signatures are similar, but with a couple of differences. While typical function signatures just _describe_ things, DSPy Signatures _declare and initialize the behavior_ of modules. Moreover, the field names matter in DSPy Signatures. You express semantic roles in plain English: a `question` is different from an `answer`, a `sql_query` is different from `python_code`.
12 | 
13 | ## Why should I use a DSPy Signature?
14 | 
15 | For modular and clean code, in which LM calls can be optimized into high-quality prompts (or automatic finetunes). Most people coerce LMs to do tasks by hacking long, brittle prompts. Or by collecting/generating data for fine-tuning. Writing signatures is far more modular, adaptive, and reproducible than hacking at prompts or finetunes. The DSPy compiler will figure out how to build a highly-optimized prompt for your LM (or finetune your small LM) for your signature, on your data, and within your pipeline. In many cases, we found that compiling leads to better prompts than humans write. Not because DSPy optimizers are more creative than humans, but simply because they can try more things and tune the metrics directly.
16 | 
17 | ## **Inline** DSPy Signatures
18 | 
19 | Signatures can be defined as a short string, with argument names and optional types that define semantic roles for inputs/outputs.
20 | 
21 | 1. Question Answering: `"question -> answer"`, which is equivalent to `"question: str -> answer: str"` as the default type is always `str`
22 | 
23 | 2. Sentiment Classification: `"sentence -> sentiment: bool"`, e.g. `True` if positive
24 | 
25 | 3. Summarization: `"document -> summary"`
26 | 
27 | Your signatures can also have multiple input/output fields with types:
28 | 
29 | 4. Retrieval-Augmented Question Answering: `"context: list[str], question: str -> answer: str"`
30 | 
31 | 5. Multiple-Choice Question Answering with Reasoning: `"question, choices: list[str] -> reasoning: str, selection: int"`
32 | 
33 | **Tip:** For fields, any valid variable names work! Field names should be semantically meaningful, but start simple and don't prematurely optimize keywords! Leave that kind of hacking to the DSPy compiler. For example, for summarization, it's probably fine to say `"document -> summary"`, `"text -> gist"`, or `"long_context -> tldr"`.
34 | 
35 | You can also add instructions to your inline signature, which can use variables at runtime. Use the `instructions` keyword argument to add instructions to your signature.
36 | 
37 | ```python
38 | toxicity = dspy.Predict(
39 |     dspy.Signature(
40 |         "comment -> toxic: bool",
41 |         instructions="Mark as 'toxic' if the comment includes insults, harassment, or sarcastic derogatory remarks.",
42 |     )
43 | )
44 | comment = "you are beautiful."
45 | toxicity(comment=comment).toxic
46 | ```
47 | 
48 | **Output:**
49 | ```text
50 | False
51 | ```
52 | 
53 | 
54 | ### Example A: Sentiment Classification
55 | 
56 | ```python
57 | sentence = "it's a charming and often affecting journey."  # example from the SST-2 dataset.
58 | 
59 | classify = dspy.Predict('sentence -> sentiment: bool')  # we'll see an example with Literal[] later
60 | classify(sentence=sentence).sentiment
61 | ```
62 | **Output:**
63 | ```text
64 | True
65 | ```
66 | 
67 | ### Example B: Summarization
68 | 
69 | ```python
70 | # Example from the XSum dataset.
71 | document = """The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page."""
72 | 
73 | summarize = dspy.ChainOfThought('document -> summary')
74 | response = summarize(document=document)
75 | 
76 | print(response.summary)
77 | ```
78 | **Possible Output:**
79 | ```text
80 | The 21-year-old Lee made seven appearances and scored one goal for West Ham last season. He had loan spells in League One with Blackpool and Colchester United, scoring twice for the latter. He has now signed a contract with Barnsley, but the length of the contract has not been revealed.
81 | ```
82 | 
83 | Many DSPy modules (except `dspy.Predict`) return auxiliary information by expanding your signature under the hood.
84 | 
85 | For example, `dspy.ChainOfThought` also adds a `reasoning` field that includes the LM's reasoning before it generates the output `summary`.
86 | 
87 | ```python
88 | print("Reasoning:", response.reasoning)
89 | ```
90 | **Possible Output:**
91 | ```text
92 | Reasoning: We need to highlight Lee's performance for West Ham, his loan spells in League One, and his new contract with Barnsley. We also need to mention that his contract length has not been disclosed.
93 | ```
94 | 
95 | ## **Class-based** DSPy Signatures
96 | 
97 | For some advanced tasks, you need more verbose signatures. This is typically to:
98 | 
99 | 1. Clarify something about the nature of the task (expressed below as a `docstring`).
100 | 
101 | 2. Supply hints on the nature of an input field, expressed as a `desc` keyword argument for `dspy.InputField`.
102 | 
103 | 3. Supply constraints on an output field, expressed as a `desc` keyword argument for `dspy.OutputField`.
104 | 
105 | ### Example C: Classification
106 | 
107 | ```python
108 | from typing import Literal
109 | 
110 | class Emotion(dspy.Signature):
111 |     """Classify emotion."""
112 |     
113 |     sentence: str = dspy.InputField()
114 |     sentiment: Literal['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'] = dspy.OutputField()
115 | 
116 | sentence = "i started feeling a little vulnerable when the giant spotlight started blinding me"  # from dair-ai/emotion
117 | 
118 | classify = dspy.Predict(Emotion)
119 | classify(sentence=sentence)
120 | ```
121 | **Possible Output:**
122 | ```text
123 | Prediction(
124 |     sentiment='fear'
125 | )
126 | ```
127 | 
128 | **Tip:** There's nothing wrong with specifying your requests to the LM more clearly. Class-based Signatures help you with that. However, don't prematurely tune the keywords of your signature by hand. The DSPy optimizers will likely do a better job (and will transfer better across LMs).
129 | 
130 | ### Example D: A metric that evaluates faithfulness to citations
131 | 
132 | ```python
133 | class CheckCitationFaithfulness(dspy.Signature):
134 |     """Verify that the text is based on the provided context."""
135 | 
136 |     context: str = dspy.InputField(desc="facts here are assumed to be true")
137 |     text: str = dspy.InputField()
138 |     faithfulness: bool = dspy.OutputField()
139 |     evidence: dict[str, list[str]] = dspy.OutputField(desc="Supporting evidence for claims")
140 | 
141 | context = "The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page."
142 | 
143 | text = "Lee scored 3 goals for Colchester United."
144 | 
145 | faithfulness = dspy.ChainOfThought(CheckCitationFaithfulness)
146 | faithfulness(context=context, text=text)
147 | ```
148 | **Possible Output:**
149 | ```text
150 | Prediction(
151 |     reasoning="Let's check the claims against the context. The text states Lee scored 3 goals for Colchester United, but the context clearly states 'He scored twice for the U's'. This is a direct contradiction.",
152 |     faithfulness=False,
153 |     evidence={'goal_count': ["scored twice for the U's"]}
154 | )
155 | ```
156 | 
157 | ### Example E: Multi-modal image classification
158 | 
159 | ```python
160 | class DogPictureSignature(dspy.Signature):
161 |     """Output the dog breed of the dog in the image."""
162 |     image_1: dspy.Image = dspy.InputField(desc="An image of a dog")
163 |     answer: str = dspy.OutputField(desc="The dog breed of the dog in the image")
164 | 
165 | image_url = "https://picsum.photos/id/237/200/300"
166 | classify = dspy.Predict(DogPictureSignature)
167 | classify(image_1=dspy.Image.from_url(image_url))
168 | ```
169 | 
170 | **Possible Output:**
171 | 
172 | ```text
173 | Prediction(
174 |     answer='Labrador Retriever'
175 | )
176 | ```
177 | 
178 | ## Type Resolution in Signatures
179 | 
180 | DSPy signatures support various annotation types:
181 | 
182 | 1. **Basic types** like `str`, `int`, `bool`
183 | 2. **Typing module types** like `list[str]`, `dict[str, int]`, `Optional[float]`. `Union[str, int]`
184 | 3. **Custom types** defined in your code
185 | 4. **Dot notation** for nested types with proper configuration
186 | 5. **Special data types** like `dspy.Image, dspy.History`
187 | 
188 | ### Working with Custom Types
189 | 
190 | ```python
191 | # Simple custom type
192 | class QueryResult(pydantic.BaseModel):
193 |     text: str
194 |     score: float
195 | 
196 | signature = dspy.Signature("query: str -> result: QueryResult")
197 | 
198 | class MyContainer:
199 |     class Query(pydantic.BaseModel):
200 |         text: str
201 |     class Score(pydantic.BaseModel):
202 |         score: float
203 | 
204 | signature = dspy.Signature("query: MyContainer.Query -> score: MyContainer.Score")
205 | ```
206 | 
207 | ## Using signatures to build modules & compiling them
208 | 
209 | While signatures are convenient for prototyping with structured inputs/outputs, that's not the only reason to use them!
210 | 
211 | You should compose multiple signatures into bigger [DSPy modules](modules.md) and [compile these modules into optimized prompts](../optimization/optimizers.md) and finetunes.
```

learn/programming/tools.md
```
1 | ---
2 | sidebar_position: 2
3 | ---
4 | 
5 | # Tools
6 | 
7 | DSPy provides powerful support for **tool-using agents** that can interact with external functions, APIs, and services. Tools enable language models to go beyond text generation by performing actions, retrieving information, and processing data dynamically.
8 | 
9 | There are two main approaches to using tools in DSPy:
10 | 
11 | 1. **`dspy.ReAct`** - A fully managed tool agent that handles reasoning and tool calls automatically
12 | 2. **Manual tool handling** - Direct control over tool calls using `dspy.Tool`, `dspy.ToolCalls`, and custom signatures
13 | 
14 | ## Approach 1: Using `dspy.ReAct` (Fully Managed)
15 | 
16 | The `dspy.ReAct` module implements the Reasoning and Acting (ReAct) pattern, where the language model iteratively reasons about the current situation and decides which tools to call.
17 | 
18 | ### Basic Example
19 | 
20 | ```python
21 | import dspy
22 | 
23 | # Define your tools as functions
24 | def get_weather(city: str) -> str:
25 |     """Get the current weather for a city."""
26 |     # In a real implementation, this would call a weather API
27 |     return f"The weather in {city} is sunny and 75°F"
28 | 
29 | def search_web(query: str) -> str:
30 |     """Search the web for information."""
31 |     # In a real implementation, this would call a search API
32 |     return f"Search results for '{query}': [relevant information...]"
33 | 
34 | # Create a ReAct agent
35 | react_agent = dspy.ReAct(
36 |     signature="question -> answer",
37 |     tools=[get_weather, search_web],
38 |     max_iters=5
39 | )
40 | 
41 | # Use the agent
42 | result = react_agent(question="What's the weather like in Tokyo?")
43 | print(result.answer)
44 | print("Tool calls made:", result.trajectory)
45 | ```
46 | 
47 | ### ReAct Features
48 | 
49 | - **Automatic reasoning**: The model thinks through the problem step by step
50 | - **Tool selection**: Automatically chooses which tool to use based on the situation
51 | - **Iterative execution**: Can make multiple tool calls to gather information
52 | - **Error handling**: Built-in error recovery for failed tool calls
53 | - **Trajectory tracking**: Complete history of reasoning and tool calls
54 | 
55 | ### ReAct Parameters
56 | 
57 | ```python
58 | react_agent = dspy.ReAct(
59 |     signature="question -> answer",  # Input/output specification
60 |     tools=[tool1, tool2, tool3],     # List of available tools
61 |     max_iters=10                     # Maximum number of tool call iterations
62 | )
63 | ```
64 | 
65 | ## Approach 2: Manual Tool Handling
66 | 
67 | For more control over the tool calling process, you can manually handle tools using DSPy's tool types.
68 | 
69 | ### Basic Setup
70 | 
71 | ```python
72 | import dspy
73 | 
74 | class ToolSignature(dspy.Signature):
75 |     """Signature for manual tool handling."""
76 |     question: str = dspy.InputField()
77 |     tools: list[dspy.Tool] = dspy.InputField()
78 |     outputs: dspy.ToolCalls = dspy.OutputField()
79 | 
80 | def weather(city: str) -> str:
81 |     """Get weather information for a city."""
82 |     return f"The weather in {city} is sunny"
83 | 
84 | def calculator(expression: str) -> str:
85 |     """Evaluate a mathematical expression."""
86 |     try:
87 |         result = eval(expression)  # Note: Use safely in production
88 |         return f"The result is {result}"
89 |     except:
90 |         return "Invalid expression"
91 | 
92 | # Create tool instances
93 | tools = {
94 |     "weather": dspy.Tool(weather),
95 |     "calculator": dspy.Tool(calculator)
96 | }
97 | 
98 | # Create predictor
99 | predictor = dspy.Predict(ToolSignature)
100 | 
101 | # Make a prediction
102 | response = predictor(
103 |     question="What's the weather in New York?",
104 |     tools=list(tools.values())
105 | )
106 | 
107 | # Execute the tool calls
108 | for call in response.outputs.tool_calls:
109 |     # Execute the tool call
110 |     result = call.execute()
111 |     print(f"Tool: {call.name}")
112 |     print(f"Args: {call.args}")
113 |     print(f"Result: {result}")
114 | ```
115 | 
116 | ### Understanding `dspy.Tool`
117 | 
118 | The `dspy.Tool` class wraps regular Python functions to make them compatible with DSPy's tool system:
119 | 
120 | ```python
121 | def my_function(param1: str, param2: int = 5) -> str:
122 |     """A sample function with parameters."""
123 |     return f"Processed {param1} with value {param2}"
124 | 
125 | # Create a tool
126 | tool = dspy.Tool(my_function)
127 | 
128 | # Tool properties
129 | print(tool.name)        # "my_function"
130 | print(tool.desc)        # The function's docstring
131 | print(tool.args)        # Parameter schema
132 | print(str(tool))        # Full tool description
133 | ```
134 | 
135 | ### Understanding `dspy.ToolCalls`
136 | 
137 | The `dspy.ToolCalls` type represents the output from a model that can make tool calls. Each individual tool call can be executed using the `execute` method:
138 | 
139 | ```python
140 | # After getting a response with tool calls
141 | for call in response.outputs.tool_calls:
142 |     print(f"Tool name: {call.name}")
143 |     print(f"Arguments: {call.args}")
144 |     
145 |     # Execute individual tool calls with different options:
146 |     
147 |     # Option 1: Automatic discovery (finds functions in locals/globals)
148 |     result = call.execute()  # Automatically finds functions by name
149 | 
150 |     # Option 2: Pass tools as a dict (most explicit)
151 |     result = call.execute(functions={"weather": weather, "calculator": calculator})
152 |     
153 |     # Option 3: Pass Tool objects as a list
154 |     result = call.execute(functions=[dspy.Tool(weather), dspy.Tool(calculator)])
155 |     
156 |     print(f"Result: {result}")
157 | ```
158 | 
159 | ## Using Native Tool Calling
160 | 
161 | DSPy adapters support **native function calling**, which leverages the underlying language model's built-in tool calling capabilities rather
162 | than relying on text-based parsing. This approach can provide more reliable tool execution and better integration with models that support
163 | native function calling.
164 | 
165 | !!! warning "Native tool calling doesn't guarantee better quality"
166 | 
167 |     It's possible that native tool calling produces lower quality than custom tool calling.
168 | 
169 | ### Adapter Behavior
170 | 
171 | Different DSPy adapters have different defaults for native function calling:
172 | 
173 | - **`ChatAdapter`** - Uses `use_native_function_calling=False` by default (relies on text parsing)
174 | - **`JSONAdapter`** - Uses `use_native_function_calling=True` by default (uses native function calling)
175 | 
176 | You can override these defaults by explicitly setting the `use_native_function_calling` parameter when creating an adapter.
177 | 
178 | ### Configuration
179 | 
180 | ```python
181 | import dspy
182 | 
183 | # ChatAdapter with native function calling enabled
184 | chat_adapter_native = dspy.ChatAdapter(use_native_function_calling=True)
185 | 
186 | # JSONAdapter with native function calling disabled
187 | json_adapter_manual = dspy.JSONAdapter(use_native_function_calling=False)
188 | 
189 | # Configure DSPy to use the adapter
190 | dspy.configure(lm=dspy.LM(model="openai/gpt-4o"), adapter=chat_adapter_native)
191 | ```
192 | 
193 | You can enable the [MLflow tracing](https://dspy.ai/tutorials/observability/) to check how native tool
194 | calling is being used. If you use `JSONAdapter` or `ChatAdapter` with native function calling enabled on the code snippet
195 | as provided in [the section above](tools.md#basic-setup), you should see native function calling arg `tools` is set like
196 | the screenshot below:
197 | 
198 | ![native tool calling](../figures/native_tool_call.png)
199 | 
200 | 
201 | ### Model Compatibility
202 | 
203 | Native function calling automatically detects model support using `litellm.supports_function_calling()`. If the model doesn't support native function calling, DSPy will fall back to manual text-based parsing even when `use_native_function_calling=True` is set.
204 | 
205 | ## Best Practices
206 | 
207 | ### 1. Tool Function Design
208 | 
209 | - **Clear docstrings**: Tools work better with descriptive documentation
210 | - **Type hints**: Provide clear parameter and return types
211 | - **Simple parameters**: Use basic types (str, int, bool, dict, list) or Pydantic models
212 | 
213 | ```python
214 | def good_tool(city: str, units: str = "celsius") -> str:
215 |     """
216 |     Get weather information for a specific city.
217 |     
218 |     Args:
219 |         city: The name of the city to get weather for
220 |         units: Temperature units, either 'celsius' or 'fahrenheit'
221 |     
222 |     Returns:
223 |         A string describing the current weather conditions
224 |     """
225 |     # Implementation with proper error handling
226 |     if not city.strip():
227 |         return "Error: City name cannot be empty"
228 |     
229 |     # Weather logic here...
230 |     return f"Weather in {city}: 25°{units[0].upper()}, sunny"
231 | ```
232 | 
233 | ### 2. Choosing Between ReAct and Manual Handling
234 | 
235 | **Use `dspy.ReAct` when:**
236 | 
237 | - You want automatic reasoning and tool selection
238 | - The task requires multiple tool calls
239 | - You need built-in error recovery
240 | - You want to focus on tool implementation rather than orchestration
241 | 
242 | **Use manual tool handling when:**
243 | 
244 | - You need precise control over tool execution
245 | - You want custom error handling logic
246 | - You want to minimize the latency
247 | - Your tool returns nothing (void function)
248 | 
249 | 
250 | Tools in DSPy provide a powerful way to extend language model capabilities beyond text generation. Whether using the fully automated ReAct approach or manual tool handling, you can build sophisticated agents that interact with the world through code.
```

tutorials/agents/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Agents\n",
8 |     "\n",
9 |     "Let's walk through a quick example of setting up a `dspy.ReAct` agent with a couple of tools and optimizing it to conduct advanced browsing for multi-hop search.\n",
10 |     "\n",
11 |     "Install the latest DSPy via `pip install -U dspy` and follow along. You also need to run `pip install datasets`.\n",
12 |     "\n",
13 |     "<details>\n",
14 |     "<summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>\n",
15 |     "\n",
16 |     "### MLflow DSPy Integration\n",
17 |     "\n",
18 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
19 |     "\n",
20 |     "![MLflow Trace](./mlflow-tracing-agent.png)\n",
21 |     "\n",
22 |     "1. Install MLflow\n",
23 |     "\n",
24 |     "```bash\n",
25 |     "%pip install mlflow>=2.20\n",
26 |     "```\n",
27 |     "\n",
28 |     "2. Start MLflow UI in a separate terminal\n",
29 |     "```bash\n",
30 |     "mlflow ui --port 5000\n",
31 |     "```\n",
32 |     "\n",
33 |     "3. Connect the notebook to MLflow\n",
34 |     "```python\n",
35 |     "import mlflow\n",
36 |     "\n",
37 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
38 |     "mlflow.set_experiment(\"DSPy\")\n",
39 |     "```\n",
40 |     "\n",
41 |     "4. Enabling tracing.\n",
42 |     "```python\n",
43 |     "mlflow.dspy.autolog()\n",
44 |     "```\n",
45 |     "\n",
46 |     "Once you have completed the steps above, you can see traces for each program execution on the notebook. They provide great visibility into the model's behavior and helps you understand the DSPy's concepts better throughout the tutorial.\n",
47 |     "\n",
48 |     "To kearn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
49 |     "\n",
50 |     "</details>"
51 |    ]
52 |   },
53 |   {
54 |    "cell_type": "markdown",
55 |    "metadata": {},
56 |    "source": [
57 |     "In this tutorial, we'll use an extremely small LM, Meta's `Llama-3.2-3B-Instruct` which has 3 billion parameters.\n",
58 |     "\n",
59 |     "A model like this is not very reliable out of the box for long or complex agent loops. However, it's extremely fast and cheap to host, as it needs very little RAM.\n",
60 |     "\n",
61 |     "You might be able to host the 3B model on your laptop with Ollama, on your GPU server with SGLang, or via a provider that hosts it for you like Databricks or Together.\n",
62 |     "\n",
63 |     "In the snippet below, we'll configure our main LM as `Llama-3.2-3B`. We'll also set up a larger LM, i.e. `GPT-4o`, as a teacher that we'll invoke a very small number of times to help teach the small LM."
64 |    ]
65 |   },
66 |   {
67 |    "cell_type": "code",
68 |    "execution_count": 1,
69 |    "metadata": {},
70 |    "outputs": [],
71 |    "source": [
72 |     "import dspy\n",
73 |     "\n",
74 |     "llama3b = dspy.LM('<provider>/Llama-3.2-3B-Instruct', temperature=0.7)\n",
75 |     "gpt4o = dspy.LM('openai/gpt-4o', temperature=0.7)\n",
76 |     "\n",
77 |     "dspy.configure(lm=llama3b)"
78 |    ]
79 |   },
80 |   {
81 |    "cell_type": "markdown",
82 |    "metadata": {},
83 |    "source": [
84 |     "Let's load a dataset for our task. We'll load examples from the HoVer multi-hop task, where the input is a (really!) complex claim and the output we're seeking is the set of Wikipedia pages that are required to fact-check that claim."
85 |    ]
86 |   },
87 |   {
88 |    "cell_type": "code",
89 |    "execution_count": 2,
90 |    "metadata": {},
91 |    "outputs": [],
92 |    "source": [
93 |     "import random\n",
94 |     "from dspy.datasets import DataLoader\n",
95 |     "\n",
96 |     "kwargs = dict(fields=(\"claim\", \"supporting_facts\", \"hpqa_id\", \"num_hops\"), input_keys=(\"claim\",))\n",
97 |     "hover = DataLoader().from_huggingface(dataset_name=\"hover-nlp/hover\", split=\"train\", trust_remote_code=True, **kwargs)\n",
98 |     "\n",
99 |     "hpqa_ids = set()\n",
100 |     "hover = [\n",
101 |     "    dspy.Example(claim=x.claim, titles=list(set([y[\"key\"] for y in x.supporting_facts]))).with_inputs(\"claim\")\n",
102 |     "    for x in hover\n",
103 |     "    if x[\"num_hops\"] == 3 and x[\"hpqa_id\"] not in hpqa_ids and not hpqa_ids.add(x[\"hpqa_id\"])\n",
104 |     "]\n",
105 |     "\n",
106 |     "random.Random(0).shuffle(hover)\n",
107 |     "trainset, devset, testset = hover[:100], hover[100:200], hover[650:]"
108 |    ]
109 |   },
110 |   {
111 |    "cell_type": "markdown",
112 |    "metadata": {},
113 |    "source": [
114 |     "Let's view an example of this task:"
115 |    ]
116 |   },
117 |   {
118 |    "cell_type": "code",
119 |    "execution_count": 3,
120 |    "metadata": {},
121 |    "outputs": [
122 |     {
123 |      "name": "stdout",
124 |      "output_type": "stream",
125 |      "text": [
126 |       "Claim: This director is known for his work on Miss Potter. The Academy of Motion Picture Arts and Sciences presents the award in which he was nominated for his work in \"Babe\".\n",
127 |       "Pages that must be retrieved: ['Miss Potter', 'Chris Noonan', 'Academy Award for Best Director']\n"
128 |      ]
129 |     }
130 |    ],
131 |    "source": [
132 |     "example = trainset[0]\n",
133 |     "\n",
134 |     "print(\"Claim:\", example.claim)\n",
135 |     "print(\"Pages that must be retrieved:\", example.titles)"
136 |    ]
137 |   },
138 |   {
139 |    "cell_type": "markdown",
140 |    "metadata": {},
141 |    "source": [
142 |     "Now, let's define a function to do the search in Wikipedia. We'll rely on a ColBERTv2 server that can search the \"abstracts\" (i.e., first paragraphs) of every article that existed in Wikipedia in 2017, which is the data used in HoVer."
143 |    ]
144 |   },
145 |   {
146 |    "cell_type": "code",
147 |    "execution_count": 4,
148 |    "metadata": {},
149 |    "outputs": [],
150 |    "source": [
151 |     "DOCS = {}\n",
152 |     "\n",
153 |     "def search(query: str, k: int) -> list[str]:\n",
154 |     "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=k)\n",
155 |     "    results = [x['text'] for x in results]\n",
156 |     "\n",
157 |     "    for result in results:\n",
158 |     "        title, text = result.split(\" | \", 1)\n",
159 |     "        DOCS[title] = text\n",
160 |     "\n",
161 |     "    return results"
162 |    ]
163 |   },
164 |   {
165 |    "cell_type": "markdown",
166 |    "metadata": {},
167 |    "source": [
168 |     "Now, let's use the `search` function to define two tools for our ReAct agent:"
169 |    ]
170 |   },
171 |   {
172 |    "cell_type": "code",
173 |    "execution_count": 5,
174 |    "metadata": {},
175 |    "outputs": [],
176 |    "source": [
177 |     "def search_wikipedia(query: str) -> list[str]:\n",
178 |     "    \"\"\"Returns top-5 results and then the titles of the top-5 to top-30 results.\"\"\"\n",
179 |     "\n",
180 |     "    topK = search(query, 30)\n",
181 |     "    titles, topK = [f\"`{x.split(' | ')[0]}`\" for x in topK[5:30]], topK[:5]\n",
182 |     "    return topK + [f\"Other retrieved pages have titles: {', '.join(titles)}.\"]\n",
183 |     "\n",
184 |     "def lookup_wikipedia(title: str) -> str:\n",
185 |     "    \"\"\"Returns the text of the Wikipedia page, if it exists.\"\"\"\n",
186 |     "\n",
187 |     "    if title in DOCS:\n",
188 |     "        return DOCS[title]\n",
189 |     "\n",
190 |     "    results = [x for x in search(title, 10) if x.startswith(title + \" | \")]\n",
191 |     "    if not results:\n",
192 |     "        return f\"No Wikipedia page found for title: {title}\"\n",
193 |     "    return results[0]"
194 |    ]
195 |   },
196 |   {
197 |    "cell_type": "markdown",
198 |    "metadata": {},
199 |    "source": [
200 |     "Now, let's define the ReAct agent in DSPy. It's going to be super simple: it'll take a `claim` and produce a list `titles: list[str]`.\n",
201 |     "\n",
202 |     "We'll instruct it to find all Wikipedia titles that are needed to fact-check the claim."
203 |    ]
204 |   },
205 |   {
206 |    "cell_type": "code",
207 |    "execution_count": 6,
208 |    "metadata": {},
209 |    "outputs": [],
210 |    "source": [
211 |     "instructions = \"Find all Wikipedia titles relevant to verifying (or refuting) the claim.\"\n",
212 |     "signature = dspy.Signature(\"claim -> titles: list[str]\", instructions)\n",
213 |     "react = dspy.ReAct(signature, tools=[search_wikipedia, lookup_wikipedia], max_iters=20)"
214 |    ]
215 |   },
216 |   {
217 |    "cell_type": "markdown",
218 |    "metadata": {},
219 |    "source": [
220 |     "Let's try it with a really simple claim to see if our tiny 3B model can do it!"
221 |    ]
222 |   },
223 |   {
224 |    "cell_type": "code",
225 |    "execution_count": 7,
226 |    "metadata": {},
227 |    "outputs": [
228 |     {
229 |      "data": {
230 |       "text/plain": [
231 |        "['David Gregory (physician)', 'David A. Gregory', 'David Harry Gregory']"
232 |       ]
233 |      },
234 |      "execution_count": 8,
235 |      "metadata": {},
236 |      "output_type": "execute_result"
237 |     }
238 |    ],
239 |    "source": [
240 |     "react(claim=\"David Gregory was born in 1625.\").titles[:3]"
241 |    ]
242 |   },
243 |   {
244 |    "cell_type": "markdown",
245 |    "metadata": {},
246 |    "source": [
247 |     "Great. Now let's set up an evaluation metric, `top5_recall`.\n",
248 |     "\n",
249 |     "It will return the fraction of the gold pages (which are always 3) that are retrieved in the top-5 titles returned by the agent."
250 |    ]
251 |   },
252 |   {
253 |    "cell_type": "code",
254 |    "execution_count": 9,
255 |    "metadata": {},
256 |    "outputs": [],
257 |    "source": [
258 |     "def top5_recall(example, pred, trace=None):\n",
259 |     "    gold_titles = example.titles\n",
260 |     "    recall = sum(x in pred.titles[:5] for x in gold_titles) / len(gold_titles)\n",
261 |     "\n",
262 |     "    # If we're \"bootstrapping\" for optimization, return True if and only if the recall is perfect.\n",
263 |     "    if trace is not None:\n",
264 |     "        return recall >= 1.0\n",
265 |     "    \n",
266 |     "    # If we're just doing inference, just measure the recall.\n",
267 |     "    return recall\n",
268 |     "\n",
269 |     "evaluate = dspy.Evaluate(devset=devset, metric=top5_recall, num_threads=16, display_progress=True, display_table=5)"
270 |    ]
271 |   },
272 |   {
273 |    "cell_type": "markdown",
274 |    "metadata": {},
275 |    "source": [
276 |     "Let's evaluate our off-the-shelf agent, with `Llama-3.2-8B`, to see how far we can go already.\n",
277 |     "\n",
278 |     "This model is tiny, so it can complain fairly often. Let's wrap it in a try/except block to hide those."
279 |    ]
280 |   },
281 |   {
282 |    "cell_type": "code",
283 |    "execution_count": 10,
284 |    "metadata": {},
285 |    "outputs": [
286 |     {
287 |      "name": "stdout",
288 |      "output_type": "stream",
289 |      "text": [
290 |       "  0%|          | 0/100 [00:00<?, ?it/s]"
291 |      ]
292 |     },
293 |     {
294 |      "name": "stdout",
295 |      "output_type": "stream",
296 |      "text": [
297 |       "Average Metric: 8.00 / 100 (8.0%): 100%|██████████| 100/100 [05:22<00:00,  3.22s/it]"
298 |      ]
299 |     },
300 |     {
301 |      "name": "stderr",
302 |      "output_type": "stream",
303 |      "text": [
304 |       "2024/12/17 14:09:47 INFO dspy.evaluate.evaluate: Average Metric: 7.999999999999997 / 100 (8.0%)\n"
305 |      ]
306 |     },
307 |     {
308 |      "name": "stdout",
309 |      "output_type": "stream",
310 |      "text": [
311 |       "\n"
312 |      ]
313 |     },
314 |     {
315 |      "data": {
316 |       "text/html": [
317 |        "<div>\n",
318 |        "<style scoped>\n",
319 |        "    .dataframe tbody tr th:only-of-type {\n",
320 |        "        vertical-align: middle;\n",
321 |        "    }\n",
322 |        "\n",
323 |        "    .dataframe tbody tr th {\n",
324 |        "        vertical-align: top;\n",
325 |        "    }\n",
326 |        "\n",
327 |        "    .dataframe thead th {\n",
328 |        "        text-align: right;\n",
329 |        "    }\n",
330 |        "</style>\n",
331 |        "<table border=\"1\" class=\"dataframe\">\n",
332 |        "  <thead>\n",
333 |        "    <tr style=\"text-align: right;\">\n",
334 |        "      <th></th>\n",
335 |        "      <th>claim</th>\n",
336 |        "      <th>example_titles</th>\n",
337 |        "      <th>trajectory</th>\n",
338 |        "      <th>reasoning</th>\n",
339 |        "      <th>pred_titles</th>\n",
340 |        "      <th>top5_success</th>\n",
341 |        "    </tr>\n",
342 |        "  </thead>\n",
343 |        "  <tbody>\n",
344 |        "    <tr>\n",
345 |        "      <th>0</th>\n",
346 |        "      <td>The Church of England's movement that inspired the Trinity Episcop...</td>\n",
347 |        "      <td>[Oxford Movement, Trinity Episcopal Church (Houghton, Michigan), S...</td>\n",
348 |        "      <td>{'thought_0': 'The claim suggests that there is a specific movemen...</td>\n",
349 |        "      <td>The search results seem to be a mix of different churches with sim...</td>\n",
350 |        "      <td>['Trinity Episcopal Church (Houghton, Michigan)', 'Trinity Episcop...</td>\n",
351 |        "      <td>✔️ [0.333]</td>\n",
352 |        "    </tr>\n",
353 |        "    <tr>\n",
354 |        "      <th>1</th>\n",
355 |        "      <td>Red, White &amp; Crüe and this athlete both fight. The french fighter ...</td>\n",
356 |        "      <td>[Red, White &amp;amp; Crüe, Mike Tyson, Bobby Stewart]</td>\n",
357 |        "      <td>NaN</td>\n",
358 |        "      <td>NaN</td>\n",
359 |        "      <td>[]</td>\n",
360 |        "      <td></td>\n",
361 |        "    </tr>\n",
362 |        "    <tr>\n",
363 |        "      <th>2</th>\n",
364 |        "      <td>The writer/director/actor from Glen or Glenda and Fernand Rivers s...</td>\n",
365 |        "      <td>[Ed Wood, Glen or Glenda, Fernand Rivers]</td>\n",
366 |        "      <td>NaN</td>\n",
367 |        "      <td>NaN</td>\n",
368 |        "      <td>[]</td>\n",
369 |        "      <td></td>\n",
370 |        "    </tr>\n",
371 |        "    <tr>\n",
372 |        "      <th>3</th>\n",
373 |        "      <td>The film by Sandi Sissel was released before The End of Suburbia.</td>\n",
374 |        "      <td>[Chicken Ranch (film), Sandi Sissel, The End of Suburbia]</td>\n",
375 |        "      <td>NaN</td>\n",
376 |        "      <td>NaN</td>\n",
377 |        "      <td>[]</td>\n",
378 |        "      <td></td>\n",
379 |        "    </tr>\n",
380 |        "    <tr>\n",
381 |        "      <th>4</th>\n",
382 |        "      <td>The actor who played captain hook in the live production with Tayl...</td>\n",
383 |        "      <td>[Christopher Walken, Taylor Louderman, Peter Pan Live!]</td>\n",
384 |        "      <td>NaN</td>\n",
385 |        "      <td>NaN</td>\n",
386 |        "      <td>[]</td>\n",
387 |        "      <td></td>\n",
388 |        "    </tr>\n",
389 |        "  </tbody>\n",
390 |        "</table>\n",
391 |        "</div>"
392 |       ],
393 |       "text/plain": [
394 |        "                                                                   claim  \\\n",
395 |        "0  The Church of England's movement that inspired the Trinity Episcop...   \n",
396 |        "1  Red, White & Crüe and this athlete both fight. The french fighter ...   \n",
397 |        "2  The writer/director/actor from Glen or Glenda and Fernand Rivers s...   \n",
398 |        "3      The film by Sandi Sissel was released before The End of Suburbia.   \n",
399 |        "4  The actor who played captain hook in the live production with Tayl...   \n",
400 |        "\n",
401 |        "                                                          example_titles  \\\n",
402 |        "0  [Oxford Movement, Trinity Episcopal Church (Houghton, Michigan), S...   \n",
403 |        "1                     [Red, White &amp; Crüe, Mike Tyson, Bobby Stewart]   \n",
404 |        "2                              [Ed Wood, Glen or Glenda, Fernand Rivers]   \n",
405 |        "3              [Chicken Ranch (film), Sandi Sissel, The End of Suburbia]   \n",
406 |        "4                [Christopher Walken, Taylor Louderman, Peter Pan Live!]   \n",
407 |        "\n",
408 |        "                                                              trajectory  \\\n",
409 |        "0  {'thought_0': 'The claim suggests that there is a specific movemen...   \n",
410 |        "1                                                                    NaN   \n",
411 |        "2                                                                    NaN   \n",
412 |        "3                                                                    NaN   \n",
413 |        "4                                                                    NaN   \n",
414 |        "\n",
415 |        "                                                               reasoning  \\\n",
416 |        "0  The search results seem to be a mix of different churches with sim...   \n",
417 |        "1                                                                    NaN   \n",
418 |        "2                                                                    NaN   \n",
419 |        "3                                                                    NaN   \n",
420 |        "4                                                                    NaN   \n",
421 |        "\n",
422 |        "                                                             pred_titles  \\\n",
423 |        "0  ['Trinity Episcopal Church (Houghton, Michigan)', 'Trinity Episcop...   \n",
424 |        "1                                                                     []   \n",
425 |        "2                                                                     []   \n",
426 |        "3                                                                     []   \n",
427 |        "4                                                                     []   \n",
428 |        "\n",
429 |        "  top5_success  \n",
430 |        "0   ✔️ [0.333]  \n",
431 |        "1               \n",
432 |        "2               \n",
433 |        "3               \n",
434 |        "4               "
435 |       ]
436 |      },
437 |      "metadata": {},
438 |      "output_type": "display_data"
439 |     },
440 |     {
441 |      "data": {
442 |       "text/html": [
443 |        "\n",
444 |        "                <div style='\n",
445 |        "                    text-align: center;\n",
446 |        "                    font-size: 16px;\n",
447 |        "                    font-weight: bold;\n",
448 |        "                    color: #555;\n",
449 |        "                    margin: 10px 0;'>\n",
450 |        "                    ... 95 more rows not displayed ...\n",
451 |        "                </div>\n",
452 |        "                "
453 |       ],
454 |       "text/plain": [
455 |        "<IPython.core.display.HTML object>"
456 |       ]
457 |      },
458 |      "metadata": {},
459 |      "output_type": "display_data"
460 |     },
461 |     {
462 |      "data": {
463 |       "text/plain": [
464 |        "8.0"
465 |       ]
466 |      },
467 |      "execution_count": 11,
468 |      "metadata": {},
469 |      "output_type": "execute_result"
470 |     }
471 |    ],
472 |    "source": [
473 |     "def safe_react(claim: str):\n",
474 |     "    try:\n",
475 |     "        return react(claim=claim)\n",
476 |     "    except Exception as e:\n",
477 |     "        return dspy.Prediction(titles=[])\n",
478 |     "\n",
479 |     "evaluate(safe_react)"
480 |    ]
481 |   },
482 |   {
483 |    "cell_type": "markdown",
484 |    "metadata": {},
485 |    "source": [
486 |     "<details>\n",
487 |     "<summary>Tracking Evaluation Results in MLflow Experiment</summary>\n",
488 |     "\n",
489 |     "<br/>\n",
490 |     "\n",
491 |     "To track and visualize the evaluation results over time, you can record the results in MLflow Experiment.\n",
492 |     "\n",
493 |     "\n",
494 |     "```python\n",
495 |     "import mlflow\n",
496 |     "\n",
497 |     "with mlflow.start_run(run_name=\"agent_evaluation\"):\n",
498 |     "    evaluate = dspy.Evaluate(\n",
499 |     "        devset=devset,\n",
500 |     "        metric=top5_recall,\n",
501 |     "        num_threads=16,\n",
502 |     "        display_progress=True,\n",
503 |     "    )\n",
504 |     "\n",
505 |     "    # Evaluate the program as usual\n",
506 |     "    result = evaluate(cot)\n",
507 |     "\n",
508 |     "    # Log the aggregated score\n",
509 |     "    mlflow.log_metric(\"top5_recall\", result.score)\n",
510 |     "    # Log the detailed evaluation results as a table\n",
511 |     "    mlflow.log_table(\n",
512 |     "        {\n",
513 |     "            \"Claim\": [example.claim for example in eval_set],\n",
514 |     "            \"Expected Titles\": [example.titles for example in eval_set],\n",
515 |     "            \"Predicted Titles\": [output[1] for output in result.results],\n",
516 |     "            \"Top 5 Recall\": [output[2] for output in result.results],\n",
517 |     "        },\n",
518 |     "        artifact_file=\"eval_results.json\",\n",
519 |     "    )\n",
520 |     "```\n",
521 |     "\n",
522 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
523 |     "\n",
524 |     "</details>"
525 |    ]
526 |   },
527 |   {
528 |    "cell_type": "markdown",
529 |    "metadata": {},
530 |    "source": [
531 |     "Wow. It only scores 8% in terms of recall. Not that good!\n",
532 |     "\n",
533 |     "Let's now optimize the two prompts inside `dspy.ReAct` jointly to maximize the recall of our agent. This may take around 30 minutes and make some $5 worth of calls to GPT-4o to optimize Llama-3.2-3B."
534 |    ]
535 |   },
536 |   {
537 |    "cell_type": "code",
538 |    "execution_count": 12,
539 |    "metadata": {},
540 |    "outputs": [],
541 |    "source": [
542 |     "kwargs = dict(teacher_settings=dict(lm=gpt4o), prompt_model=gpt4o, max_errors=999)\n",
543 |     "\n",
544 |     "tp = dspy.MIPROv2(metric=top5_recall, auto=\"medium\", num_threads=16, **kwargs)\n",
545 |     "optimized_react = tp.compile(react, trainset=trainset, max_bootstrapped_demos=3, max_labeled_demos=0)"
546 |    ]
547 |   },
548 |   {
549 |    "cell_type": "markdown",
550 |    "metadata": {},
551 |    "source": [
552 |     "Let's now evaluate again, after optimization."
553 |    ]
554 |   },
555 |   {
556 |    "cell_type": "code",
557 |    "execution_count": 13,
558 |    "metadata": {},
559 |    "outputs": [
560 |     {
561 |      "name": "stdout",
562 |      "output_type": "stream",
563 |      "text": [
564 |       "Average Metric: 41.67 / 100 (41.7%): 100%|██████████| 100/100 [03:00<00:00,  1.81s/it]"
565 |      ]
566 |     },
567 |     {
568 |      "name": "stderr",
569 |      "output_type": "stream",
570 |      "text": [
571 |       "2024/12/17 15:12:06 INFO dspy.evaluate.evaluate: Average Metric: 41.66666666666667 / 100 (41.7%)\n"
572 |      ]
573 |     },
574 |     {
575 |      "name": "stdout",
576 |      "output_type": "stream",
577 |      "text": [
578 |       "\n"
579 |      ]
580 |     },
581 |     {
582 |      "data": {
583 |       "text/html": [
584 |        "<div>\n",
585 |        "<style scoped>\n",
586 |        "    .dataframe tbody tr th:only-of-type {\n",
587 |        "        vertical-align: middle;\n",
588 |        "    }\n",
589 |        "\n",
590 |        "    .dataframe tbody tr th {\n",
591 |        "        vertical-align: top;\n",
592 |        "    }\n",
593 |        "\n",
594 |        "    .dataframe thead th {\n",
595 |        "        text-align: right;\n",
596 |        "    }\n",
597 |        "</style>\n",
598 |        "<table border=\"1\" class=\"dataframe\">\n",
599 |        "  <thead>\n",
600 |        "    <tr style=\"text-align: right;\">\n",
601 |        "      <th></th>\n",
602 |        "      <th>claim</th>\n",
603 |        "      <th>example_titles</th>\n",
604 |        "      <th>trajectory</th>\n",
605 |        "      <th>reasoning</th>\n",
606 |        "      <th>pred_titles</th>\n",
607 |        "      <th>top5_success</th>\n",
608 |        "    </tr>\n",
609 |        "  </thead>\n",
610 |        "  <tbody>\n",
611 |        "    <tr>\n",
612 |        "      <th>0</th>\n",
613 |        "      <td>The Church of England's movement that inspired the Trinity Episcop...</td>\n",
614 |        "      <td>[Oxford Movement, Trinity Episcopal Church (Houghton, Michigan), S...</td>\n",
615 |        "      <td>{'thought_0': 'To verify the claim, I need to identify the Church ...</td>\n",
616 |        "      <td>The claim states that the Church of England's movement that inspir...</td>\n",
617 |        "      <td>['Trinity Episcopal Church (Houghton, Michigan)', 'Church of All S...</td>\n",
618 |        "      <td>✔️ [0.667]</td>\n",
619 |        "    </tr>\n",
620 |        "    <tr>\n",
621 |        "      <th>1</th>\n",
622 |        "      <td>Red, White &amp; Crüe and this athlete both fight. The french fighter ...</td>\n",
623 |        "      <td>[Red, White &amp;amp; Crüe, Mike Tyson, Bobby Stewart]</td>\n",
624 |        "      <td>{'thought_0': 'To verify the claim, I need to identify the French ...</td>\n",
625 |        "      <td>The claim states that Red, White &amp; Crüe is a term applied to sport...</td>\n",
626 |        "      <td>[Bobby Stewart, Bernardin Ledoux Kingue Matam, Mötley Crüe, Milan ...</td>\n",
627 |        "      <td>✔️ [0.333]</td>\n",
628 |        "    </tr>\n",
629 |        "    <tr>\n",
630 |        "      <th>2</th>\n",
631 |        "      <td>The writer/director/actor from Glen or Glenda and Fernand Rivers s...</td>\n",
632 |        "      <td>[Ed Wood, Glen or Glenda, Fernand Rivers]</td>\n",
633 |        "      <td>{'thought_0': 'To verify the claim, I need to identify the writer/...</td>\n",
634 |        "      <td>The claim states that Glen or Glenda and Fernand Rivers share the ...</td>\n",
635 |        "      <td>[Ed Wood, Bela Lugosi, Dolores Fuller]</td>\n",
636 |        "      <td>✔️ [0.333]</td>\n",
637 |        "    </tr>\n",
638 |        "    <tr>\n",
639 |        "      <th>3</th>\n",
640 |        "      <td>The film by Sandi Sissel was released before The End of Suburbia.</td>\n",
641 |        "      <td>[Chicken Ranch (film), Sandi Sissel, The End of Suburbia]</td>\n",
642 |        "      <td>{'thought_0': 'To verify the claim, I need to find the release dat...</td>\n",
643 |        "      <td>The claim states that the film by Sandi Sissel was released before...</td>\n",
644 |        "      <td>[Sandi Sissel, The End of Suburbia (film)]</td>\n",
645 |        "      <td>✔️ [0.333]</td>\n",
646 |        "    </tr>\n",
647 |        "    <tr>\n",
648 |        "      <th>4</th>\n",
649 |        "      <td>The actor who played captain hook in the live production with Tayl...</td>\n",
650 |        "      <td>[Christopher Walken, Taylor Louderman, Peter Pan Live!]</td>\n",
651 |        "      <td>{'thought_0': 'To verify the claim, I need to find the actor who p...</td>\n",
652 |        "      <td>The claim suggests that the actor who played Captain Hook in the l...</td>\n",
653 |        "      <td>[Cyril Ritchard, Ruth Connell]</td>\n",
654 |        "      <td></td>\n",
655 |        "    </tr>\n",
656 |        "  </tbody>\n",
657 |        "</table>\n",
658 |        "</div>"
659 |       ],
660 |       "text/plain": [
661 |        "                                                                   claim  \\\n",
662 |        "0  The Church of England's movement that inspired the Trinity Episcop...   \n",
663 |        "1  Red, White & Crüe and this athlete both fight. The french fighter ...   \n",
664 |        "2  The writer/director/actor from Glen or Glenda and Fernand Rivers s...   \n",
665 |        "3      The film by Sandi Sissel was released before The End of Suburbia.   \n",
666 |        "4  The actor who played captain hook in the live production with Tayl...   \n",
667 |        "\n",
668 |        "                                                          example_titles  \\\n",
669 |        "0  [Oxford Movement, Trinity Episcopal Church (Houghton, Michigan), S...   \n",
670 |        "1                     [Red, White &amp; Crüe, Mike Tyson, Bobby Stewart]   \n",
671 |        "2                              [Ed Wood, Glen or Glenda, Fernand Rivers]   \n",
672 |        "3              [Chicken Ranch (film), Sandi Sissel, The End of Suburbia]   \n",
673 |        "4                [Christopher Walken, Taylor Louderman, Peter Pan Live!]   \n",
674 |        "\n",
675 |        "                                                              trajectory  \\\n",
676 |        "0  {'thought_0': 'To verify the claim, I need to identify the Church ...   \n",
677 |        "1  {'thought_0': 'To verify the claim, I need to identify the French ...   \n",
678 |        "2  {'thought_0': 'To verify the claim, I need to identify the writer/...   \n",
679 |        "3  {'thought_0': 'To verify the claim, I need to find the release dat...   \n",
680 |        "4  {'thought_0': 'To verify the claim, I need to find the actor who p...   \n",
681 |        "\n",
682 |        "                                                               reasoning  \\\n",
683 |        "0  The claim states that the Church of England's movement that inspir...   \n",
684 |        "1  The claim states that Red, White & Crüe is a term applied to sport...   \n",
685 |        "2  The claim states that Glen or Glenda and Fernand Rivers share the ...   \n",
686 |        "3  The claim states that the film by Sandi Sissel was released before...   \n",
687 |        "4  The claim suggests that the actor who played Captain Hook in the l...   \n",
688 |        "\n",
689 |        "                                                             pred_titles  \\\n",
690 |        "0  ['Trinity Episcopal Church (Houghton, Michigan)', 'Church of All S...   \n",
691 |        "1  [Bobby Stewart, Bernardin Ledoux Kingue Matam, Mötley Crüe, Milan ...   \n",
692 |        "2                                 [Ed Wood, Bela Lugosi, Dolores Fuller]   \n",
693 |        "3                             [Sandi Sissel, The End of Suburbia (film)]   \n",
694 |        "4                                         [Cyril Ritchard, Ruth Connell]   \n",
695 |        "\n",
696 |        "  top5_success  \n",
697 |        "0   ✔️ [0.667]  \n",
698 |        "1   ✔️ [0.333]  \n",
699 |        "2   ✔️ [0.333]  \n",
700 |        "3   ✔️ [0.333]  \n",
701 |        "4               "
702 |       ]
703 |      },
704 |      "metadata": {},
705 |      "output_type": "display_data"
706 |     },
707 |     {
708 |      "data": {
709 |       "text/html": [
710 |        "\n",
711 |        "                <div style='\n",
712 |        "                    text-align: center;\n",
713 |        "                    font-size: 16px;\n",
714 |        "                    font-weight: bold;\n",
715 |        "                    color: #555;\n",
716 |        "                    margin: 10px 0;'>\n",
717 |        "                    ... 95 more rows not displayed ...\n",
718 |        "                </div>\n",
719 |        "                "
720 |       ],
721 |       "text/plain": [
722 |        "<IPython.core.display.HTML object>"
723 |       ]
724 |      },
725 |      "metadata": {},
726 |      "output_type": "display_data"
727 |     },
728 |     {
729 |      "data": {
730 |       "text/plain": [
731 |        "41.67"
732 |       ]
733 |      },
734 |      "execution_count": 14,
735 |      "metadata": {},
736 |      "output_type": "execute_result"
737 |     }
738 |    ],
739 |    "source": [
740 |     "evaluate(optimized_react)"
741 |    ]
742 |   },
743 |   {
744 |    "cell_type": "markdown",
745 |    "metadata": {},
746 |    "source": [
747 |     "Awesome. It looks like the system improved drastically from 8% recall to around 40% recall. That was a pretty straightforward approach, but DSPy gives you many tools to continue iterating on this from here.\n",
748 |     "\n",
749 |     "Next, let's inspect the optimized prompts to understand what it has learned. We'll run one query and then inspect the last two prompts, which will show us the prompts used for both ReAct sub-modules, the one that does the agentic loop and the other than prepares the final results. (Alternatively, if you enabled MLflow Tracing following the instructions above, you can see all steps done by the agent including LLM calls, prompts, tool execution, in a rich tree-view.)"
750 |    ]
751 |   },
752 |   {
753 |    "cell_type": "code",
754 |    "execution_count": 15,
755 |    "metadata": {},
756 |    "outputs": [
757 |     {
758 |      "data": {
759 |       "text/plain": [
760 |        "['Bernard-Marie Koltès', 'Joe Orton']"
761 |       ]
762 |      },
763 |      "execution_count": 16,
764 |      "metadata": {},
765 |      "output_type": "execute_result"
766 |     }
767 |    ],
768 |    "source": [
769 |     "optimized_react(claim=\"The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Koltès are both playwrights.\").titles"
770 |    ]
771 |   },
772 |   {
773 |    "cell_type": "code",
774 |    "execution_count": 17,
775 |    "metadata": {},
776 |    "outputs": [
777 |     {
778 |      "name": "stdout",
779 |      "output_type": "stream",
780 |      "text": [
781 |       "\n",
782 |       "\n",
783 |       "\n",
784 |       "\n",
785 |       "\u001b[34m[2024-12-17T15:13:25.420335]\u001b[0m\n",
786 |       "\n",
787 |       "\u001b[31mSystem message:\u001b[0m\n",
788 |       "\n",
789 |       "Your input fields are:\n",
790 |       "1. `claim` (str)\n",
791 |       "2. `trajectory` (str)\n",
792 |       "\n",
793 |       "Your output fields are:\n",
794 |       "1. `next_thought` (str)\n",
795 |       "2. `next_tool_name` (Literal[search_wikipedia, lookup_wikipedia, finish])\n",
796 |       "3. `next_tool_args` (dict[str, Any])\n",
797 |       "\n",
798 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
799 |       "\n",
800 |       "[[ ## claim ## ]]\n",
801 |       "{claim}\n",
802 |       "\n",
803 |       "[[ ## trajectory ## ]]\n",
804 |       "{trajectory}\n",
805 |       "\n",
806 |       "[[ ## next_thought ## ]]\n",
807 |       "{next_thought}\n",
808 |       "\n",
809 |       "[[ ## next_tool_name ## ]]\n",
810 |       "{next_tool_name}        # note: the value you produce must be one of: search_wikipedia; lookup_wikipedia; finish\n",
811 |       "\n",
812 |       "[[ ## next_tool_args ## ]]\n",
813 |       "{next_tool_args}        # note: the value you produce must be pareseable according to the following JSON schema: {\"type\": \"object\"}\n",
814 |       "\n",
815 |       "[[ ## completed ## ]]\n",
816 |       "\n",
817 |       "In adhering to this structure, your objective is: \n",
818 |       "        Find all Wikipedia titles relevant to verifying (or refuting) the claim.\n",
819 |       "        \n",
820 |       "        You will be given `claim` and your goal is to finish with `titles`.\n",
821 |       "        \n",
822 |       "        To do this, you will interleave Thought, Tool Name, and Tool Args, and receive a resulting Observation.\n",
823 |       "        \n",
824 |       "        Thought can reason about the current situation, and Tool Name can be the following types:\n",
825 |       "        \n",
826 |       "        (1) search_wikipedia, whose description is <desc>Returns top-5 results and then the titles of the top-5 to top-30 results.</desc>. It takes arguments {'query': 'str'} in JSON format.\n",
827 |       "        (2) lookup_wikipedia, whose description is <desc>Returns the text of the Wikipedia page, if it exists.</desc>. It takes arguments {'title': 'str'} in JSON format.\n",
828 |       "        (3) finish, whose description is <desc>Signals that the final outputs, i.e. `titles`, are now available and marks the task as complete.</desc>. It takes arguments {} in JSON format.\n",
829 |       "\n",
830 |       "\n",
831 |       "\u001b[31mUser message:\u001b[0m\n",
832 |       "\n",
833 |       "[[ ## claim ## ]]\n",
834 |       "1990 Film that Khiladiyon Ka Khiladi is loosely based on stars this actor who is best known for martial arts action films.\n",
835 |       "\n",
836 |       "[[ ## trajectory ## ]]\n",
837 |       "[[ ## thought_0 ## ]]\n",
838 |       "To verify the claim, I need to identify the 1990 film that \"Khiladiyon Ka Khiladi\" is loosely based on and the actor known for martial arts action films who starred in it. I will start by searching for information on \"Khiladiyon Ka Khiladi\" to find details about its inspiration.\n",
839 |       "\n",
840 |       "[[ ## tool_name_0 ## ]]\n",
841 |       "search_wikipedia\n",
842 |       "\n",
843 |       "[[ ## tool_args_0 ## ]]\n",
844 |       "{\"query\": \"Khiladiyon Ka Khiladi\"}\n",
845 |       "\n",
846 |       "[[ ## observation_0 ## ]]\n",
847 |       "[1] «Khiladiyon Ka Khiladi | Khiladiyon Ka Khiladi (English: Player of Players) is a 1996 Indian action film starring Rekha in her first villain role, Akshay Kumar, Raveena Tandon and former WWE wrestlers \"Crush\" and Brian Lee as \"The Undertaker\". It was the 5th highest grossing movie of the year 1996 and was declared 'SuperHit' by Box Office India. It was the fourth installment in the Khiladi (film series). The movie is loosely based based on Hollywood film Lionheart»\n",
848 |       "[2] «Khiladi 420 | Khiladi 420 (English: \"Con Player\") is an Indian Hindi action film directed by Neeraj Vora and starring Akshay Kumar and Mahima Chaudhry. The film was written by Uttam Gudda and released on 29 December 2000. It is the seventh installment in the \"Khiladi\" series starring Kumar, which included \"Khiladi\" (1992), \"Main Khiladi Tu Anari\" (1994), \"Sabse Bada Khiladi\" (1995), \"Khiladiyon Ka Khiladi\" (1996), \"Mr. and Mrs. Khiladi\" (1997) and \"International Khiladi\" (1999).»\n",
849 |       "[3] «Khiladi (1992 film) | Khiladi (English: \"Player\" ) is a 1992 Indian suspense thriller film directed by Abbas Mustan. The film was Akshay Kumar's breakthrough role and also stars Ayesha Jhulka, Deepak Tijori, Sabeeha. While Prem Chopra, Shakti Kapoor, Anant Mahadevan and Johnny Lever played supporting roles. \"Khiladi\" was the first installment in the Khiladi (film series) which had \"Khiladi\" in the title and Akshay Kumar in the leading role. It was followed by \"Main Khiladi Tu Anari\" (1994), \"Sabse Bada Khiladi\" (1995), \"Khiladiyon Ka Khiladi\" (1996), \"Mr. and Mrs. Khiladi\" (1997), \"International Khiladi\" (1999), \"Khiladi 420\"(2000) and \"Khiladi 786\" (2012). Khiladi was critically and commercially success at the box-office and the tenth highest grossing film of 1992. It was Akshay Kumar's first successful movie and was declared a \"Super Hit\" at the box office. The basic premise of the story is similar to 1975 released movie Khel Khel Mein starring Rishi Kapoor and Neetu Singh. The film was remade in Kannada as \"Aata Hudugaata\".»\n",
850 |       "[4] «Khiladi (film series) | Khiladi series is a Bollywood action film series starring Akshay Kumar in the lead role. However, unlike other film series, other than having Akshay Kumar in lead role, and other than having the word \"Khiladi\" in the title, these films have nothing in common. The producers, directors and stories of these films are totally different. \" Khiladi\" (1992) was the first in a series of films which had Akshay Kumar in the title role and gave it his first breakthrough role. It was followed by \"Main Khiladi Tu Anari\" (1994), \"Sabse Bada Khiladi\" (1995), \"Khiladiyon Ka Khiladi\" (1996), \"Mr. and Mrs. Khiladi\" (1997), \"International Khiladi\" (1999) and \"Khiladi 420\" (2000), all featuring Kumar in the lead role. The latest film in the franchise is \"Khiladi 786\" (2012).»\n",
851 |       "[5] «Khiladi 786 | Khiladi 786 (खिलाड़ी 786) is a 2012 Indian Hindi Punjabi action comedy film directed by Ashish R Mohan, featuring Akshay Kumar in the title role alongside Asin playing the female lead. It features Himesh Reshammiya, Mithun Chakraborty, Raj Babbar and Mukesh Rishi in supporting roles. The film marks the return of Akshay Kumar to his \"Khiladi\" series after 12 years. It is mostly shot in Mumbai and Punjab. Khiladi786 is AkshayKumar 100th film.»\n",
852 |       "[6] «Other retrieved pages have titles: `Khiladi (2013 film)`, `Khiladi (1968 film)`, `International Khiladi`, `Khatron Ke Khiladi (1988 film)`, `Khatron Ke Khiladi (2001 film)`, `Umesh Mehra`, `Shatranj ke Khiladi`, `Main Khiladi Tu Anari`, `Fear Factor: Khatron Ke Khiladi (season 7)`, `Fear Factor: Khatron Ke Khiladi`, `Mr. and Mrs. Khiladi`, `Sabse Bada Khiladi`, `Ek Khiladi Bawan Pattey`, `Ek Khiladi Ek Haseena (TV series)`, `Do Khiladi`, `Ek Khiladi Ek Haseena (film)`, `Khel Khilari Ka`, `Fear Factor: Khatron Ke Khiladi (season 8)`, `Fear Factor: Khatron Ke Khiladi Darr Ka Blockbuster`, `Khilona (1942 film)`, `Fear Factor: Khatron Ke Khiladi Darr Ka Blockbuster Returns`, `Chhota Bheem: Dholakpur ke Khiladi`, `Khilani`, `Kandireega`, `Khilaaf`.»\n",
853 |       "\n",
854 |       "[[ ## thought_1 ## ]]\n",
855 |       "The search results indicate that \"Khiladiyon Ka Khiladi\" is loosely based on the Hollywood film \"Lionheart\". I should now verify which actor, known for martial arts action films, starred in \"Lionheart\".\n",
856 |       "\n",
857 |       "[[ ## tool_name_1 ## ]]\n",
858 |       "search_wikipedia\n",
859 |       "\n",
860 |       "[[ ## tool_args_1 ## ]]\n",
861 |       "{\"query\": \"Lionheart 1990 film\"}\n",
862 |       "\n",
863 |       "[[ ## observation_1 ## ]]\n",
864 |       "[1] «Lionheart (1990 film) | Lionheart (also known as Wrong Bet, A.W.O.L.: Absent Without Leave, Leon and Full Contact) is a 1990 action film, directed by Sheldon Lettich, starring Jean-Claude Van Damme and co-starring Brian Thompson, along with Harrison Page, Deborah Rennard, Lisa Pelikan, and Ashley Johnson.»\n",
865 |       "[2] «Truly, Madly, Deeply | Truly, Madly, Deeply is a 1990 British fantasy drama film made for the BBC's \"Screen Two\" series, by BBC Films, Lionheart and Winston Pictures. The film, written and directed by Anthony Minghella, stars Juliet Stevenson and Alan Rickman.»\n",
866 |       "[3] «Lionheart (1987 film) | Lionheart, also known as Lionheart: The Children's Crusade, is a 1987 adventure film directed by Franklin J. Schaffner and produced by Talia Shire and Stanley O'Toole. Shire's brother, Francis Ford Coppola, initially planned to direct the film but instead opted to be executive producer along with Shire's husband, Jack Schwartzman. The screenplay was written by Menno Meyjes and Richard Outten from a story by Meyjes. The composer Jerry Goldsmith wrote the score. The film was released in August 1987. It was distributed by Orion Pictures.»\n",
867 |       "[4] «Lionheart (2016 film) | Lionheart is a 2016 American boxing film short written and produced by Oscar DeRosa and Orlando Cicilia III. The film stars Oscar DeRosa and Marc Macaulay. The film portrays struggling professional boxer Max Rossi who is finally presented with the fight he's been waiting for that will launch his career to the next level but when he is suddenly confronted with a life-changing opportunity he must decide which path to take.»\n",
868 |       "[5] «Richard the Lionheart (2013 film) | Richard The Lionheart is a 2013 film, starring Chandler Maness as Richard the Lionheart, Malcolm McDowell as King Henry II and Andrea Zirio as Henry the Young.»\n",
869 |       "[6] «Other retrieved pages have titles: `Richard the Lionheart: Rebellion`, `The Brothers Lionheart (1977 film)`, `Kodama Simham`, `Jean-Claude Van Damme`, `Lionsheart`, `A Day for Lionhearts`, `The Lion in Winter (1968 film)`, `Lion Jagapathi Rao`, `Theatre of Blood`, `Young Lions (album)`, `Richard the Lion-Hearted (1923 film)`, `Bullseye! (1990 film)`, `Fear (1990 film)`, `The Guard (1990 film)`, `Ironheart`, `The Guardian (1990 film)`, `Tiger Cage 2`, `Lion Rampant (game publisher)`, `Success (1991 film)`, `Lion in the Streets`, `The King of the Kickboxers`, `Lionheart (video game)`, `Richard the Lionheart (TV series)`, `Dermot Walsh`, `Lars Söderdahl`.»\n",
870 |       "\n",
871 |       "Respond with the corresponding output fields, starting with the field `[[ ## next_thought ## ]]`, then `[[ ## next_tool_name ## ]]` (must be formatted as a valid Python Literal[search_wikipedia, lookup_wikipedia, finish]), then `[[ ## next_tool_args ## ]]` (must be formatted as a valid Python dict[str, Any]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
872 |       "\n",
873 |       "\n",
874 |       "\u001b[31mAssistant message:\u001b[0m\n",
875 |       "\n",
876 |       "[[ ## next_thought ## ]]\n",
877 |       "The search results confirm that the 1990 film \"Lionheart\" stars Jean-Claude Van Damme, who is well-known for his martial arts action films. This information aligns with what is needed to verify the claim. I now have the relevant titles related to the claim: \"Khiladiyon Ka Khiladi\" and \"Lionheart.\"\n",
878 |       "\n",
879 |       "[[ ## next_tool_name ## ]]\n",
880 |       "finish\n",
881 |       "\n",
882 |       "[[ ## next_tool_args ## ]]\n",
883 |       "{}\n",
884 |       "\n",
885 |       "[[ ## completed ## ]]\n",
886 |       "\n",
887 |       "\n",
888 |       "\u001b[31mUser message:\u001b[0m\n",
889 |       "\n",
890 |       "[[ ## claim ## ]]\n",
891 |       "The figure who currently holds the role Michael Noonan held from 2001 to 2002, was the Minister for Social Protection from 2016 until 2017.\n",
892 |       "\n",
893 |       "[[ ## trajectory ## ]]\n",
894 |       "\n",
895 |       "\n",
896 |       "Respond with the corresponding output fields, starting with the field `[[ ## next_thought ## ]]`, then `[[ ## next_tool_name ## ]]` (must be formatted as a valid Python Literal[search_wikipedia, lookup_wikipedia, finish]), then `[[ ## next_tool_args ## ]]` (must be formatted as a valid Python dict[str, Any]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
897 |       "\n",
898 |       "\n",
899 |       "\u001b[31mAssistant message:\u001b[0m\n",
900 |       "\n",
901 |       "[[ ## next_thought ## ]]\n",
902 |       "To verify the claim, I need to identify the role Michael Noonan held from 2001 to 2002. Then, I can find out who currently holds this role and check if they were the Minister for Social Protection from 2016 to 2017.\n",
903 |       "\n",
904 |       "[[ ## next_tool_name ## ]]\n",
905 |       "search_wikipedia\n",
906 |       "\n",
907 |       "[[ ## next_tool_args ## ]]\n",
908 |       "{\"query\": \"Michael Noonan role 2001 to 2002\"}\n",
909 |       "\n",
910 |       "[[ ## completed ## ]]\n",
[TRUNCATED]
```

tutorials/ai_text_game/index.md
```
1 | # Building a Creative Text-Based AI Game with DSPy
2 | 
3 | This tutorial demonstrates how to create an interactive text-based adventure game using DSPy's modular programming approach. You'll build a dynamic game where AI handles narrative generation, character interactions, and adaptive gameplay.
4 | 
5 | ## What You'll Build
6 | 
7 | An intelligent text-based adventure game featuring:
8 | 
9 | - Dynamic story generation and branching narratives
10 | - AI-powered character interactions and dialogue
11 | - Adaptive gameplay that responds to player choices
12 | - Inventory and character progression systems
13 | - Save/load game state functionality
14 | 
15 | ## Setup
16 | 
17 | ```bash
18 | pip install dspy rich typer
19 | ```
20 | 
21 | ## Step 1: Core Game Framework
22 | 
23 | ```python
24 | import dspy
25 | import json
26 | from typing import Dict, List, Optional, Any
27 | from dataclasses import dataclass, field
28 | from enum import Enum
29 | import random
30 | from rich.console import Console
31 | from rich.panel import Panel
32 | from rich.text import Text
33 | import typer
34 | 
35 | # Configure DSPy
36 | lm = dspy.LM(model='openai/gpt-4o-mini')
37 | dspy.configure(lm=lm)
38 | 
39 | console = Console()
40 | 
41 | class GameState(Enum):
42 |     MENU = "menu"
43 |     PLAYING = "playing"
44 |     INVENTORY = "inventory"
45 |     CHARACTER = "character"
46 |     GAME_OVER = "game_over"
47 | 
48 | @dataclass
49 | class Player:
50 |     name: str
51 |     health: int = 100
52 |     level: int = 1
53 |     experience: int = 0
54 |     inventory: list[str] = field(default_factory=list)
55 |     skills: dict[str, int] = field(default_factory=lambda: {
56 |         "strength": 10,
57 |         "intelligence": 10,
58 |         "charisma": 10,
59 |         "stealth": 10
60 |     })
61 |     
62 |     def add_item(self, item: str):
63 |         self.inventory.append(item)
64 |         console.print(f"[green]Added {item} to inventory![/green]")
65 |     
66 |     def remove_item(self, item: str) -> bool:
67 |         if item in self.inventory:
68 |             self.inventory.remove(item)
69 |             return True
70 |         return False
71 |     
72 |     def gain_experience(self, amount: int):
73 |         self.experience += amount
74 |         old_level = self.level
75 |         self.level = 1 + (self.experience // 100)
76 |         if self.level > old_level:
77 |             console.print(f"[bold yellow]Level up! You are now level {self.level}![/bold yellow]")
78 | 
79 | @dataclass
80 | class GameContext:
81 |     current_location: str = "Village Square"
82 |     story_progress: int = 0
83 |     visited_locations: list[str] = field(default_factory=list)
84 |     npcs_met: list[str] = field(default_factory=list)
85 |     completed_quests: list[str] = field(default_factory=list)
86 |     game_flags: dict[str, bool] = field(default_factory=dict)
87 |     
88 |     def add_flag(self, flag: str, value: bool = True):
89 |         self.game_flags[flag] = value
90 |     
91 |     def has_flag(self, flag: str) -> bool:
92 |         return self.game_flags.get(flag, False)
93 | 
94 | class GameEngine:
95 |     def __init__(self):
96 |         self.player = None
97 |         self.context = GameContext()
98 |         self.state = GameState.MENU
99 |         self.running = True
100 |         
101 |     def save_game(self, filename: str = "savegame.json"):
102 |         """Save current game state."""
103 |         save_data = {
104 |             "player": {
105 |                 "name": self.player.name,
106 |                 "health": self.player.health,
107 |                 "level": self.player.level,
108 |                 "experience": self.player.experience,
109 |                 "inventory": self.player.inventory,
110 |                 "skills": self.player.skills
111 |             },
112 |             "context": {
113 |                 "current_location": self.context.current_location,
114 |                 "story_progress": self.context.story_progress,
115 |                 "visited_locations": self.context.visited_locations,
116 |                 "npcs_met": self.context.npcs_met,
117 |                 "completed_quests": self.context.completed_quests,
118 |                 "game_flags": self.context.game_flags
119 |             }
120 |         }
121 |         
122 |         with open(filename, 'w') as f:
123 |             json.dump(save_data, f, indent=2)
124 |         console.print(f"[green]Game saved to {filename}![/green]")
125 |     
126 |     def load_game(self, filename: str = "savegame.json") -> bool:
127 |         """Load game state from file."""
128 |         try:
129 |             with open(filename, 'r') as f:
130 |                 save_data = json.load(f)
131 |             
132 |             # Reconstruct player
133 |             player_data = save_data["player"]
134 |             self.player = Player(
135 |                 name=player_data["name"],
136 |                 health=player_data["health"],
137 |                 level=player_data["level"],
138 |                 experience=player_data["experience"],
139 |                 inventory=player_data["inventory"],
140 |                 skills=player_data["skills"]
141 |             )
142 |             
143 |             # Reconstruct context
144 |             context_data = save_data["context"]
145 |             self.context = GameContext(
146 |                 current_location=context_data["current_location"],
147 |                 story_progress=context_data["story_progress"],
148 |                 visited_locations=context_data["visited_locations"],
149 |                 npcs_met=context_data["npcs_met"],
150 |                 completed_quests=context_data["completed_quests"],
151 |                 game_flags=context_data["game_flags"]
152 |             )
153 |             
154 |             console.print(f"[green]Game loaded from {filename}![/green]")
155 |             return True
156 |             
157 |         except FileNotFoundError:
158 |             console.print(f"[red]Save file {filename} not found![/red]")
159 |             return False
160 |         except Exception as e:
161 |             console.print(f"[red]Error loading game: {e}![/red]")
162 |             return False
163 | 
164 | # Initialize game engine
165 | game = GameEngine()
166 | ```
167 | 
168 | ## Step 2: AI-Powered Story Generation
169 | 
170 | ```python
171 | class StoryGenerator(dspy.Signature):
172 |     """Generate dynamic story content based on current game state."""
173 |     location: str = dspy.InputField(desc="Current location")
174 |     player_info: str = dspy.InputField(desc="Player information and stats")
175 |     story_progress: int = dspy.InputField(desc="Current story progress level")
176 |     recent_actions: str = dspy.InputField(desc="Player's recent actions")
177 |     
178 |     scene_description: str = dspy.OutputField(desc="Vivid description of current scene")
179 |     available_actions: list[str] = dspy.OutputField(desc="List of possible player actions")
180 |     npcs_present: list[str] = dspy.OutputField(desc="NPCs present in this location")
181 |     items_available: list[str] = dspy.OutputField(desc="Items that can be found or interacted with")
182 | 
183 | class DialogueGenerator(dspy.Signature):
184 |     """Generate NPC dialogue and responses."""
185 |     npc_name: str = dspy.InputField(desc="Name and type of NPC")
186 |     npc_personality: str = dspy.InputField(desc="NPC personality and background")
187 |     player_input: str = dspy.InputField(desc="What the player said or did")
188 |     context: str = dspy.InputField(desc="Current game context and history")
189 |     
190 |     npc_response: str = dspy.OutputField(desc="NPC's dialogue response")
191 |     mood_change: str = dspy.OutputField(desc="How NPC's mood changed (positive/negative/neutral)")
192 |     quest_offered: bool = dspy.OutputField(desc="Whether NPC offers a quest")
193 |     information_revealed: str = dspy.OutputField(desc="Any important information shared")
194 | 
195 | class ActionResolver(dspy.Signature):
196 |     """Resolve player actions and determine outcomes."""
197 |     action: str = dspy.InputField(desc="Player's chosen action")
198 |     player_stats: str = dspy.InputField(desc="Player's current stats and skills")
199 |     context: str = dspy.InputField(desc="Current game context")
200 |     difficulty: str = dspy.InputField(desc="Difficulty level of the action")
201 |     
202 |     success: bool = dspy.OutputField(desc="Whether the action succeeded")
203 |     outcome_description: str = dspy.OutputField(desc="Description of what happened")
204 |     stat_changes: dict[str, int] = dspy.OutputField(desc="Changes to player stats")
205 |     items_gained: list[str] = dspy.OutputField(desc="Items gained from this action")
206 |     experience_gained: int = dspy.OutputField(desc="Experience points gained")
207 | 
208 | class GameAI(dspy.Module):
209 |     """Main AI module for game logic and narrative."""
210 |     
211 |     def __init__(self):
212 |         super().__init__()
213 |         self.story_gen = dspy.ChainOfThought(StoryGenerator)
214 |         self.dialogue_gen = dspy.ChainOfThought(DialogueGenerator)
215 |         self.action_resolver = dspy.ChainOfThought(ActionResolver)
216 |     
217 |     def generate_scene(self, player: Player, context: GameContext, recent_actions: str = "") -> Dict:
218 |         """Generate current scene description and options."""
219 |         
220 |         player_info = f"Level {player.level} {player.name}, Health: {player.health}, Skills: {player.skills}"
221 |         
222 |         scene = self.story_gen(
223 |             location=context.current_location,
224 |             player_info=player_info,
225 |             story_progress=context.story_progress,
226 |             recent_actions=recent_actions
227 |         )
228 |         
229 |         return {
230 |             "description": scene.scene_description,
231 |             "actions": scene.available_actions,
232 |             "npcs": scene.npcs_present,
233 |             "items": scene.items_available
234 |         }
235 |     
236 |     def handle_dialogue(self, npc_name: str, player_input: str, context: GameContext) -> Dict:
237 |         """Handle conversation with NPCs."""
238 |         
239 |         # Create NPC personality based on name and context
240 |         personality_map = {
241 |             "Village Elder": "Wise, knowledgeable, speaks in riddles, has ancient knowledge",
242 |             "Merchant": "Greedy but fair, loves to bargain, knows about valuable items",
243 |             "Guard": "Dutiful, suspicious of strangers, follows rules strictly",
244 |             "Thief": "Sneaky, untrustworthy, has information about hidden things",
245 |             "Wizard": "Mysterious, powerful, speaks about magic and ancient forces"
246 |         }
247 |         
248 |         personality = personality_map.get(npc_name, "Friendly villager with local knowledge")
249 |         game_context = f"Location: {context.current_location}, Story progress: {context.story_progress}"
250 |         
251 |         response = self.dialogue_gen(
252 |             npc_name=npc_name,
253 |             npc_personality=personality,
254 |             player_input=player_input,
255 |             context=game_context
256 |         )
257 |         
258 |         return {
259 |             "response": response.npc_response,
260 |             "mood": response.mood_change,
261 |             "quest": response.quest_offered,
262 |             "info": response.information_revealed
263 |         }
264 |     
265 |     def resolve_action(self, action: str, player: Player, context: GameContext) -> Dict:
266 |         """Resolve player actions and determine outcomes."""
267 |         
268 |         player_stats = f"Level {player.level}, Health {player.health}, Skills: {player.skills}"
269 |         game_context = f"Location: {context.current_location}, Progress: {context.story_progress}"
270 |         
271 |         # Determine difficulty based on action type
272 |         difficulty = "medium"
273 |         if any(word in action.lower() for word in ["fight", "battle", "attack"]):
274 |             difficulty = "hard"
275 |         elif any(word in action.lower() for word in ["look", "examine", "talk"]):
276 |             difficulty = "easy"
277 |         
278 |         result = self.action_resolver(
279 |             action=action,
280 |             player_stats=player_stats,
281 |             context=game_context,
282 |             difficulty=difficulty
283 |         )
284 |         
285 |         return {
286 |             "success": result.success,
287 |             "description": result.outcome_description,
288 |             "stat_changes": result.stat_changes,
289 |             "items": result.items_gained,
290 |             "experience": result.experience_gained
291 |         }
292 | 
293 | # Initialize AI
294 | ai = GameAI()
295 | ```
296 | 
297 | ## Step 3: Game Interface and Interaction
298 | 
299 | ```python
300 | def display_game_header():
301 |     """Display the game header."""
302 |     header = Text("🏰 MYSTIC REALM ADVENTURE 🏰", style="bold magenta")
303 |     console.print(Panel(header, style="bright_blue"))
304 | 
305 | def display_player_status(player: Player):
306 |     """Display player status panel."""
307 |     status = f"""
308 | [bold]Name:[/bold] {player.name}
309 | [bold]Level:[/bold] {player.level} (XP: {player.experience})
310 | [bold]Health:[/bold] {player.health}/100
311 | [bold]Skills:[/bold]
312 |   • Strength: {player.skills['strength']}
313 |   • Intelligence: {player.skills['intelligence']}
314 |   • Charisma: {player.skills['charisma']}
315 |   • Stealth: {player.skills['stealth']}
316 | [bold]Inventory:[/bold] {len(player.inventory)} items
317 |     """
318 |     console.print(Panel(status.strip(), title="Player Status", style="green"))
319 | 
320 | def display_location(context: GameContext, scene: Dict):
321 |     """Display current location and scene."""
322 |     location_panel = f"""
323 | [bold yellow]{context.current_location}[/bold yellow]
324 | 
325 | {scene['description']}
326 |     """
327 |     
328 |     if scene['npcs']:
329 |         location_panel += f"\n\n[bold]NPCs present:[/bold] {', '.join(scene['npcs'])}"
330 |     
331 |     if scene['items']:
332 |         location_panel += f"\n[bold]Items visible:[/bold] {', '.join(scene['items'])}"
333 |     
334 |     console.print(Panel(location_panel.strip(), title="Current Location", style="cyan"))
335 | 
336 | def display_actions(actions: list[str]):
337 |     """Display available actions."""
338 |     action_text = "\n".join([f"{i+1}. {action}" for i, action in enumerate(actions)])
339 |     console.print(Panel(action_text, title="Available Actions", style="yellow"))
340 | 
341 | def get_player_choice(max_choices: int) -> int:
342 |     """Get player's choice with input validation."""
343 |     while True:
344 |         try:
345 |             choice = typer.prompt("Choose an action (number)")
346 |             choice_num = int(choice)
347 |             if 1 <= choice_num <= max_choices:
348 |                 return choice_num - 1
349 |             else:
350 |                 console.print(f"[red]Please enter a number between 1 and {max_choices}[/red]")
351 |         except ValueError:
352 |             console.print("[red]Please enter a valid number[/red]")
353 | 
354 | def show_inventory(player: Player):
355 |     """Display player inventory."""
356 |     if not player.inventory:
357 |         console.print(Panel("Your inventory is empty.", title="Inventory", style="red"))
358 |     else:
359 |         items = "\n".join([f"• {item}" for item in player.inventory])
360 |         console.print(Panel(items, title="Inventory", style="green"))
361 | 
362 | def main_menu():
363 |     """Display main menu and handle selection."""
364 |     console.clear()
365 |     display_game_header()
366 |     
367 |     menu_options = [
368 |         "1. New Game",
369 |         "2. Load Game", 
370 |         "3. How to Play",
371 |         "4. Exit"
372 |     ]
373 |     
374 |     menu_text = "\n".join(menu_options)
375 |     console.print(Panel(menu_text, title="Main Menu", style="bright_blue"))
376 |     
377 |     choice = typer.prompt("Select an option")
378 |     return choice
379 | 
380 | def show_help():
381 |     """Display help information."""
382 |     help_text = """
383 | [bold]How to Play:[/bold]
384 | 
385 | • This is a text-based adventure game powered by AI
386 | • Make choices by selecting numbered options
387 | • Talk to NPCs to learn about the world and get quests
388 | • Explore different locations to find items and adventures
389 | • Your choices affect the story and character development
390 | • Use 'inventory' to check your items
391 | • Use 'status' to see your character info
392 | • Type 'save' to save your progress
393 | • Type 'quit' to return to main menu
394 | 
395 | [bold]Tips:[/bold]
396 | • Different skills affect your success in various actions
397 | • NPCs remember your previous interactions
398 | • Explore thoroughly - there are hidden secrets!
399 | • Your reputation affects how NPCs treat you
400 |     """
401 |     console.print(Panel(help_text.strip(), title="Game Help", style="blue"))
402 |     typer.prompt("Press Enter to continue")
403 | ```
404 | 
405 | ## Step 4: Main Game Loop
406 | 
407 | ```python
408 | def create_new_character():
409 |     """Create a new player character."""
410 |     console.clear()
411 |     display_game_header()
412 |     
413 |     name = typer.prompt("Enter your character's name")
414 |     
415 |     # Character creation with skill point allocation
416 |     console.print("\n[bold]Character Creation[/bold]")
417 |     console.print("You have 10 extra skill points to distribute among your skills.")
418 |     console.print("Base skills start at 10 each.\n")
419 |     
420 |     skills = {"strength": 10, "intelligence": 10, "charisma": 10, "stealth": 10}
421 |     points_remaining = 10
422 |     
423 |     for skill in skills.keys():
424 |         if points_remaining > 0:
425 |             console.print(f"Points remaining: {points_remaining}")
426 |             while True:
427 |                 try:
428 |                     points = int(typer.prompt(f"Points to add to {skill} (0-{points_remaining})"))
429 |                     if 0 <= points <= points_remaining:
430 |                         skills[skill] += points
431 |                         points_remaining -= points
432 |                         break
433 |                     else:
434 |                         console.print(f"[red]Enter a number between 0 and {points_remaining}[/red]")
435 |                 except ValueError:
436 |                     console.print("[red]Please enter a valid number[/red]")
437 |     
438 |     player = Player(name=name, skills=skills)
439 |     console.print(f"\n[green]Welcome to Mystic Realm, {name}![/green]")
440 |     return player
441 | 
442 | def game_loop():
443 |     """Main game loop."""
444 |     recent_actions = ""
445 |     
446 |     while game.running and game.state == GameState.PLAYING:
447 |         console.clear()
448 |         display_game_header()
449 |         
450 |         # Generate current scene
451 |         scene = ai.generate_scene(game.player, game.context, recent_actions)
452 |         
453 |         # Display game state
454 |         display_player_status(game.player)
455 |         display_location(game.context, scene)
456 |         
457 |         # Add standard actions
458 |         all_actions = scene['actions'] + ["Check inventory", "Character status", "Save game", "Quit to menu"]
459 |         display_actions(all_actions)
460 |         
461 |         # Get player choice
462 |         choice_idx = get_player_choice(len(all_actions))
463 |         chosen_action = all_actions[choice_idx]
464 |         
465 |         # Handle special commands
466 |         if chosen_action == "Check inventory":
467 |             show_inventory(game.player)
468 |             typer.prompt("Press Enter to continue")
469 |             continue
470 |         elif chosen_action == "Character status":
471 |             display_player_status(game.player)
472 |             typer.prompt("Press Enter to continue")
473 |             continue
474 |         elif chosen_action == "Save game":
475 |             game.save_game()
476 |             typer.prompt("Press Enter to continue")
477 |             continue
478 |         elif chosen_action == "Quit to menu":
479 |             game.state = GameState.MENU
480 |             break
481 |         
482 |         # Handle game actions
483 |         if chosen_action in scene['actions']:
484 |             # Check if it's dialogue with an NPC
485 |             npc_target = None
486 |             for npc in scene['npcs']:
487 |                 if npc.lower() in chosen_action.lower():
488 |                     npc_target = npc
489 |                     break
490 |             
491 |             if npc_target:
492 |                 # Handle NPC interaction
493 |                 console.print(f"\n[bold]Talking to {npc_target}...[/bold]")
494 |                 dialogue = ai.handle_dialogue(npc_target, chosen_action, game.context)
495 |                 
496 |                 console.print(f"\n[italic]{npc_target}:[/italic] \"{dialogue['response']}\"")
497 |                 
498 |                 if dialogue['quest']:
499 |                     console.print(f"[yellow]💼 Quest opportunity detected![/yellow]")
500 |                 
501 |                 if dialogue['info']:
502 |                     console.print(f"[blue]ℹ️  {dialogue['info']}[/blue]")
503 |                     
504 |                 # Add NPC to met list
505 |                 if npc_target not in game.context.npcs_met:
506 |                     game.context.npcs_met.append(npc_target)
507 |                 
508 |                 recent_actions = f"Talked to {npc_target}: {chosen_action}"
509 |             else:
510 |                 # Handle general action
511 |                 result = ai.resolve_action(chosen_action, game.player, game.context)
512 |                 
513 |                 console.print(f"\n{result['description']}")
514 |                 
515 |                 # Apply results
516 |                 if result['success']:
517 |                     console.print("[green]✅ Success![/green]")
518 |                     
519 |                     # Apply stat changes
520 |                     for stat, change in result['stat_changes'].items():
521 |                         if stat in game.player.skills:
522 |                             game.player.skills[stat] += change
523 |                             if change > 0:
524 |                                 console.print(f"[green]{stat.title()} increased by {change}![/green]")
525 |                         elif stat == "health":
526 |                             game.player.health = max(0, min(100, game.player.health + change))
527 |                             if change > 0:
528 |                                 console.print(f"[green]Health restored by {change}![/green]")
529 |                             elif change < 0:
530 |                                 console.print(f"[red]Health decreased by {abs(change)}![/red]")
531 |                     
532 |                     # Add items
533 |                     for item in result['items']:
534 |                         game.player.add_item(item)
535 |                     
536 |                     # Give experience
537 |                     if result['experience'] > 0:
538 |                         game.player.gain_experience(result['experience'])
539 |                     
540 |                     # Update story progress
541 |                     game.context.story_progress += 1
542 |                 else:
543 |                     console.print("[red]❌ The action didn't go as planned...[/red]")
544 |                 
545 |                 recent_actions = f"Attempted: {chosen_action}"
546 |             
547 |             # Check for game over conditions
548 |             if game.player.health <= 0:
549 |                 console.print("\n[bold red]💀 You have died! Game Over![/bold red]")
550 |                 game.state = GameState.GAME_OVER
551 |                 break
552 |             
553 |             typer.prompt("\nPress Enter to continue")
554 | 
555 | def main():
556 |     """Main game function."""
557 |     while game.running:
558 |         if game.state == GameState.MENU:
559 |             choice = main_menu()
560 |             
561 |             if choice == "1":
562 |                 game.player = create_new_character()
563 |                 game.context = GameContext()
564 |                 game.state = GameState.PLAYING
565 |                 console.print("\n[italic]Your adventure begins...[/italic]")
566 |                 typer.prompt("Press Enter to start")
567 |                 
568 |             elif choice == "2":
569 |                 if game.load_game():
570 |                     game.state = GameState.PLAYING
571 |                 typer.prompt("Press Enter to continue")
572 |                 
573 |             elif choice == "3":
574 |                 show_help()
575 |                 
576 |             elif choice == "4":
577 |                 game.running = False
578 |                 console.print("[bold]Thanks for playing! Goodbye![/bold]")
579 |             
580 |         elif game.state == GameState.PLAYING:
581 |             game_loop()
582 |             
583 |         elif game.state == GameState.GAME_OVER:
584 |             console.print("\n[bold]Game Over[/bold]")
585 |             restart = typer.confirm("Would you like to return to the main menu?")
586 |             if restart:
587 |                 game.state = GameState.MENU
588 |             else:
589 |                 game.running = False
590 | 
591 | if __name__ == "__main__":
592 |     main()
593 | ```
594 | 
595 | ## Example Gameplay
596 | 
597 | When you run the game, you'll experience:
598 | 
599 | **Character Creation:**
600 | ```
601 | 🏰 MYSTIC REALM ADVENTURE 🏰
602 | 
603 | Enter your character's name: Aria
604 | 
605 | Character Creation
606 | You have 10 extra skill points to distribute among your skills.
607 | Base skills start at 10 each.
608 | 
609 | Points remaining: 10
610 | Points to add to strength (0-10): 2
611 | Points to add to intelligence (0-8): 4
612 | Points to add to charisma (0-4): 3
613 | Points to add to stealth (0-1): 1
614 | 
615 | Welcome to Mystic Realm, Aria!
616 | ```
617 | 
618 | **Dynamic Scene Generation:**
619 | ```
620 | ┌──────────── Current Location ────────────┐
621 | │ Village Square                           │
622 | │                                          │
623 | │ You stand in the bustling heart of       │
624 | │ Willowbrook Village. The ancient stone   │
625 | │ fountain bubbles cheerfully as merchants │
626 | │ hawk their wares and children play. A    │
627 | │ mysterious hooded figure lurks near the  │
628 | │ shadows of the old oak tree.             │
629 | │                                          │
630 | │ NPCs present: Village Elder, Merchant    │
631 | │ Items visible: Strange Medallion, Herbs  │
632 | └──────────────────────────────────────────┘
633 | 
634 | ┌────────── Available Actions ─────────────┐
635 | │ 1. Approach the hooded figure            │
636 | │ 2. Talk to the Village Elder             │
637 | │ 3. Browse the merchant's wares           │
638 | │ 4. Examine the strange medallion         │
639 | │ 5. Gather herbs near the fountain        │
640 | │ 6. Head to the forest path               │
641 | └───────────────────────────────────────────┘
642 | ```
643 | 
644 | **AI-Generated Dialogue:**
645 | ```
646 | Talking to Village Elder...
647 | 
648 | Village Elder: "Ah, young traveler, I sense a great destiny 
649 | surrounds you like morning mist. The ancient prophecy speaks 
650 | of one who would come bearing the mark of courage. Tell me, 
651 | have you noticed anything... unusual in your travels?"
652 | 
653 | 💼 Quest opportunity detected!
654 | ℹ️ The Village Elder knows about an ancient prophecy that might involve you
655 | ```
656 | 
657 | ## Next Steps
658 | 
659 | - **Combat System**: Add turn-based battles with strategy
660 | - **Magic System**: Spellcasting with resource management
661 | - **Multiplayer**: Network support for cooperative adventures
662 | - **Quest System**: Complex multi-step missions with branching outcomes
663 | - **World Building**: Procedurally generated locations and characters
664 | - **Audio**: Add sound effects and background music
665 | 
666 | This tutorial demonstrates how DSPy's modular approach enables complex, interactive systems where AI handles creative content generation while maintaining consistent game logic and player agency.
```

tutorials/async/index.md
```
1 | # Async DSPy Programming
2 | 
3 | DSPy provides native support for asynchronous programming, allowing you to build more efficient and
4 | scalable applications. This guide will walk you through how to leverage async capabilities in DSPy,
5 | covering both built-in modules and custom implementations.
6 | 
7 | ## Why Use Async in DSPy?
8 | 
9 | Asynchronous programming in DSPy offers several benefits:
10 | - Improved performance through concurrent operations
11 | - Better resource utilization
12 | - Reduced waiting time for I/O-bound operations
13 | - Enhanced scalability for handling multiple requests
14 | 
15 | ## When Should I use Sync or Async?
16 | 
17 | Choosing between synchronous and asynchronous programming in DSPy depends on your specific use case.
18 | Here's a guide to help you make the right choice:
19 | 
20 | Use Synchronous Programming When
21 | 
22 | - You're exploring or prototyping new ideas
23 | - You're conducting research or experiments
24 | - You're building small to medium-sized applications
25 | - You need simpler, more straightforward code
26 | - You want easier debugging and error tracking
27 | 
28 | Use Asynchronous Programming When:
29 | 
30 | - You're building a high-throughput service (high QPS)
31 | - You're working with tools that only support async operations
32 | - You need to handle multiple concurrent requests efficiently
33 | - You're building a production service that requires high scalability
34 | 
35 | ### Important Considerations
36 | 
37 | While async programming offers performance benefits, it comes with some trade-offs:
38 | 
39 | - More complex error handling and debugging
40 | - Potential for subtle, hard-to-track bugs
41 | - More complex code structure
42 | - Different code between ipython (Colab, Jupyter lab, Databricks notebooks, ...) and normal python runtime.
43 | 
44 | We recommend starting with synchronous programming for most development scenarios and switching to async
45 | only when you have a clear need for its benefits. This approach allows you to focus on the core logic of
46 | your application before dealing with the additional complexity of async programming.
47 | 
48 | ## Using Built-in Modules Asynchronously
49 | 
50 | Most DSPy built-in modules support asynchronous operations through the `acall()` method. This method
51 | maintains the same interface as the synchronous `__call__` method but operates asynchronously.
52 | 
53 | Here's a basic example using `dspy.Predict`:
54 | 
55 | ```python
56 | import dspy
57 | import asyncio
58 | import os
59 | 
60 | os.environ["OPENAI_API_KEY"] = "your_api_key"
61 | 
62 | dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
63 | predict = dspy.Predict("question->answer")
64 | 
65 | async def main():
66 |     # Use acall() for async execution
67 |     output = await predict.acall(question="why did a chicken cross the kitchen?")
68 |     print(output)
69 | 
70 | 
71 | asyncio.run(main())
72 | ```
73 | 
74 | ### Working with Async Tools
75 | 
76 | DSPy's `Tool` class seamlessly integrates with async functions. When you provide an async
77 | function to `dspy.Tool`, you can execute it using `acall()`. This is particularly useful
78 | for I/O-bound operations or when working with external services.
79 | 
80 | ```python
81 | import asyncio
82 | import dspy
83 | import os
84 | 
85 | os.environ["OPENAI_API_KEY"] = "your_api_key"
86 | 
87 | async def foo(x):
88 |     # Simulate an async operation
89 |     await asyncio.sleep(0.1)
90 |     print(f"I get: {x}")
91 | 
92 | # Create a tool from the async function
93 | tool = dspy.Tool(foo)
94 | 
95 | async def main():
96 |     # Execute the tool asynchronously
97 |     await tool.acall(x=2)
98 | 
99 | asyncio.run(main())
100 | ```
101 | 
102 | Note: When using `dspy.ReAct` with tools, calling `acall()` on the ReAct instance will automatically
103 | execute all tools asynchronously using their `acall()` methods.
104 | 
105 | ## Creating Custom Async DSPy Modules
106 | 
107 | To create your own async DSPy module, implement the `aforward()` method instead of `forward()`. This method
108 | should contain your module's async logic. Here's an example of a custom module that chains two async operations:
109 | 
110 | ```python
111 | import dspy
112 | import asyncio
113 | import os
114 | 
115 | os.environ["OPENAI_API_KEY"] = "your_api_key"
116 | dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
117 | 
118 | class MyModule(dspy.Module):
119 |     def __init__(self):
120 |         self.predict1 = dspy.ChainOfThought("question->answer")
121 |         self.predict2 = dspy.ChainOfThought("answer->simplified_answer")
122 | 
123 |     async def aforward(self, question, **kwargs):
124 |         # Execute predictions sequentially but asynchronously
125 |         answer = await self.predict1.acall(question=question)
126 |         return await self.predict2.acall(answer=answer)
127 | 
128 | 
129 | async def main():
130 |     mod = MyModule()
131 |     result = await mod.acall(question="Why did a chicken cross the kitchen?")
132 |     print(result)
133 | 
134 | 
135 | asyncio.run(main())
136 | ```
```

tutorials/audio/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Using Audio in DSPy Programs\n",
8 |     "\n",
9 |     "This tutorial walks through building pipelines for audio-based applications using DSPy."
10 |    ]
11 |   },
12 |   {
13 |    "cell_type": "markdown",
14 |    "metadata": {},
15 |    "source": [
16 |     "### Install Dependencies\n",
17 |     "\n",
18 |     "Ensure you're using the latest DSPy version:\n",
19 |     "\n",
20 |     "```shell\n",
21 |     "pip install -U dspy\n",
22 |     "```\n",
23 |     "\n",
24 |     "To handle audio data, install the following dependencies:\n",
25 |     "\n",
26 |     "```shell\n",
27 |     "pip install datasets soundfile torch==2.0.1+cu118 torchaudio==2.0.2+cu118\n",
28 |     "```\n"
29 |    ]
30 |   },
31 |   {
32 |    "cell_type": "markdown",
33 |    "metadata": {},
34 |    "source": [
35 |     "### Load the Spoken-SQuAD Dataset\n",
36 |     "\n",
37 |     "We'll use the Spoken-SQuAD dataset ([Official](https://github.com/Chia-Hsuan-Lee/Spoken-SQuAD) & [HuggingFace version](https://huggingface.co/datasets/AudioLLMs/spoken_squad_test) for tutorial demonstration), which contains spoken audio passages used for question-answering:"
38 |    ]
39 |   },
40 |   {
41 |    "cell_type": "code",
42 |    "execution_count": null,
43 |    "metadata": {},
44 |    "outputs": [],
45 |    "source": [
46 |     "import random\n",
47 |     "import dspy\n",
48 |     "from dspy.datasets import DataLoader\n",
49 |     "\n",
50 |     "kwargs = dict(fields=(\"context\", \"instruction\", \"answer\"), input_keys=(\"context\", \"instruction\"))\n",
51 |     "spoken_squad = DataLoader().from_huggingface(dataset_name=\"AudioLLMs/spoken_squad_test\", split=\"train\", trust_remote_code=True, **kwargs)\n",
52 |     "\n",
53 |     "random.Random(42).shuffle(spoken_squad)\n",
54 |     "spoken_squad = spoken_squad[:100]\n",
55 |     "\n",
56 |     "split_idx = len(spoken_squad) // 2\n",
57 |     "trainset_raw, testset_raw = spoken_squad[:split_idx], spoken_squad[split_idx:]"
58 |    ]
59 |   },
60 |   {
61 |    "cell_type": "markdown",
62 |    "metadata": {},
63 |    "source": [
64 |     "### Preprocess Audio Data\n",
65 |     "\n",
66 |     "The audio clips in the dataset require some preprocessing into byte arrays with their corresponding sampling rates."
67 |    ]
68 |   },
69 |   {
70 |    "cell_type": "code",
71 |    "execution_count": null,
72 |    "metadata": {},
73 |    "outputs": [],
74 |    "source": [
75 |     "def preprocess(x):\n",
76 |     "    audio = dspy.Audio.from_array(x.context[\"array\"], x.context[\"sampling_rate\"])\n",
77 |     "    return dspy.Example(\n",
78 |     "        passage_audio=audio,\n",
79 |     "        question=x.instruction,\n",
80 |     "        answer=x.answer\n",
81 |     "    ).with_inputs(\"passage_audio\", \"question\")\n",
82 |     "\n",
83 |     "trainset = [preprocess(x) for x in trainset_raw]\n",
84 |     "testset = [preprocess(x) for x in testset_raw]\n",
85 |     "\n",
86 |     "len(trainset), len(testset)"
87 |    ]
88 |   },
89 |   {
90 |    "cell_type": "markdown",
91 |    "metadata": {},
92 |    "source": [
93 |     "## DSPy program for spoken question answering\n",
94 |     "\n",
95 |     "Let's define a simple DSPy program that uses audio inputs to answer questions directly. This is very similar to the [BasicQA](https://dspy.ai/cheatsheet/?h=basicqa#dspysignature) task, with the only difference being that the passage context is provided as an audio file for the model to listen to and answer the question:"
96 |    ]
97 |   },
98 |   {
99 |    "cell_type": "code",
100 |    "execution_count": null,
101 |    "metadata": {},
102 |    "outputs": [],
103 |    "source": [
104 |     "class SpokenQASignature(dspy.Signature):\n",
105 |     "    \"\"\"Answer the question based on the audio clip.\"\"\"\n",
106 |     "    passage_audio: dspy.Audio = dspy.InputField()\n",
107 |     "    question: str = dspy.InputField()\n",
108 |     "    answer: str = dspy.OutputField(desc = 'factoid answer between 1 and 5 words')\n",
109 |     "\n",
110 |     "spoken_qa = dspy.ChainOfThought(SpokenQASignature)\n"
111 |    ]
112 |   },
113 |   {
114 |    "cell_type": "markdown",
115 |    "metadata": {},
116 |    "source": [
117 |     "Now let's configure our LLM which can process input audio. \n",
118 |     "\n",
119 |     "```python\n",
120 |     "dspy.settings.configure(lm=dspy.LM(model='gpt-4o-mini-audio-preview-2024-12-17'))\n",
121 |     "```\n",
122 |     "\n",
123 |     "Note: Using `dspy.Audio` in signatures allows passing in audio directly to the model. "
124 |    ]
125 |   },
126 |   {
127 |    "cell_type": "markdown",
128 |    "metadata": {},
129 |    "source": [
130 |     "### Define Evaluation Metric\n",
131 |     "\n",
132 |     "We'll use the Exact Match metric (`dspy.evaluate.answer_exact_match`) to measure answer accuracy compared to the provided reference answers:"
133 |    ]
134 |   },
135 |   {
136 |    "cell_type": "code",
137 |    "execution_count": null,
138 |    "metadata": {},
139 |    "outputs": [],
140 |    "source": [
141 |     "evaluate_program = dspy.Evaluate(devset=testset, metric=dspy.evaluate.answer_exact_match,display_progress=True, num_threads = 10, display_table=True)\n",
142 |     "\n",
143 |     "evaluate_program(spoken_qa)"
144 |    ]
145 |   },
146 |   {
147 |    "cell_type": "markdown",
148 |    "metadata": {},
149 |    "source": [
150 |     "### Optimize with DSPy\n",
151 |     "\n",
152 |     "You can optimize this audio-based program as you would for any DSPy program using any DSPy optimizer.\n",
153 |     "\n",
154 |     "Note: Audio tokens can be costly so it is recommended to configure optimizers like `dspy.BootstrapFewShotWithRandomSearch` or `dspy.MIPROv2` conservatively with 0-2 few shot examples and less candidates / trials than the optimizer default parameters."
155 |    ]
156 |   },
157 |   {
158 |    "cell_type": "code",
159 |    "execution_count": null,
160 |    "metadata": {},
161 |    "outputs": [],
162 |    "source": [
163 |     "optimizer = dspy.BootstrapFewShotWithRandomSearch(metric = dspy.evaluate.answer_exact_match, max_bootstrapped_demos=2, max_labeled_demos=2, num_candidate_programs=5)\n",
164 |     "\n",
165 |     "optimized_program = optimizer.compile(spoken_qa, trainset = trainset)\n",
166 |     "\n",
167 |     "evaluate_program(optimized_program)"
168 |    ]
169 |   },
170 |   {
171 |    "cell_type": "code",
172 |    "execution_count": null,
173 |    "metadata": {},
174 |    "outputs": [],
175 |    "source": [
176 |     "prompt_lm = dspy.LM(model='gpt-4o-mini') #NOTE - this is the LLM guiding the MIPROv2 instruction candidate proposal\n",
177 |     "optimizer = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto=\"light\", prompt_model = prompt_lm)\n",
178 |     "\n",
179 |     "#NOTE - MIPROv2's dataset summarizer cannot process the audio files in the dataset, so we turn off the data_aware_proposer \n",
180 |     "optimized_program = optimizer.compile(spoken_qa, trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2, data_aware_proposer=False)\n",
181 |     "\n",
182 |     "evaluate_program(optimized_program)"
183 |    ]
184 |   },
185 |   {
186 |    "cell_type": "markdown",
187 |    "metadata": {},
188 |    "source": [
189 |     "With this small subset, MIPROv2 led to a ~10% improvement over baseline performance."
190 |    ]
191 |   },
192 |   {
193 |    "cell_type": "markdown",
194 |    "metadata": {},
195 |    "source": [
196 |     "---"
197 |    ]
198 |   },
199 |   {
200 |    "cell_type": "markdown",
201 |    "metadata": {},
202 |    "source": [
203 |     "Now that we’ve seen how to use an audio-input-capable LLM in DSPy, let’s flip the setup.\n",
204 |     "\n",
205 |     "In this next task, we'll use a standard text-based LLM to generate prompts for a text-to-speech model and then evaluate the quality of the produced speech for some downstream task. This approach is generally more cost-effective than asking an LLM like `gpt-4o-mini-audio-preview-2024-12-17` to generate audio directly, while still enabling a pipeline that can be optimized for higher-quality speech output."
206 |    ]
207 |   },
208 |   {
209 |    "cell_type": "markdown",
210 |    "metadata": {},
211 |    "source": [
212 |     "### Load the CREMA-D Dataset\n",
213 |     "\n",
214 |     "We'll use the CREMA-D dataset ([Official](https://github.com/CheyneyComputerScience/CREMA-D) & [HuggingFace version](https://huggingface.co/datasets/myleslinder/crema-d) for tutorial demonstration), which includes audio clips of chosen participants speaking the same line with one of six target emotions: neutral, happy, sad, anger, fear, and disgust."
215 |    ]
216 |   },
217 |   {
218 |    "cell_type": "code",
219 |    "execution_count": null,
220 |    "metadata": {},
221 |    "outputs": [],
222 |    "source": [
223 |     "from collections import defaultdict\n",
224 |     "\n",
225 |     "label_map = ['neutral', 'happy', 'sad', 'anger', 'fear', 'disgust']\n",
226 |     "\n",
227 |     "kwargs = dict(fields=(\"sentence\", \"label\", \"audio\"), input_keys=(\"sentence\", \"label\"))\n",
228 |     "crema_d = DataLoader().from_huggingface(dataset_name=\"myleslinder/crema-d\", split=\"train\", trust_remote_code=True, **kwargs)\n",
229 |     "\n",
230 |     "def preprocess(x):\n",
231 |     "    return dspy.Example(\n",
232 |     "        raw_line=x.sentence,\n",
233 |     "        target_style=label_map[x.label],\n",
234 |     "        reference_audio=dspy.Audio.from_array(x.audio[\"array\"], x.audio[\"sampling_rate\"])\n",
235 |     "    ).with_inputs(\"raw_line\", \"target_style\")\n",
236 |     "\n",
237 |     "random.Random(42).shuffle(crema_d)\n",
238 |     "crema_d = crema_d[:100]\n",
239 |     "\n",
240 |     "random.seed(42)\n",
241 |     "label_to_indices = defaultdict(list)\n",
242 |     "for idx, x in enumerate(crema_d):\n",
243 |     "    label_to_indices[x.label].append(idx)\n",
244 |     "\n",
245 |     "per_label = 100 // len(label_map)\n",
246 |     "train_indices, test_indices = [], []\n",
247 |     "for indices in label_to_indices.values():\n",
248 |     "    selected = random.sample(indices, min(per_label, len(indices)))\n",
249 |     "    split = len(selected) // 2\n",
250 |     "    train_indices.extend(selected[:split])\n",
251 |     "    test_indices.extend(selected[split:])\n",
252 |     "\n",
253 |     "trainset = [preprocess(crema_d[idx]) for idx in train_indices]\n",
254 |     "testset = [preprocess(crema_d[idx]) for idx in test_indices]"
255 |    ]
256 |   },
257 |   {
258 |    "cell_type": "markdown",
259 |    "metadata": {},
260 |    "source": [
261 |     "## DSPy pipeline for generating TTS instructions for speaking with a target emotion\n",
262 |     "\n",
263 |     "We’ll now build a pipeline that generates emotionally expressive speech by prompting a TTS model with both a line of text and an instruction on how to say it. \n",
264 |     "The goal of this task will be to use DSPy to generate prompts that guide the TTS output to match the emotion and style of reference audio from the dataset.\n",
265 |     "\n",
266 |     "First let’s set up the TTS generator to produce generate spoken audio with a specified emotion or style. \n",
267 |     "We utilize `gpt-4o-mini-tts` as it supports prompting the model with raw input and speaking and produces an audio response as a `.wav` file processed with `dspy.Audio`. \n",
268 |     "We also set up a cache for the TTS outputs."
269 |    ]
270 |   },
271 |   {
272 |    "cell_type": "code",
273 |    "execution_count": null,
274 |    "metadata": {},
275 |    "outputs": [],
276 |    "source": [
277 |     "import os\n",
278 |     "import base64\n",
279 |     "import hashlib\n",
280 |     "from openai import OpenAI\n",
281 |     "\n",
282 |     "CACHE_DIR = \".audio_cache\"\n",
283 |     "os.makedirs(CACHE_DIR, exist_ok=True)\n",
284 |     "\n",
285 |     "def hash_key(raw_line: str, prompt: str) -> str:\n",
286 |     "    return hashlib.sha256(f\"{raw_line}|||{prompt}\".encode(\"utf-8\")).hexdigest()\n",
287 |     "\n",
288 |     "def generate_dspy_audio(raw_line: str, prompt: str) -> dspy.Audio:\n",
289 |     "    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
290 |     "    key = hash_key(raw_line, prompt)\n",
291 |     "    wav_path = os.path.join(CACHE_DIR, f\"{key}.wav\")\n",
292 |     "    if not os.path.exists(wav_path):\n",
293 |     "        response = client.audio.speech.create(\n",
294 |     "            model=\"gpt-4o-mini-tts\",\n",
295 |     "            voice=\"coral\", #NOTE - this can be configured to any of the 11 offered OpenAI TTS voices - https://platform.openai.com/docs/guides/text-to-speech#voice-options. \n",
296 |     "            input=raw_line,\n",
297 |     "            instructions=prompt,\n",
298 |     "            response_format=\"wav\"\n",
299 |     "        )\n",
300 |     "        with open(wav_path, \"wb\") as f:\n",
301 |     "            f.write(response.content)\n",
302 |     "    with open(wav_path, \"rb\") as f:\n",
303 |     "        encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n",
304 |     "    return dspy.Audio(data=encoded, format=\"wav\")"
305 |    ]
306 |   },
307 |   {
308 |    "cell_type": "markdown",
309 |    "metadata": {},
310 |    "source": [
311 |     "Now let's define the DSPy program for generating TTS instructions. For this program, we can use standard text-based LLMs again since we're just generating instructions."
312 |    ]
313 |   },
314 |   {
315 |    "cell_type": "code",
316 |    "execution_count": null,
317 |    "metadata": {},
318 |    "outputs": [],
319 |    "source": [
320 |     "class EmotionStylePromptSignature(dspy.Signature):\n",
321 |     "    \"\"\"Generate an OpenAI TTS instruction that makes the TTS model speak the given line with the target emotion or style.\"\"\"\n",
322 |     "    raw_line: str = dspy.InputField()\n",
323 |     "    target_style: str = dspy.InputField()\n",
324 |     "    openai_instruction: str = dspy.OutputField()\n",
325 |     "\n",
326 |     "class EmotionStylePrompter(dspy.Module):\n",
327 |     "    def __init__(self):\n",
328 |     "        self.prompter = dspy.ChainOfThought(EmotionStylePromptSignature)\n",
329 |     "\n",
330 |     "    def forward(self, raw_line, target_style):\n",
331 |     "        out = self.prompter(raw_line=raw_line, target_style=target_style)\n",
332 |     "        audio = generate_dspy_audio(raw_line, out.openai_instruction)\n",
333 |     "        return dspy.Prediction(audio=audio)\n",
334 |     "    \n",
335 |     "dspy.settings.configure(lm=dspy.LM(model='gpt-4o-mini'))"
336 |    ]
337 |   },
338 |   {
339 |    "cell_type": "markdown",
340 |    "metadata": {},
341 |    "source": [
342 |     "### Define Evaluation Metric\n",
343 |     "\n",
344 |     "Audio reference comparisons is generally a non-trivial task due to subjective variations of evaluating speech, especially with emotional expression. For the purposes of this tutorial, we use an embedding-based similarity metric for objective evaluation, leveraging Wav2Vec 2.0 to convert audio into embeddings and computing cosine similarity between the reference and generated audio. To evaluate audio quality more accurately, human feedback or perceptual metrics would be more suitable. "
345 |    ]
346 |   },
347 |   {
348 |    "cell_type": "code",
349 |    "execution_count": null,
350 |    "metadata": {},
351 |    "outputs": [],
352 |    "source": [
353 |     "import torch\n",
354 |     "import torchaudio\n",
355 |     "import soundfile as sf\n",
356 |     "import io\n",
357 |     "\n",
358 |     "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
359 |     "model = bundle.get_model().eval()\n",
360 |     "\n",
361 |     "def decode_dspy_audio(dspy_audio):\n",
362 |     "    audio_bytes = base64.b64decode(dspy_audio.data)\n",
363 |     "    array, _ = sf.read(io.BytesIO(audio_bytes), dtype=\"float32\")\n",
364 |     "    return torch.tensor(array).unsqueeze(0)\n",
365 |     "\n",
366 |     "def extract_embedding(audio_tensor):\n",
367 |     "    with torch.inference_mode():\n",
368 |     "        return model(audio_tensor)[0].mean(dim=1)\n",
369 |     "\n",
370 |     "def cosine_similarity(a, b):\n",
371 |     "    return torch.nn.functional.cosine_similarity(a, b).item()\n",
372 |     "\n",
373 |     "def audio_similarity_metric(example, pred, trace=None):\n",
374 |     "    ref_audio = decode_dspy_audio(example.reference_audio)\n",
375 |     "    gen_audio = decode_dspy_audio(pred.audio)\n",
376 |     "\n",
377 |     "    ref_embed = extract_embedding(ref_audio)\n",
378 |     "    gen_embed = extract_embedding(gen_audio)\n",
379 |     "\n",
380 |     "    score = cosine_similarity(ref_embed, gen_embed)\n",
381 |     "\n",
382 |     "    if trace is not None:\n",
383 |     "        return score > 0.8 \n",
384 |     "    return score\n",
385 |     "\n",
386 |     "evaluate_program = dspy.Evaluate(devset=testset, metric=audio_similarity_metric, display_progress=True, num_threads = 10, display_table=True)\n",
387 |     "\n",
388 |     "evaluate_program(EmotionStylePrompter())"
389 |    ]
390 |   },
391 |   {
392 |    "cell_type": "markdown",
393 |    "metadata": {},
394 |    "source": [
395 |     "We can look at an example to see what instructions the DSPy program generated and the corresponding score:"
396 |    ]
397 |   },
398 |   {
399 |    "cell_type": "code",
400 |    "execution_count": 24,
401 |    "metadata": {},
402 |    "outputs": [
403 |     {
404 |      "name": "stdout",
405 |      "output_type": "stream",
406 |      "text": [
407 |       "\n",
408 |       "\n",
409 |       "\n",
410 |       "\n",
411 |       "\u001b[34m[2025-05-15T22:01:22.667596]\u001b[0m\n",
412 |       "\n",
413 |       "\u001b[31mSystem message:\u001b[0m\n",
414 |       "\n",
415 |       "Your input fields are:\n",
416 |       "1. `raw_line` (str)\n",
417 |       "2. `target_style` (str)\n",
418 |       "Your output fields are:\n",
419 |       "1. `reasoning` (str)\n",
420 |       "2. `openai_instruction` (str)\n",
421 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
422 |       "\n",
423 |       "[[ ## raw_line ## ]]\n",
424 |       "{raw_line}\n",
425 |       "\n",
426 |       "[[ ## target_style ## ]]\n",
427 |       "{target_style}\n",
428 |       "\n",
429 |       "[[ ## reasoning ## ]]\n",
430 |       "{reasoning}\n",
431 |       "\n",
432 |       "[[ ## openai_instruction ## ]]\n",
433 |       "{openai_instruction}\n",
434 |       "\n",
435 |       "[[ ## completed ## ]]\n",
436 |       "In adhering to this structure, your objective is: \n",
437 |       "        Generate an OpenAI TTS instruction that makes the TTS model speak the given line with the target emotion or style.\n",
438 |       "\n",
439 |       "\n",
440 |       "\u001b[31mUser message:\u001b[0m\n",
441 |       "\n",
442 |       "[[ ## raw_line ## ]]\n",
443 |       "It's eleven o'clock\n",
444 |       "\n",
445 |       "[[ ## target_style ## ]]\n",
446 |       "disgust\n",
447 |       "\n",
448 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## openai_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
449 |       "\n",
450 |       "\n",
451 |       "\u001b[31mResponse:\u001b[0m\n",
452 |       "\n",
453 |       "\u001b[32m[[ ## reasoning ## ]]\n",
454 |       "To generate the OpenAI TTS instruction, we need to specify the target emotion or style, which in this case is 'disgust'. We will use the OpenAI TTS instruction format, which includes the text to be spoken and the desired emotion or style.\n",
455 |       "\n",
456 |       "[[ ## openai_instruction ## ]]\n",
457 |       "\"Speak the following line with a tone of disgust: It's eleven o'clock\"\n",
458 |       "\n",
459 |       "[[ ## completed ## ]]\u001b[0m\n",
460 |       "\n",
461 |       "\n",
462 |       "\n",
463 |       "\n",
464 |       "\n"
465 |      ]
466 |     }
467 |    ],
468 |    "source": [
469 |     "program = EmotionStylePrompter()\n",
470 |     "\n",
471 |     "pred = program(raw_line=testset[1].raw_line, target_style=testset[1].target_style)\n",
472 |     "\n",
473 |     "print(audio_similarity_metric(testset[1], pred)) #0.5725605487823486\n",
474 |     "\n",
475 |     "dspy.inspect_history(n=1)"
476 |    ]
477 |   },
478 |   {
479 |    "cell_type": "markdown",
480 |    "metadata": {},
481 |    "source": [
482 |     "TTS Instruction: \n",
483 |     "```text\n",
484 |     "Speak the following line with a tone of disgust: It's eleven o'clock\n",
485 |     "```\n"
486 |    ]
487 |   },
488 |   {
489 |    "cell_type": "code",
490 |    "execution_count": null,
491 |    "metadata": {},
492 |    "outputs": [
493 |     {
494 |      "data": {
495 |       "text/html": [
496 |        "\n",
497 |        "                <audio  controls=\"controls\" >\n",
[TRUNCATED]
```

tutorials/build_ai_program/index.md
```
1 | # Build AI Programs with DSPy
2 | 
3 | This section contains hands-on tutorials that guide you through building production-ready AI applications using DSPy. Each tutorial demonstrates practical use cases and shows you how to leverage DSPy's modular programming approach to create robust, maintainable AI systems.
4 | 
5 | ## Core Applications
6 | 
7 | ### [Managing Conversation History](../conversation_history/index.md)
8 | Learn how to manage conversation history in DSPy applications.
9 | 
10 | ### [Building AI Agents with DSPy](../customer_service_agent/index.ipynb)
11 | Learn to create intelligent agents that can handle complex customer service scenarios. This tutorial shows how to build agents that can understand context, maintain conversation state, and provide helpful responses.
12 | 
13 | ### [Building AI Applications by Customizing DSPy Modules](../custom_module/index.ipynb)
14 | Discover how to create custom DSPy modules tailored to your specific needs. Learn the patterns for building reusable, composable components that can be shared across different applications.
15 | 
16 | ## Retrieval-Augmented Generation (RAG)
17 | 
18 | ### [Retrieval-Augmented Generation (RAG)](../rag/index.ipynb)
19 | Master the fundamentals of RAG systems with DSPy. Learn how to combine retrieval mechanisms with language models to build systems that can answer questions using external knowledge sources.
20 | 
21 | ### [Building RAG as Agent](../agents/index.ipynb)
22 | Take RAG to the next level by building `ReAct` agent-based systems that can reason about when and how to retrieve information, making your RAG systems more intelligent and adaptive.
23 | 
24 | ### [Multi-Hop RAG](../multihop_search/index.ipynb)
25 | Build sophisticated RAG systems that can perform multi-step reasoning across multiple information sources, perfect for complex research and analysis tasks.
26 | 
27 | ## Specialized Use Cases
28 | 
29 | ### [Entity Extraction](../entity_extraction/index.ipynb)
30 | Learn to build systems that can identify and extract specific entities from text, essential for information processing and data analysis applications.
31 | 
32 | ### [Classification](../classification/index.md)
33 | Build robust text classification systems using DSPy's modular approach with a topic classification example.
34 | 
35 | ### [Privacy-Conscious Delegation](../papillon/index.md)
36 | Explore advanced techniques for building AI systems that respect privacy constraints while maintaining high performance by combining a small local model and an advanced external model.
37 | 
38 | ## Advanced Reasoning
39 | 
40 | ### [Program Of Thought](../program_of_thought/index.ipynb)
41 | Learn to build systems that can generate and execute code to solve complex problems, combining the power of language models with programmatic reasoning.
42 | 
43 | ## Multimodal Applications
44 | 
45 | ### [Image Generation Prompt iteration](../image_generation_prompting/index.ipynb)
46 | Discover how to use DSPy to iteratively improve image generation prompts, creating better visual content through systematic optimization.
47 | 
48 | ### [Audio](../audio/index.ipynb)
49 | Explore audio processing applications with DSPy, learning to build systems that can understand, process, and generate audio content.
```

tutorials/cache/index.md
```
1 | # Use and Customize DSPy Cache
2 | 
3 | In this tutorial, we will explore the design of DSPy's caching mechanism and demonstrate how to effectively use and customize it.
4 | 
5 | ## DSPy Cache Structure
6 | 
7 | DSPy's caching system is architected in three distinct layers:
8 | 
9 | 1.  **In-memory cache**: Implemented using `cachetools.LRUCache`, this layer provides fast access to frequently used data.
10 | 2.  **On-disk cache**: Leveraging `diskcache.FanoutCache`, this layer offers persistent storage for cached items.
11 | 3.  **Prompt cache (Server-side cache)**: This layer is managed by the LLM service provider (e.g., OpenAI, Anthropic).
12 | 
13 | While DSPy does not directly control the server-side prompt cache, it offers users the flexibility to enable, disable, and customize the in-memory and on-disk caches to suit their specific requirements.
14 | 
15 | ## Using DSPy Cache
16 | 
17 | By default, both in-memory and on-disk caching are automatically enabled in DSPy. No specific action is required to start using the cache. When a cache hit occurs, you will observe a significant reduction in the module call's execution time. Furthermore, if usage tracking is enabled, the usage metrics for a cached call will be `None`.
18 | 
19 | Consider the following example:
20 | 
21 | ```python
22 | import dspy
23 | import os
24 | import time
25 | 
26 | os.environ["OPENAI_API_KEY"] = "{your_openai_key}"
27 | 
28 | dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"), track_usage=True)
29 | 
30 | predict = dspy.Predict("question->answer")
31 | 
32 | start = time.time()
33 | result1 = predict(question="Who is the GOAT of basketball?")
34 | print(f"Time elapse: {time.time() - start: 2f}\n\nTotal usage: {result1.get_lm_usage()}")
35 | 
36 | start = time.time()
37 | result2 = predict(question="Who is the GOAT of basketball?")
38 | print(f"Time elapse: {time.time() - start: 2f}\n\nTotal usage: {result2.get_lm_usage()}")
39 | ```
40 | 
41 | A sample output looks like:
42 | 
43 | ```
44 | Time elapse:  4.384113
45 | Total usage: {'openai/gpt-4o-mini': {'completion_tokens': 97, 'prompt_tokens': 144, 'total_tokens': 241, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'text_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'text_tokens': None, 'image_tokens': None}}}
46 | 
47 | Time elapse:  0.000529
48 | Total usage: {}
49 | ```
50 | 
51 | ## Disabling/Enabling DSPy Cache
52 | 
53 | There are scenarios where you might need to disable caching, either entirely or selectively for in-memory or on-disk caches. For instance:
54 | 
55 | - You require different responses for identical LM requests.
56 | - You lack disk write permissions and need to disable the on-disk cache.
57 | - You have limited memory resources and wish to disable the in-memory cache.
58 | 
59 | DSPy provides the `dspy.configure_cache()` utility function for this purpose. You can use the corresponding flags to control the enabled/disabled state of each cache type:
60 | 
61 | ```python
62 | dspy.configure_cache(
63 |     enable_disk_cache=False,
64 |     enable_memory_cache=False,
65 | )
66 | ```
67 | 
68 | In additions, you can manage the capacity of the in-memory and on-disk caches:
69 | 
70 | ```python
71 | dspy.configure_cache(
72 |     enable_disk_cache=True,
73 |     enable_memory_cache=True,
74 |     disk_size_limit_bytes=YOUR_DESIRED_VALUE,
75 |     memory_max_entries=YOUR_DESIRED_VALUE,
76 | )
77 | ```
78 | 
79 | Please note that `disk_size_limit_bytes` defines the maximum size in bytes for the on-disk cache, while `memory_max_entries` specifies the maximum number of entries for the in-memory cache.
80 | 
81 | ## Understanding and Customizing the Cache
82 | 
83 | In specific situations, you might want to implement a custom cache, for example, to gain finer control over how cache keys are generated. By default, the cache key is derived from a hash of all request arguments sent to `litellm`, excluding credentials like `api_key`.
84 | 
85 | To create a custom cache, you need to subclass `dspy.clients.Cache` and override the relevant methods:
86 | 
87 | ```python
88 | class CustomCache(dspy.clients.Cache):
89 |     def __init__(self, **kwargs):
90 |         {write your own constructor}
91 | 
92 |     def cache_key(self, request: dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> str:
93 |         {write your logic of computing cache key}
94 | 
95 |     def get(self, request: dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> Any:
96 |         {write your cache read logic}
97 | 
98 |     def put(
99 |         self,
100 |         request: dict[str, Any],
101 |         value: Any,
102 |         ignored_args_for_cache_key: Optional[list[str]] = None,
103 |         enable_memory_cache: bool = True,
104 |     ) -> None:
105 |         {write your cache write logic}
106 | ```
107 | 
108 | To ensure seamless integration with the rest of DSPy, it is recommended to implement your custom cache using the same method signatures as the base class, or at a minimum, include `**kwargs` in your method definitions to prevent runtime errors during cache read/write operations.
109 | 
110 | Once your custom cache class is defined, you can instruct DSPy to use it:
111 | 
112 | ```python
113 | dspy.cache = CustomCache()
114 | ```
115 | 
116 | Let's illustrate this with a practical example. Suppose we want the cache key computation to depend solely on the request message content, ignoring other parameters like the specific LM being called. We can create a custom cache as follows:
117 | 
118 | ```python
119 | class CustomCache(dspy.clients.Cache):
120 | 
121 |     def cache_key(self, request: dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> str:
122 |         messages = request.get("messages", [])
123 |         return sha256(ujson.dumps(messages, sort_keys=True).encode()).hexdigest()
124 | 
125 | dspy.cache = CustomCache(enable_disk_cache=True, enable_memory_cache=True, disk_cache_dir=dspy.clients.DISK_CACHE_DIR)
126 | ```
127 | 
128 | For comparison, consider executing the code below without the custom cache:
129 | 
130 | ```python
131 | import dspy
132 | import os
133 | import time
134 | 
135 | os.environ["OPENAI_API_KEY"] = "{your_openai_key}"
136 | 
137 | dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))
138 | 
139 | predict = dspy.Predict("question->answer")
140 | 
141 | start = time.time()
142 | result1 = predict(question="Who is the GOAT of soccer?")
143 | print(f"Time elapse: {time.time() - start: 2f}")
144 | 
145 | start = time.time()
146 | with dspy.context(lm=dspy.LM("openai/gpt-4.1-mini")):
147 |     result2 = predict(question="Who is the GOAT of soccer?")
148 | print(f"Time elapse: {time.time() - start: 2f}")
149 | ```
150 | 
151 | The time elapsed will indicate that the cache is not hit on the second call. However, when using the custom cache:
152 | 
153 | ```python
154 | import dspy
155 | import os
156 | import time
157 | from typing import Dict, Any, Optional
158 | import ujson
159 | from hashlib import sha256
160 | 
161 | os.environ["OPENAI_API_KEY"] = "{your_openai_key}"
162 | 
163 | dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))
164 | 
165 | class CustomCache(dspy.clients.Cache):
166 | 
167 |     def cache_key(self, request: dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> str:
168 |         messages = request.get("messages", [])
169 |         return sha256(ujson.dumps(messages, sort_keys=True).encode()).hexdigest()
170 | 
171 | dspy.cache = CustomCache(enable_disk_cache=True, enable_memory_cache=True, disk_cache_dir=dspy.clients.DISK_CACHE_DIR)
172 | 
173 | predict = dspy.Predict("question->answer")
174 | 
175 | start = time.time()
176 | result1 = predict(question="Who is the GOAT of volleyball?")
177 | print(f"Time elapse: {time.time() - start: 2f}")
178 | 
179 | start = time.time()
180 | with dspy.context(lm=dspy.LM("openai/gpt-4.1-mini")):
181 |     result2 = predict(question="Who is the GOAT of volleyball?")
182 | print(f"Time elapse: {time.time() - start: 2f}")
183 | ```
184 | 
185 | You will observe that the cache is hit on the second call, demonstrating the effect of the custom cache key logic.
```

tutorials/classification/index.md
```
1 | Please refer to [this tutorial from Drew Breunig](https://www.dbreunig.com/2024/12/12/pipelines-prompt-optimization-with-dspy.html) using DSPy.
2 | 
3 | This tutorial demonstrates a few aspects of using DSPy in a highly-accessible, concrete context for categorizing historic events with a tiny LM.
```

tutorials/classification_finetuning/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Classification Fine-tuning\n",
8 |     "\n",
9 |     "Let's walk through a quick example of fine-tuning the LM weights within a DSPy program. We'll apply to a simple 77-way classification task.\n",
10 |     "\n",
11 |     "Our finetuned program will use a tiny `Llama-3.2-1B` language model, hosted locally on your GPU. To make this more interesting, we'll assume that (i) we don't have **any training labels** but (ii) we have 500 unlabeled training examples.\n",
12 |     "\n",
13 |     "### Install dependencies and download data\n",
14 |     "\n",
15 |     "Install the latest DSPy via `pip install -U dspy` and follow along (or `uv pip`, if you prefer). This tutorial depends on DSPy >= 2.6.0. You also need to run `pip install datasets`.\n",
16 |     "\n",
17 |     "This tutorial requires a local GPU at the moment for inference, though we plan to support ollama serving for finetuned models as well.\n",
18 |     "\n",
19 |     "You will also need the following dependencies:\n",
20 |     "1. Inference: We use SGLang to run local inference servers. You can install the latest version by following the instructions here: https://docs.sglang.ai/start/install.html\n",
21 |     "Shared below is the most recent install command as of 04/02/2025, but we recommend that you follow the instructions in the most up to date version by navigating to the installation link.\n",
22 |     "This ensures that the fine-tuning packages and the `sglang` package are in sync.\n",
23 |     "    ```shell\n",
24 |     "    > pip install --upgrade pip\n",
25 |     "    > pip install uv\n",
26 |     "    > uv pip install \"sglang[all]>=0.4.4.post3\" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python\n",
27 |     "    ```\n",
28 |     "1. Fine-tuning: We use the following packages. Note that we specify the version for the transformers package as a temporary fix to a recent issue: https://github.com/huggingface/trl/issues/2338\n",
29 |     "    ```shell\n",
30 |     "    > uv pip install -U torch transformers==4.48.3 accelerate trl peft\n",
31 |     "    ```\n",
32 |     "\n",
33 |     "We recommend using `uv` package manager to speed up the installation."
34 |    ]
35 |   },
36 |   {
37 |    "cell_type": "markdown",
38 |    "metadata": {},
39 |    "source": [
40 |     "<details>\n",
41 |     "<summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>\n",
42 |     "\n",
43 |     "### MLflow DSPy Integration\n",
44 |     "\n",
45 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
46 |     "\n",
47 |     "![MLflow Trace](./mlflow-tracing-classification.png)\n",
48 |     "\n",
49 |     "1. Install MLflow\n",
50 |     "\n",
51 |     "```bash\n",
52 |     "%pip install mlflow>=2.20\n",
53 |     "```\n",
54 |     "\n",
55 |     "2. Start MLflow UI in a separate terminal\n",
56 |     "```bash\n",
57 |     "mlflow ui --port 5000\n",
58 |     "```\n",
59 |     "\n",
60 |     "3. Connect the notebook to MLflow\n",
61 |     "```python\n",
62 |     "import mlflow\n",
63 |     "\n",
64 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
65 |     "mlflow.set_experiment(\"DSPy\")\n",
66 |     "```\n",
67 |     "\n",
68 |     "4. Enabling tracing.\n",
69 |     "```python\n",
70 |     "mlflow.dspy.autolog()\n",
71 |     "```\n",
72 |     "\n",
73 |     "\n",
74 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
75 |     "</details>"
76 |    ]
77 |   },
78 |   {
79 |    "cell_type": "markdown",
80 |    "metadata": {},
81 |    "source": [
82 |     "### Dataset\n",
83 |     "\n",
84 |     "For this tutorial, we will use the Banking77 dataset."
85 |    ]
86 |   },
87 |   {
88 |    "cell_type": "code",
89 |    "execution_count": 1,
90 |    "metadata": {},
91 |    "outputs": [],
92 |    "source": [
93 |     "import dspy\n",
94 |     "import random\n",
95 |     "from dspy.datasets import DataLoader\n",
96 |     "from datasets import load_dataset\n",
97 |     "\n",
98 |     "# Load the Banking77 dataset.\n",
99 |     "CLASSES = load_dataset(\"PolyAI/banking77\", split=\"train\", trust_remote_code=True).features['label'].names\n",
100 |     "kwargs = dict(fields=(\"text\", \"label\"), input_keys=(\"text\",), split=\"train\", trust_remote_code=True)\n",
101 |     "\n",
102 |     "# Load the first 2000 examples from the dataset, and assign a hint to each *training* example.\n",
103 |     "raw_data = [\n",
104 |     "    dspy.Example(x, label=CLASSES[x.label]).with_inputs(\"text\")\n",
105 |     "    for x in DataLoader().from_huggingface(dataset_name=\"PolyAI/banking77\", **kwargs)[:1000]\n",
106 |     "]\n",
107 |     "\n",
108 |     "random.Random(0).shuffle(raw_data)"
109 |    ]
110 |   },
111 |   {
112 |    "cell_type": "markdown",
113 |    "metadata": {},
114 |    "source": [
115 |     "This dataset has 77 different categories for classification. Let's review some of them."
116 |    ]
117 |   },
118 |   {
119 |    "cell_type": "code",
120 |    "execution_count": 2,
121 |    "metadata": {},
122 |    "outputs": [
123 |     {
124 |      "data": {
125 |       "text/plain": [
126 |        "(77,\n",
127 |        " ['activate_my_card',\n",
128 |        "  'age_limit',\n",
129 |        "  'apple_pay_or_google_pay',\n",
130 |        "  'atm_support',\n",
131 |        "  'automatic_top_up',\n",
132 |        "  'balance_not_updated_after_bank_transfer',\n",
133 |        "  'balance_not_updated_after_cheque_or_cash_deposit',\n",
134 |        "  'beneficiary_not_allowed',\n",
135 |        "  'cancel_transfer',\n",
136 |        "  'card_about_to_expire'])"
137 |       ]
138 |      },
139 |      "execution_count": 2,
140 |      "metadata": {},
141 |      "output_type": "execute_result"
142 |     }
143 |    ],
144 |    "source": [
145 |     "len(CLASSES), CLASSES[:10]"
146 |    ]
147 |   },
148 |   {
149 |    "cell_type": "markdown",
150 |    "metadata": {},
151 |    "source": [
152 |     "Let us sample 500 (unlabeled) queries from Banking77. We'll use these for our bootstrapped finetuning."
153 |    ]
154 |   },
155 |   {
156 |    "cell_type": "code",
157 |    "execution_count": 3,
158 |    "metadata": {},
159 |    "outputs": [
160 |     {
161 |      "data": {
162 |       "text/plain": [
163 |        "Example({'text': 'What if there is an error on the exchange rate?'}) (input_keys={'text'})"
164 |       ]
165 |      },
166 |      "execution_count": 3,
167 |      "metadata": {},
168 |      "output_type": "execute_result"
169 |     }
170 |    ],
171 |    "source": [
172 |     "unlabeled_trainset = [dspy.Example(text=x.text).with_inputs(\"text\") for x in raw_data[:500]]\n",
173 |     "\n",
174 |     "unlabeled_trainset[0]"
175 |    ]
176 |   },
177 |   {
178 |    "cell_type": "markdown",
179 |    "metadata": {},
180 |    "source": [
181 |     "### DSPy program\n",
182 |     "\n",
183 |     "Let's say that we want a program that takes the `text` and reasons step by step and then selects one of the classes from Banking77.\n",
184 |     "\n",
185 |     "Note that this is meant mainly for illustration, or for cases where you want to inspect the model's reasoning, e.g. for a small degree of explainability. In other words, this type of task is not necessarily likely to benefit very much from explicit reasoning."
186 |    ]
187 |   },
188 |   {
189 |    "cell_type": "code",
190 |    "execution_count": 4,
191 |    "metadata": {},
192 |    "outputs": [],
193 |    "source": [
194 |     "from typing import Literal\n",
195 |     "\n",
196 |     "classify = dspy.ChainOfThought(f\"text -> label: Literal{CLASSES}\")"
197 |    ]
198 |   },
199 |   {
200 |    "cell_type": "markdown",
201 |    "metadata": {},
202 |    "source": [
203 |     "### Bootstrapped finetuning\n",
204 |     "\n",
205 |     "There are many ways to go about this, e.g. allowing the model to teach itself or using inference-time compute (e.g., ensembling) to identify cases of high confidence without labels.\n",
206 |     "\n",
207 |     "Perhaps the simplest is to use a model that we'd expect can do a reasonable job at this task as a teacher of reasoning and classification, and to distill that to our small model. All of these patterns can be expressed in a handful of lines of code.\n",
208 |     "\n",
209 |     "Let's set up the tiny `Llama-3.2-1B-Instruct` as a student LM. We'll use GPT-4o-mini as a teacher LM."
210 |    ]
211 |   },
212 |   {
213 |    "cell_type": "code",
214 |    "execution_count": null,
215 |    "metadata": {},
216 |    "outputs": [],
217 |    "source": [
218 |     "from dspy.clients.lm_local import LocalProvider\n",
219 |     "\n",
220 |     "student_lm_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
221 |     "student_lm = dspy.LM(model=f\"openai/local:{student_lm_name}\", provider=LocalProvider(), max_tokens=2000)\n",
222 |     "teacher_lm = dspy.LM('openai/gpt-4o-mini', max_tokens=3000)"
223 |    ]
224 |   },
225 |   {
226 |    "cell_type": "markdown",
227 |    "metadata": {},
228 |    "source": [
229 |     "Now, let's assign classifiers to our LMs."
230 |    ]
231 |   },
232 |   {
233 |    "cell_type": "code",
234 |    "execution_count": 6,
235 |    "metadata": {},
236 |    "outputs": [],
237 |    "source": [
238 |     "student_classify = classify.deepcopy()\n",
239 |     "student_classify.set_lm(student_lm)\n",
240 |     "\n",
241 |     "teacher_classify = classify.deepcopy()\n",
242 |     "teacher_classify.set_lm(teacher_lm)"
243 |    ]
244 |   },
245 |   {
246 |    "cell_type": "markdown",
247 |    "metadata": {},
248 |    "source": [
249 |     "Let's now launch the bootstrapped finetuning. The word \"bootstrapped\" here means that the program itself will be invoked on the training inputs and the resulting traces seen over all modules will be recorded and used for finetuning. This is the weight-optimizing variant of the various BootstrapFewShot methods in DSPy.\n",
250 |     "\n",
251 |     "On every question in the (unlabeled) training set, this will invoke the teacher program, which will produce reasoning and select a class. This will be traced and then constitute a training set for all modules (in this case, just the one CoT module) in the student program.\n",
252 |     "\n",
253 |     "When the `compile` method is called, the `BootstrapFinetune` optimizer will use the passed teacher program (or programs, you can pass a list!) to create a training dataset.\n",
254 |     "It will then use this training dataset to create a fine-tuned version of the LM set for the `student` program, replacing it with the trained LM.\n",
255 |     "Note that the trained LM will be a new LM instance (the `student_lm` object we instantiated here will be untouched!)\n",
256 |     "\n",
257 |     "Note: If you have labels, you can pass `metric` to the constructor of `BootstrapFinetune`. If you want to apply this in practice, you can pass `train_kwargs` to the constructor to control local LM training settings: `device`, `use_peft`, `num_train_epochs`, `per_device_train_batch_size`, `gradient_accumulation_steps`, `learning_rate`, `max_seq_length`, `packing`, `bf16`, and `output_dir`."
258 |    ]
259 |   },
260 |   {
261 |    "cell_type": "code",
262 |    "execution_count": null,
263 |    "metadata": {},
264 |    "outputs": [],
265 |    "source": [
266 |     "# Optional:\n",
267 |     "# [1] You can set `DSPY_FINETUNEDIR` environment variable to control where the directory that will be used to store the\n",
268 |     "#     checkpoints and fine-tuning data. If this is not set, `DSPY_CACHEDIR` is used by default.\n",
269 |     "# [2] You can set the `CUDA_VISIBLE_DEVICES` environment variable to control the GPU that will be used for fine-tuning\n",
270 |     "#     and inference. If this is not set and the default GPU that's used by HuggingFace's `transformers` library is\n",
271 |     "#     occupied, an OutOfMemoryError might be raised.\n",
272 |     "#\n",
273 |     "# import os\n",
274 |     "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
275 |     "# os.environ[\"DSPY_FINETUNEDIR\"] = \"/path/to/dir\""
276 |    ]
277 |   },
278 |   {
279 |    "cell_type": "code",
280 |    "execution_count": 7,
281 |    "metadata": {},
282 |    "outputs": [],
283 |    "source": [
284 |     "dspy.settings.experimental = True  # fine-tuning is an experimental feature, so we set a flag to enable it\n",
285 |     "\n",
286 |     "optimizer = dspy.BootstrapFinetune(num_threads=16)  # if you *do* have labels, pass metric=your_metric here!\n",
287 |     "classify_ft = optimizer.compile(student_classify, teacher=teacher_classify, trainset=unlabeled_trainset)"
288 |    ]
289 |   },
290 |   {
291 |    "cell_type": "markdown",
292 |    "metadata": {},
293 |    "source": [
294 |     "Since this is a local model, we need to explicitly launch it."
295 |    ]
296 |   },
297 |   {
298 |    "cell_type": "code",
299 |    "execution_count": 8,
300 |    "metadata": {},
301 |    "outputs": [],
302 |    "source": [
303 |     "classify_ft.get_lm().launch()"
304 |    ]
305 |   },
306 |   {
307 |    "cell_type": "markdown",
308 |    "metadata": {},
309 |    "source": [
310 |     "### Validating the finetuned program\n",
311 |     "\n",
312 |     "Let's now figure out if this was successful. We can ask the system one question and inspect its behavior."
313 |    ]
314 |   },
315 |   {
316 |    "cell_type": "code",
317 |    "execution_count": 9,
318 |    "metadata": {},
319 |    "outputs": [
320 |     {
321 |      "data": {
322 |       "text/plain": [
323 |        "Prediction(\n",
324 |        "    reasoning='The user is inquiring about a specific issue, which they did not receive and is still showing as a pending transaction. This situation typically indicates a problem with the cash withdrawal process, as the user is not receiving the money they attempted to withdraw. The appropriate label for this scenario is \"pending_cash_withdrawal,\" as it directly relates to the status of the cash withdrawal transaction.',\n",
325 |        "    label='pending_cash_withdrawal'\n",
326 |        ")"
327 |       ]
328 |      },
329 |      "execution_count": 9,
330 |      "metadata": {},
331 |      "output_type": "execute_result"
332 |     }
333 |    ],
334 |    "source": [
335 |     "classify_ft(text=\"I didn't receive my money earlier and it says the transaction is still in progress. Can you fix it?\")"
336 |    ]
337 |   },
338 |   {
339 |    "cell_type": "markdown",
340 |    "metadata": {},
341 |    "source": [
342 |     "We could also get a small set of gold labels and see if the system can generalize to unseen queries."
343 |    ]
344 |   },
345 |   {
346 |    "cell_type": "code",
347 |    "execution_count": 10,
348 |    "metadata": {},
349 |    "outputs": [
350 |     {
351 |      "data": {
352 |       "text/plain": [
353 |        "Example({'text': 'Which fiat currencies do you currently support? Will this change in this future?', 'label': 'fiat_currency_support'}) (input_keys={'text'})"
354 |       ]
355 |      },
356 |      "execution_count": 10,
357 |      "metadata": {},
358 |      "output_type": "execute_result"
359 |     }
360 |    ],
361 |    "source": [
362 |     "devset = raw_data[500:600]\n",
363 |     "devset[0]"
364 |    ]
365 |   },
366 |   {
367 |    "cell_type": "markdown",
368 |    "metadata": {},
369 |    "source": [
370 |     "Let's define an evaluator on this small dev set, where the metric ignores the reasoning and checks that the label is exactly correct."
371 |    ]
372 |   },
373 |   {
374 |    "cell_type": "code",
375 |    "execution_count": 11,
376 |    "metadata": {},
377 |    "outputs": [],
378 |    "source": [
379 |     "metric = (lambda x, y, trace=None: x.label == y.label)\n",
380 |     "evaluate = dspy.Evaluate(devset=devset, metric=metric, display_progress=True, display_table=5, num_threads=16)"
381 |    ]
382 |   },
383 |   {
384 |    "cell_type": "markdown",
385 |    "metadata": {},
386 |    "source": [
387 |     "Now, let's evaluate the finetuned 1B classifier."
388 |    ]
389 |   },
390 |   {
391 |    "cell_type": "code",
392 |    "execution_count": 12,
393 |    "metadata": {},
394 |    "outputs": [
395 |     {
396 |      "name": "stdout",
397 |      "output_type": "stream",
398 |      "text": [
399 |       "Average Metric: 51.00 / 99 (51.5%): 100%|██████████| 100/100 [00:35<00:00,  2.79it/s]"
400 |      ]
401 |     },
402 |     {
403 |      "data": {
404 |       "text/html": [
405 |        "<div>\n",
406 |        "<style scoped>\n",
407 |        "    .dataframe tbody tr th:only-of-type {\n",
408 |        "        vertical-align: middle;\n",
409 |        "    }\n",
410 |        "\n",
411 |        "    .dataframe tbody tr th {\n",
412 |        "        vertical-align: top;\n",
413 |        "    }\n",
414 |        "\n",
415 |        "    .dataframe thead th {\n",
416 |        "        text-align: right;\n",
417 |        "    }\n",
418 |        "</style>\n",
419 |        "<table border=\"1\" class=\"dataframe\">\n",
420 |        "  <thead>\n",
421 |        "    <tr style=\"text-align: right;\">\n",
422 |        "      <th></th>\n",
423 |        "      <th>text</th>\n",
424 |        "      <th>example_label</th>\n",
425 |        "      <th>reasoning</th>\n",
426 |        "      <th>pred_label</th>\n",
427 |        "      <th>&lt;lambda&gt;</th>\n",
428 |        "      <th>label</th>\n",
429 |        "    </tr>\n",
430 |        "  </thead>\n",
431 |        "  <tbody>\n",
432 |        "    <tr>\n",
433 |        "      <th>0</th>\n",
434 |        "      <td>Which fiat currencies do you currently support? Will this change i...</td>\n",
435 |        "      <td>fiat_currency_support</td>\n",
436 |        "      <td>The user is inquiring about the current support for fiat currencie...</td>\n",
437 |        "      <td>fiat_currency_support</td>\n",
438 |        "      <td>✔️ [True]</td>\n",
439 |        "      <td>NaN</td>\n",
440 |        "    </tr>\n",
441 |        "    <tr>\n",
442 |        "      <th>1</th>\n",
443 |        "      <td>I didn't receive my money earlier and it says the transaction is s...</td>\n",
444 |        "      <td>pending_cash_withdrawal</td>\n",
445 |        "      <td>The user is inquiring about a specific issue, which they did not r...</td>\n",
446 |        "      <td>pending_cash_withdrawal</td>\n",
447 |        "      <td>✔️ [True]</td>\n",
448 |        "      <td>NaN</td>\n",
449 |        "    </tr>\n",
450 |        "    <tr>\n",
451 |        "      <th>2</th>\n",
452 |        "      <td>what currencies do you accept?</td>\n",
453 |        "      <td>fiat_currency_support</td>\n",
454 |        "      <td>The user is inquiring about the currencies that are accepted, whic...</td>\n",
455 |        "      <td>fiat_currency_support</td>\n",
456 |        "      <td>✔️ [True]</td>\n",
457 |        "      <td>NaN</td>\n",
458 |        "    </tr>\n",
459 |        "    <tr>\n",
460 |        "      <th>3</th>\n",
461 |        "      <td>Where can I find your exchange rates?</td>\n",
462 |        "      <td>exchange_rate</td>\n",
463 |        "      <td>The user is inquiring about where to find exchange rates, which re...</td>\n",
464 |        "      <td>exchange_rate</td>\n",
465 |        "      <td>✔️ [True]</td>\n",
466 |        "      <td>NaN</td>\n",
467 |        "    </tr>\n",
468 |        "    <tr>\n",
469 |        "      <th>4</th>\n",
470 |        "      <td>why hasnt my card come in yet?</td>\n",
471 |        "      <td>card_arrival</td>\n",
472 |        "      <td>The user is inquiring about the status of their card, which sugges...</td>\n",
473 |        "      <td>card_arrival</td>\n",
474 |        "      <td>✔️ [True]</td>\n",
475 |        "      <td>NaN</td>\n",
476 |        "    </tr>\n",
477 |        "  </tbody>\n",
478 |        "</table>\n",
479 |        "</div>"
480 |       ],
481 |       "text/plain": [
482 |        "                                                                    text  \\\n",
483 |        "0  Which fiat currencies do you currently support? Will this change i...   \n",
484 |        "1  I didn't receive my money earlier and it says the transaction is s...   \n",
485 |        "2                                         what currencies do you accept?   \n",
486 |        "3                                  Where can I find your exchange rates?   \n",
487 |        "4                                         why hasnt my card come in yet?   \n",
488 |        "\n",
489 |        "             example_label  \\\n",
490 |        "0    fiat_currency_support   \n",
491 |        "1  pending_cash_withdrawal   \n",
492 |        "2    fiat_currency_support   \n",
493 |        "3            exchange_rate   \n",
494 |        "4             card_arrival   \n",
495 |        "\n",
496 |        "                                                               reasoning  \\\n",
497 |        "0  The user is inquiring about the current support for fiat currencie...   \n",
498 |        "1  The user is inquiring about a specific issue, which they did not r...   \n",
499 |        "2  The user is inquiring about the currencies that are accepted, whic...   \n",
500 |        "3  The user is inquiring about where to find exchange rates, which re...   \n",
501 |        "4  The user is inquiring about the status of their card, which sugges...   \n",
502 |        "\n",
503 |        "                pred_label   <lambda> label  \n",
504 |        "0    fiat_currency_support  ✔️ [True]   NaN  \n",
505 |        "1  pending_cash_withdrawal  ✔️ [True]   NaN  \n",
506 |        "2    fiat_currency_support  ✔️ [True]   NaN  \n",
507 |        "3            exchange_rate  ✔️ [True]   NaN  \n",
508 |        "4             card_arrival  ✔️ [True]   NaN  "
509 |       ]
510 |      },
511 |      "metadata": {},
512 |      "output_type": "display_data"
513 |     },
514 |     {
515 |      "data": {
516 |       "text/html": [
517 |        "\n",
518 |        "                <div style='\n",
519 |        "                    text-align: center;\n",
520 |        "                    font-size: 16px;\n",
521 |        "                    font-weight: bold;\n",
522 |        "                    color: #555;\n",
523 |        "                    margin: 10px 0;'>\n",
524 |        "                    ... 95 more rows not displayed ...\n",
525 |        "                </div>\n",
526 |        "                "
527 |       ],
528 |       "text/plain": [
529 |        "<IPython.core.display.HTML object>"
530 |       ]
531 |      },
532 |      "metadata": {},
533 |      "output_type": "display_data"
534 |     },
535 |     {
536 |      "data": {
537 |       "text/plain": [
538 |        "51.0"
539 |       ]
540 |      },
541 |      "execution_count": 13,
542 |      "metadata": {},
543 |      "output_type": "execute_result"
544 |     }
545 |    ],
546 |    "source": [
547 |     "evaluate(classify_ft)"
548 |    ]
549 |   },
550 |   {
551 |    "cell_type": "markdown",
552 |    "metadata": {},
553 |    "source": [
554 |     "<details>\n",
555 |     "<summary>Tracking Evaluation Results in MLflow Experiment</summary>\n",
556 |     "\n",
557 |     "<br/>\n",
558 |     "\n",
559 |     "To track and visualize the evaluation results over time, you can record the results in MLflow Experiment.\n",
560 |     "\n",
561 |     "\n",
562 |     "```python\n",
563 |     "import mlflow\n",
564 |     "\n",
565 |     "with mlflow.start_run(run_name=\"classifier_evaluation\"):\n",
566 |     "    evaluate_correctness = dspy.Evaluate(\n",
567 |     "        devset=devset,\n",
568 |     "        metric=extraction_correctness_metric,\n",
569 |     "        num_threads=16,\n",
570 |     "        display_progress=True,\n",
571 |     "    )\n",
572 |     "\n",
573 |     "    # Evaluate the program as usual\n",
574 |     "    result = evaluate_correctness(people_extractor)\n",
575 |     "\n",
576 |     "    # Log the aggregated score\n",
577 |     "    mlflow.log_metric(\"exact_match\", result.score)\n",
578 |     "    # Log the detailed evaluation results as a table\n",
579 |     "    mlflow.log_table(\n",
580 |     "        {\n",
581 |     "            \"Text\": [example.text for example in devset],\n",
582 |     "            \"Expected\": [example.example_label for example in devset],\n",
583 |     "            \"Predicted\": [output[1] for output in result.results],\n",
584 |     "            \"Exact match\": [output[2] for output in result.results],\n",
585 |     "        },\n",
586 |     "        artifact_file=\"eval_results.json\",\n",
587 |     "    )\n",
588 |     "```\n",
589 |     "\n",
590 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
591 |     "\n",
592 |     "</details>"
593 |    ]
594 |   },
595 |   {
596 |    "cell_type": "markdown",
597 |    "metadata": {},
598 |    "source": [
599 |     "Not bad, given that we started with no labels of the task. Even though we have no labels, you can use various strategies to boost the quality of the bootstrapped training data.\n",
600 |     "\n",
601 |     "To try that next, let's free our GPU memory by killing the finetuned LM."
602 |    ]
603 |   },
604 |   {
605 |    "cell_type": "code",
606 |    "execution_count": 14,
607 |    "metadata": {},
608 |    "outputs": [],
609 |    "source": [
610 |     "classify_ft.get_lm().kill()"
611 |    ]
612 |   },
613 |   {
614 |    "cell_type": "markdown",
615 |    "metadata": {},
616 |    "source": [
617 |     "### Bootstrapped finetuning against a metric\n",
618 |     "\n",
619 |     "If you have labels, you can generally boost this by a large margin. To do so, you can pass a `metric` to BootstrapFinetune, which it will use for filtering the trajectories over your program before it builds the finetuning data."
620 |    ]
621 |   },
622 |   {
623 |    "cell_type": "code",
624 |    "execution_count": 15,
625 |    "metadata": {},
626 |    "outputs": [],
627 |    "source": [
628 |     "optimizer = dspy.BootstrapFinetune(num_threads=16, metric=metric)\n",
629 |     "classify_ft = optimizer.compile(student_classify, teacher=teacher_classify, trainset=raw_data[:500])"
630 |    ]
631 |   },
632 |   {
633 |    "cell_type": "markdown",
634 |    "metadata": {},
635 |    "source": [
636 |     "Let's now launch and evaluate this."
637 |    ]
638 |   },
639 |   {
640 |    "cell_type": "code",
641 |    "execution_count": 16,
642 |    "metadata": {},
643 |    "outputs": [],
644 |    "source": [
645 |     "classify_ft.get_lm().launch()"
646 |    ]
647 |   },
648 |   {
649 |    "cell_type": "code",
650 |    "execution_count": 17,
651 |    "metadata": {},
652 |    "outputs": [
653 |     {
654 |      "name": "stdout",
655 |      "output_type": "stream",
656 |      "text": [
657 |       "Average Metric: 85.00 / 98 (86.7%): 100%|██████████| 100/100 [00:46<00:00,  2.14it/s]"
658 |      ]
659 |     },
660 |     {
661 |      "name": "stdout",
662 |      "output_type": "stream",
663 |      "text": [
664 |       "\n"
665 |      ]
666 |     },
667 |     {
668 |      "data": {
669 |       "text/html": [
670 |        "<div>\n",
671 |        "<style scoped>\n",
672 |        "    .dataframe tbody tr th:only-of-type {\n",
673 |        "        vertical-align: middle;\n",
674 |        "    }\n",
675 |        "\n",
676 |        "    .dataframe tbody tr th {\n",
677 |        "        vertical-align: top;\n",
678 |        "    }\n",
679 |        "\n",
680 |        "    .dataframe thead th {\n",
681 |        "        text-align: right;\n",
682 |        "    }\n",
683 |        "</style>\n",
684 |        "<table border=\"1\" class=\"dataframe\">\n",
685 |        "  <thead>\n",
686 |        "    <tr style=\"text-align: right;\">\n",
687 |        "      <th></th>\n",
688 |        "      <th>text</th>\n",
689 |        "      <th>example_label</th>\n",
690 |        "      <th>reasoning</th>\n",
691 |        "      <th>pred_label</th>\n",
692 |        "      <th>&lt;lambda&gt;</th>\n",
693 |        "      <th>label</th>\n",
694 |        "    </tr>\n",
695 |        "  </thead>\n",
696 |        "  <tbody>\n",
697 |        "    <tr>\n",
698 |        "      <th>0</th>\n",
699 |        "      <td>Which fiat currencies do you currently support? Will this change i...</td>\n",
700 |        "      <td>fiat_currency_support</td>\n",
701 |        "      <td>The user is inquiring about the fiat currencies currently supporte...</td>\n",
702 |        "      <td>fiat_currency_support</td>\n",
703 |        "      <td>✔️ [True]</td>\n",
704 |        "      <td>NaN</td>\n",
705 |        "    </tr>\n",
706 |        "    <tr>\n",
707 |        "      <th>1</th>\n",
708 |        "      <td>I didn't receive my money earlier and it says the transaction is s...</td>\n",
709 |        "      <td>pending_cash_withdrawal</td>\n",
710 |        "      <td>The user is inquiring about an unexpected fee on their account, wh...</td>\n",
711 |        "      <td>extra_charge_on_statement</td>\n",
712 |        "      <td></td>\n",
713 |        "      <td>NaN</td>\n",
714 |        "    </tr>\n",
715 |        "    <tr>\n",
716 |        "      <th>2</th>\n",
717 |        "      <td>what currencies do you accept?</td>\n",
718 |        "      <td>fiat_currency_support</td>\n",
719 |        "      <td>The user is inquiring about the types of currencies that are accep...</td>\n",
720 |        "      <td>fiat_currency_support</td>\n",
721 |        "      <td>✔️ [True]</td>\n",
722 |        "      <td>NaN</td>\n",
723 |        "    </tr>\n",
724 |        "    <tr>\n",
725 |        "      <th>3</th>\n",
726 |        "      <td>Where can I find your exchange rates?</td>\n",
727 |        "      <td>exchange_rate</td>\n",
728 |        "      <td>The user is inquiring about where to find exchange rates, which re...</td>\n",
729 |        "      <td>exchange_rate</td>\n",
730 |        "      <td>✔️ [True]</td>\n",
731 |        "      <td>NaN</td>\n",
732 |        "    </tr>\n",
733 |        "    <tr>\n",
734 |        "      <th>4</th>\n",
735 |        "      <td>why hasnt my card come in yet?</td>\n",
736 |        "      <td>card_arrival</td>\n",
737 |        "      <td>The user is inquiring about the status of their card delivery, whi...</td>\n",
738 |        "      <td>card_arrival</td>\n",
739 |        "      <td>✔️ [True]</td>\n",
740 |        "      <td>NaN</td>\n",
741 |        "    </tr>\n",
742 |        "  </tbody>\n",
743 |        "</table>\n",
744 |        "</div>"
745 |       ],
746 |       "text/plain": [
747 |        "                                                                    text  \\\n",
748 |        "0  Which fiat currencies do you currently support? Will this change i...   \n",
749 |        "1  I didn't receive my money earlier and it says the transaction is s...   \n",
750 |        "2                                         what currencies do you accept?   \n",
751 |        "3                                  Where can I find your exchange rates?   \n",
752 |        "4                                         why hasnt my card come in yet?   \n",
753 |        "\n",
754 |        "             example_label  \\\n",
755 |        "0    fiat_currency_support   \n",
756 |        "1  pending_cash_withdrawal   \n",
757 |        "2    fiat_currency_support   \n",
758 |        "3            exchange_rate   \n",
759 |        "4             card_arrival   \n",
760 |        "\n",
761 |        "                                                               reasoning  \\\n",
762 |        "0  The user is inquiring about the fiat currencies currently supporte...   \n",
763 |        "1  The user is inquiring about an unexpected fee on their account, wh...   \n",
764 |        "2  The user is inquiring about the types of currencies that are accep...   \n",
765 |        "3  The user is inquiring about where to find exchange rates, which re...   \n",
766 |        "4  The user is inquiring about the status of their card delivery, whi...   \n",
767 |        "\n",
768 |        "                  pred_label   <lambda> label  \n",
769 |        "0      fiat_currency_support  ✔️ [True]   NaN  \n",
770 |        "1  extra_charge_on_statement              NaN  \n",
771 |        "2      fiat_currency_support  ✔️ [True]   NaN  \n",
772 |        "3              exchange_rate  ✔️ [True]   NaN  \n",
773 |        "4               card_arrival  ✔️ [True]   NaN  "
774 |       ]
775 |      },
776 |      "metadata": {},
777 |      "output_type": "display_data"
778 |     },
779 |     {
780 |      "data": {
781 |       "text/html": [
782 |        "\n",
783 |        "                <div style='\n",
784 |        "                    text-align: center;\n",
785 |        "                    font-size: 16px;\n",
786 |        "                    font-weight: bold;\n",
787 |        "                    color: #555;\n",
788 |        "                    margin: 10px 0;'>\n",
789 |        "                    ... 95 more rows not displayed ...\n",
790 |        "                </div>\n",
791 |        "                "
792 |       ],
793 |       "text/plain": [
794 |        "<IPython.core.display.HTML object>"
795 |       ]
796 |      },
797 |      "metadata": {},
798 |      "output_type": "display_data"
799 |     },
800 |     {
801 |      "data": {
802 |       "text/plain": [
803 |        "85.0"
804 |       ]
805 |      },
806 |      "execution_count": 29,
807 |      "metadata": {},
808 |      "output_type": "execute_result"
809 |     }
810 |    ],
811 |    "source": [
812 |     "evaluate(classify_ft)"
813 |    ]
814 |   },
815 |   {
816 |    "cell_type": "markdown",
817 |    "metadata": {},
818 |    "source": [
819 |     "That's quite a bit better, given just 500 labels. In fact, it seems to be a lot stronger than the teacher LM gets out of the box!"
820 |    ]
821 |   },
822 |   {
823 |    "cell_type": "code",
824 |    "execution_count": 30,
825 |    "metadata": {},
826 |    "outputs": [
827 |     {
828 |      "name": "stdout",
829 |      "output_type": "stream",
830 |      "text": [
831 |       "Average Metric: 55.00 / 100 (55.0%): 100%|██████████| 100/100 [00:11<00:00,  8.88it/s]"
832 |      ]
833 |     },
834 |     {
835 |      "name": "stderr",
836 |      "output_type": "stream",
837 |      "text": [
838 |       "2025/01/08 12:38:35 INFO dspy.evaluate.evaluate: Average Metric: 55 / 100 (55.0%)\n"
839 |      ]
840 |     },
841 |     {
842 |      "name": "stdout",
843 |      "output_type": "stream",
844 |      "text": [
845 |       "\n"
846 |      ]
847 |     },
848 |     {
849 |      "data": {
850 |       "text/html": [
851 |        "<div>\n",
852 |        "<style scoped>\n",
853 |        "    .dataframe tbody tr th:only-of-type {\n",
854 |        "        vertical-align: middle;\n",
855 |        "    }\n",
856 |        "\n",
857 |        "    .dataframe tbody tr th {\n",
858 |        "        vertical-align: top;\n",
859 |        "    }\n",
860 |        "\n",
861 |        "    .dataframe thead th {\n",
862 |        "        text-align: right;\n",
863 |        "    }\n",
864 |        "</style>\n",
865 |        "<table border=\"1\" class=\"dataframe\">\n",
866 |        "  <thead>\n",
867 |        "    <tr style=\"text-align: right;\">\n",
868 |        "      <th></th>\n",
869 |        "      <th>text</th>\n",
870 |        "      <th>example_label</th>\n",
871 |        "      <th>reasoning</th>\n",
872 |        "      <th>pred_label</th>\n",
873 |        "      <th>&lt;lambda&gt;</th>\n",
874 |        "    </tr>\n",
875 |        "  </thead>\n",
876 |        "  <tbody>\n",
877 |        "    <tr>\n",
878 |        "      <th>0</th>\n",
879 |        "      <td>Which fiat currencies do you currently support? Will this change i...</td>\n",
880 |        "      <td>fiat_currency_support</td>\n",
881 |        "      <td>The user is inquiring about the fiat currencies supported by the s...</td>\n",
882 |        "      <td>fiat_currency_support</td>\n",
883 |        "      <td>✔️ [True]</td>\n",
884 |        "    </tr>\n",
885 |        "    <tr>\n",
886 |        "      <th>1</th>\n",
887 |        "      <td>I didn't receive my money earlier and it says the transaction is s...</td>\n",
888 |        "      <td>pending_cash_withdrawal</td>\n",
889 |        "      <td>The user is experiencing an issue with a transaction that is still...</td>\n",
890 |        "      <td>pending_transfer</td>\n",
891 |        "      <td></td>\n",
892 |        "    </tr>\n",
893 |        "    <tr>\n",
894 |        "      <th>2</th>\n",
895 |        "      <td>what currencies do you accept?</td>\n",
896 |        "      <td>fiat_currency_support</td>\n",
897 |        "      <td>The question is asking about the types of currencies accepted, whi...</td>\n",
898 |        "      <td>fiat_currency_support</td>\n",
899 |        "      <td>✔️ [True]</td>\n",
900 |        "    </tr>\n",
901 |        "    <tr>\n",
902 |        "      <th>3</th>\n",
903 |        "      <td>Where can I find your exchange rates?</td>\n",
904 |        "      <td>exchange_rate</td>\n",
905 |        "      <td>The user is inquiring about where to find exchange rates, which re...</td>\n",
906 |        "      <td>exchange_rate</td>\n",
907 |        "      <td>✔️ [True]</td>\n",
908 |        "    </tr>\n",
909 |        "    <tr>\n",
910 |        "      <th>4</th>\n",
911 |        "      <td>why hasnt my card come in yet?</td>\n",
912 |        "      <td>card_arrival</td>\n",
913 |        "      <td>The user is inquiring about the status of their card delivery, whi...</td>\n",
914 |        "      <td>card_delivery_estimate</td>\n",
915 |        "      <td></td>\n",
916 |        "    </tr>\n",
917 |        "  </tbody>\n",
918 |        "</table>\n",
919 |        "</div>"
920 |       ],
921 |       "text/plain": [
922 |        "                                                                    text  \\\n",
923 |        "0  Which fiat currencies do you currently support? Will this change i...   \n",
924 |        "1  I didn't receive my money earlier and it says the transaction is s...   \n",
925 |        "2                                         what currencies do you accept?   \n",
926 |        "3                                  Where can I find your exchange rates?   \n",
927 |        "4                                         why hasnt my card come in yet?   \n",
928 |        "\n",
929 |        "             example_label  \\\n",
930 |        "0    fiat_currency_support   \n",
931 |        "1  pending_cash_withdrawal   \n",
932 |        "2    fiat_currency_support   \n",
933 |        "3            exchange_rate   \n",
934 |        "4             card_arrival   \n",
935 |        "\n",
936 |        "                                                               reasoning  \\\n",
937 |        "0  The user is inquiring about the fiat currencies supported by the s...   \n",
938 |        "1  The user is experiencing an issue with a transaction that is still...   \n",
939 |        "2  The question is asking about the types of currencies accepted, whi...   \n",
940 |        "3  The user is inquiring about where to find exchange rates, which re...   \n",
941 |        "4  The user is inquiring about the status of their card delivery, whi...   \n",
942 |        "\n",
943 |        "               pred_label   <lambda>  \n",
944 |        "0   fiat_currency_support  ✔️ [True]  \n",
945 |        "1        pending_transfer             \n",
946 |        "2   fiat_currency_support  ✔️ [True]  \n",
947 |        "3           exchange_rate  ✔️ [True]  \n",
948 |        "4  card_delivery_estimate             "
949 |       ]
950 |      },
951 |      "metadata": {},
952 |      "output_type": "display_data"
953 |     },
954 |     {
955 |      "data": {
956 |       "text/html": [
957 |        "\n",
958 |        "                <div style='\n",
959 |        "                    text-align: center;\n",
960 |        "                    font-size: 16px;\n",
961 |        "                    font-weight: bold;\n",
962 |        "                    color: #555;\n",
963 |        "                    margin: 10px 0;'>\n",
964 |        "                    ... 95 more rows not displayed ...\n",
965 |        "                </div>\n",
966 |        "                "
967 |       ],
968 |       "text/plain": [
969 |        "<IPython.core.display.HTML object>"
970 |       ]
971 |      },
972 |      "metadata": {},
973 |      "output_type": "display_data"
974 |     },
975 |     {
976 |      "data": {
977 |       "text/plain": [
978 |        "55.0"
979 |       ]
980 |      },
981 |      "execution_count": 17,
982 |      "metadata": {},
983 |      "output_type": "execute_result"
984 |     }
985 |    ],
986 |    "source": [
987 |     "evaluate(teacher_classify)"
988 |    ]
989 |   },
990 |   {
991 |    "cell_type": "markdown",
992 |    "metadata": {},
993 |    "source": [
994 |     "And thanks to bootstrapping, the model learns to apply our modules to get the right label, in this case, reasoning explicitly:"
995 |    ]
996 |   },
997 |   {
998 |    "cell_type": "code",
999 |    "execution_count": 18,
1000 |    "metadata": {},
1001 |    "outputs": [
1002 |     {
1003 |      "name": "stdout",
1004 |      "output_type": "stream",
1005 |      "text": [
1006 |       "\n",
1007 |       "\n",
1008 |       "\n",
1009 |       "\n",
1010 |       "\u001b[34m[2025-01-08T12:39:42.143798]\u001b[0m\n",
1011 |       "\n",
1012 |       "\u001b[31mSystem message:\u001b[0m\n",
1013 |       "\n",
1014 |       "Your input fields are:\n",
1015 |       "1. `text` (str)\n",
1016 |       "\n",
1017 |       "Your output fields are:\n",
1018 |       "1. `reasoning` (str)\n",
1019 |       "2. `label` (Literal[activate_my_card, age_limit, apple_pay_or_google_pay, atm_support, automatic_top_up, balance_not_updated_after_bank_transfer, balance_not_updated_after_cheque_or_cash_deposit, beneficiary_not_allowed, cancel_transfer, card_about_to_expire, card_acceptance, card_arrival, card_delivery_estimate, card_linking, card_not_working, card_payment_fee_charged, card_payment_not_recognised, card_payment_wrong_exchange_rate, card_swallowed, cash_withdrawal_charge, cash_withdrawal_not_recognised, change_pin, compromised_card, contactless_not_working, country_support, declined_card_payment, declined_cash_withdrawal, declined_transfer, direct_debit_payment_not_recognised, disposable_card_limits, edit_personal_details, exchange_charge, exchange_rate, exchange_via_app, extra_charge_on_statement, failed_transfer, fiat_currency_support, get_disposable_virtual_card, get_physical_card, getting_spare_card, getting_virtual_card, lost_or_stolen_card, lost_or_stolen_phone, order_physical_card, passcode_forgotten, pending_card_payment, pending_cash_withdrawal, pending_top_up, pending_transfer, pin_blocked, receiving_money, Refund_not_showing_up, request_refund, reverted_card_payment?, supported_cards_and_currencies, terminate_account, top_up_by_bank_transfer_charge, top_up_by_card_charge, top_up_by_cash_or_cheque, top_up_failed, top_up_limits, top_up_reverted, topping_up_by_card, transaction_charged_twice, transfer_fee_charged, transfer_into_account, transfer_not_received_by_recipient, transfer_timing, unable_to_verify_identity, verify_my_identity, verify_source_of_funds, verify_top_up, virtual_card_not_working, visa_or_mastercard, why_verify_identity, wrong_amount_of_cash_received, wrong_exchange_rate_for_cash_withdrawal])\n",
1020 |       "\n",
1021 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
1022 |       "\n",
1023 |       "[[ ## text ## ]]\n",
1024 |       "{text}\n",
1025 |       "\n",
1026 |       "[[ ## reasoning ## ]]\n",
1027 |       "{reasoning}\n",
1028 |       "\n",
1029 |       "[[ ## label ## ]]\n",
1030 |       "{label}        # note: the value you produce must be one of: activate_my_card; age_limit; apple_pay_or_google_pay; atm_support; automatic_top_up; balance_not_updated_after_bank_transfer; balance_not_updated_after_cheque_or_cash_deposit; beneficiary_not_allowed; cancel_transfer; card_about_to_expire; card_acceptance; card_arrival; card_delivery_estimate; card_linking; card_not_working; card_payment_fee_charged; card_payment_not_recognised; card_payment_wrong_exchange_rate; card_swallowed; cash_withdrawal_charge; cash_withdrawal_not_recognised; change_pin; compromised_card; contactless_not_working; country_support; declined_card_payment; declined_cash_withdrawal; declined_transfer; direct_debit_payment_not_recognised; disposable_card_limits; edit_personal_details; exchange_charge; exchange_rate; exchange_via_app; extra_charge_on_statement; failed_transfer; fiat_currency_support; get_disposable_virtual_card; get_physical_card; getting_spare_card; getting_virtual_card; lost_or_stolen_card; lost_or_stolen_phone; order_physical_card; passcode_forgotten; pending_card_payment; pending_cash_withdrawal; pending_top_up; pending_transfer; pin_blocked; receiving_money; Refund_not_showing_up; request_refund; reverted_card_payment?; supported_cards_and_currencies; terminate_account; top_up_by_bank_transfer_charge; top_up_by_card_charge; top_up_by_cash_or_cheque; top_up_failed; top_up_limits; top_up_reverted; topping_up_by_card; transaction_charged_twice; transfer_fee_charged; transfer_into_account; transfer_not_received_by_recipient; transfer_timing; unable_to_verify_identity; verify_my_identity; verify_source_of_funds; verify_top_up; virtual_card_not_working; visa_or_mastercard; why_verify_identity; wrong_amount_of_cash_received; wrong_exchange_rate_for_cash_withdrawal\n",
1031 |       "\n",
1032 |       "[[ ## completed ## ]]\n",
1033 |       "\n",
1034 |       "In adhering to this structure, your objective is: \n",
1035 |       "        Given the fields `text`, produce the fields `label`.\n",
1036 |       "\n",
1037 |       "\n",
1038 |       "\u001b[31mUser message:\u001b[0m\n",
1039 |       "\n",
1040 |       "[[ ## text ## ]]\n",
1041 |       "why hasnt my card come in yet?\n",
1042 |       "\n",
[TRUNCATED]
```

tutorials/conversation_history/index.md
```
1 | # Managing Conversation History
2 | 
3 | Maintaining conversation history is a fundamental feature when building AI applications such as chatbots. While DSPy does not provide automatic conversation history management within `dspy.Module`, it offers the `dspy.History` utility to help you manage conversation history effectively.
4 | 
5 | ## Using `dspy.History` to Manage Conversation History
6 | 
7 | The `dspy.History` class can be used as an input field type, containing a `messages: list[dict[str, Any]]` attribute that stores the conversation history. Each entry in this list is a dictionary with keys corresponding to the fields defined in your signature. See the example below:
8 | 
9 | ```python
10 | import dspy
11 | import os
12 | 
13 | os.environ["OPENAI_API_KEY"] = "{your_openai_api_key}"
14 | 
15 | dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))
16 | 
17 | class QA(dspy.Signature):
18 |     question: str = dspy.InputField()
19 |     history: dspy.History = dspy.InputField()
20 |     answer: str = dspy.OutputField()
21 | 
22 | predict = dspy.Predict(QA)
23 | history = dspy.History(messages=[])
24 | 
25 | while True:
26 |     question = input("Type your question, end conversation by typing 'finish': ")
27 |     if question == "finish":
28 |         break
29 |     outputs = predict(question=question, history=history)
30 |     print(f"\n{outputs.answer}\n")
31 |     history.messages.append({"question": question, **outputs})
32 | 
33 | dspy.inspect_history()
34 | ```
35 | 
36 | There are two key steps when using the conversation history:
37 | 
38 | - **Include a field of type `dspy.History` in your Signature.**
39 | - **Maintain a history instance at runtime, appending new conversation turns to it.** Each entry should include all relevant input and output field information.
40 | 
41 | A sample run might look like this:
42 | 
43 | ```
44 | Type your question, end conversation by typing 'finish': do you know the competition between pytorch and tensorflow?
45 | 
46 | Yes, there is a notable competition between PyTorch and TensorFlow, which are two of the most popular deep learning frameworks. PyTorch, developed by Facebook, is known for its dynamic computation graph, which allows for more flexibility and ease of use, especially in research settings. TensorFlow, developed by Google, initially used a static computation graph but has since introduced eager execution to improve usability. TensorFlow is often favored in production environments due to its scalability and deployment capabilities. Both frameworks have strong communities and extensive libraries, and the choice between them often depends on specific project requirements and personal preference.
47 | 
48 | Type your question, end conversation by typing 'finish': which one won the battle? just tell me the result, don't include any reasoning, thanks!
49 | 
50 | There is no definitive winner; both PyTorch and TensorFlow are widely used and have their own strengths.
51 | Type your question, end conversation by typing 'finish': finish
52 | 
53 | 
54 | 
55 | 
56 | [2025-07-11T16:35:57.592762]
57 | 
58 | System message:
59 | 
60 | Your input fields are:
61 | 1. `question` (str): 
62 | 2. `history` (History):
63 | Your output fields are:
64 | 1. `answer` (str):
65 | All interactions will be structured in the following way, with the appropriate values filled in.
66 | 
67 | [[ ## question ## ]]
68 | {question}
69 | 
70 | [[ ## history ## ]]
71 | {history}
72 | 
73 | [[ ## answer ## ]]
74 | {answer}
75 | 
76 | [[ ## completed ## ]]
77 | In adhering to this structure, your objective is: 
78 |         Given the fields `question`, `history`, produce the fields `answer`.
79 | 
80 | 
81 | User message:
82 | 
83 | [[ ## question ## ]]
84 | do you know the competition between pytorch and tensorflow?
85 | 
86 | 
87 | Assistant message:
88 | 
89 | [[ ## answer ## ]]
90 | Yes, there is a notable competition between PyTorch and TensorFlow, which are two of the most popular deep learning frameworks. PyTorch, developed by Facebook, is known for its dynamic computation graph, which allows for more flexibility and ease of use, especially in research settings. TensorFlow, developed by Google, initially used a static computation graph but has since introduced eager execution to improve usability. TensorFlow is often favored in production environments due to its scalability and deployment capabilities. Both frameworks have strong communities and extensive libraries, and the choice between them often depends on specific project requirements and personal preference.
91 | 
92 | [[ ## completed ## ]]
93 | 
94 | 
95 | User message:
96 | 
97 | [[ ## question ## ]]
98 | which one won the battle? just tell me the result, don't include any reasoning, thanks!
99 | 
100 | Respond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.
101 | 
102 | 
103 | Response:
104 | 
105 | [[ ## answer ## ]]
106 | There is no definitive winner; both PyTorch and TensorFlow are widely used and have their own strengths.
107 | 
108 | [[ ## completed ## ]]
109 | ```
110 | 
111 | Notice how each user input and assistant response is appended to the history, allowing the model to maintain context across turns.
112 | 
113 | The actual prompt sent to the language model is a multi-turn message, as shown by the output of `dspy.inspect_history`. Each conversation turn is represented as a user message followed by an assistant message.
114 | 
115 | ## History in Few-shot Examples
116 | 
117 | You may notice that `history` does not appear in the input fields section of the prompt, even though it is listed as an input field (e.g., "2. `history` (History):" in the system message). This is intentional: when formatting few-shot examples that include conversation history, DSPy does not expand the history into multiple turns. Instead, to remain compatible with the OpenAI standard format, each few-shot example is represented as a single turn.
118 | 
119 | For example:
120 | 
121 | ```
122 | import dspy
123 | 
124 | dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))
125 | 
126 | 
127 | class QA(dspy.Signature):
128 |     question: str = dspy.InputField()
129 |     history: dspy.History = dspy.InputField()
130 |     answer: str = dspy.OutputField()
131 | 
132 | 
133 | predict = dspy.Predict(QA)
134 | history = dspy.History(messages=[])
135 | 
136 | predict.demos.append(
137 |     dspy.Example(
138 |         question="What is the capital of France?",
139 |         history=dspy.History(
140 |             messages=[{"question": "What is the capital of Germany?", "answer": "The capital of Germany is Berlin."}]
141 |         ),
142 |         answer="The capital of France is Paris.",
143 |     )
144 | )
145 | 
146 | predict(question="What is the capital of America?", history=dspy.History(messages=[]))
147 | dspy.inspect_history()
148 | ```
149 | 
150 | The resulting history will look like this:
151 | 
152 | ```
153 | [2025-07-11T16:53:10.994111]
154 | 
155 | System message:
156 | 
157 | Your input fields are:
158 | 1. `question` (str): 
159 | 2. `history` (History):
160 | Your output fields are:
161 | 1. `answer` (str):
162 | All interactions will be structured in the following way, with the appropriate values filled in.
163 | 
164 | [[ ## question ## ]]
165 | {question}
166 | 
167 | [[ ## history ## ]]
168 | {history}
169 | 
170 | [[ ## answer ## ]]
171 | {answer}
172 | 
173 | [[ ## completed ## ]]
174 | In adhering to this structure, your objective is: 
175 |         Given the fields `question`, `history`, produce the fields `answer`.
176 | 
177 | 
178 | User message:
179 | 
180 | [[ ## question ## ]]
181 | What is the capital of France?
182 | 
183 | [[ ## history ## ]]
184 | {"messages": [{"question": "What is the capital of Germany?", "answer": "The capital of Germany is Berlin."}]}
185 | 
186 | 
187 | Assistant message:
188 | 
189 | [[ ## answer ## ]]
190 | The capital of France is Paris.
191 | 
192 | [[ ## completed ## ]]
193 | 
194 | 
195 | User message:
196 | 
197 | [[ ## question ## ]]
198 | What is the capital of Germany?
199 | 
200 | Respond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.
201 | 
202 | 
203 | Response:
204 | 
205 | [[ ## answer ## ]]
206 | The capital of Germany is Berlin.
207 | 
208 | [[ ## completed ## ]]
209 | ```
210 | 
211 | As you can see, the few-shot example does not expand the conversation history into multiple turns. Instead, it represents the history as JSON data within its section:
212 | 
213 | ```
214 | [[ ## history ## ]]
215 | {"messages": [{"question": "What is the capital of Germany?", "answer": "The capital of Germany is Berlin."}]}
216 | ```
217 | 
218 | This approach ensures compatibility with standard prompt formats while still providing the model with relevant conversational context.
219 | 
```

tutorials/core_development/index.md
```
1 | # Tools, Development, and Deployment
2 | 
3 | This section covers essential DSPy features and best practices for professional AI development. Learn how to implement key functionalities like streaming, caching, deployment, and monitoring in your DSPy applications. These tutorials focus on the practical aspects of building production-ready systems.
4 | 
5 | ## Integration and Tooling
6 | 
7 | ### [Use MCP in DSPy](../mcp/index.md)
8 | 
9 | Learn to integrate Model Context Protocol (MCP) with DSPy applications. This tutorial shows how to leverage MCP for enhanced context management and more sophisticated AI interactions.
10 | 
11 | ### [Output Refinement](../output_refinement/best-of-n-and-refine.md)
12 | 
13 | Master techniques for improving output quality through refinement strategies. Learn how to implement best-of-N sampling and iterative refinement to get higher-quality results from your DSPy programs.
14 | 
15 | ## Data Management and Persistence
16 | 
17 | ### [Saving and Loading](../saving/index.md)
18 | 
19 | Understand how to persist and restore DSPy programs and their optimized states. Learn best practices for model versioning, checkpoint management, and program serialization.
20 | 
21 | ### [Cache](../cache/index.md)
22 | 
23 | Implement efficient caching strategies to improve performance and reduce API costs. Learn how to configure and use DSPy's caching mechanisms effectively in different scenarios.
24 | 
25 | ## Production Deployment
26 | 
27 | ### [Deployment](../deployment/index.md)
28 | 
29 | Learn to deploy DSPy applications in production environments. This tutorial covers multiple deployment strategies such as FastAPI and MLflow.
30 | 
31 | ### [Streaming](../streaming/index.md)
32 | 
33 | Implement real-time streaming capabilities in your DSPy applications. Learn how to handle streaming responses for better user experience in interactive applications.
34 | 
35 | ### [Async](../async/index.md)
36 | 
37 | Build asynchronous DSPy applications for improved performance and scalability. Learn async/await patterns and concurrent execution strategies for high-throughput systems.
38 | 
39 | ## Monitoring and Optimization
40 | 
41 | ### [Debugging & Observability](../observability/index.md)
42 | 
43 | Master debugging and monitoring techniques for DSPy applications. Learn to use comprehensive logging, tracing, and error handling for production systems.
44 | 
45 | ### [Tracking DSPy Optimizers](../optimizer_tracking/index.md)
46 | 
47 | Learn to track and analyze optimizer performance and behavior. Understand how to monitor optimization processes and enhance the reproducibility of the optimization.
```

tutorials/custom_module/index.ipynb
```
1 | {
2 |   "cells": [
3 |     {
4 |       "cell_type": "markdown",
5 |       "metadata": {
6 |         "id": "1cR_pjqz1AsF"
7 |       },
8 |       "source": [
9 |         "# Building AI Applications by Customizing DSPy Modules\n",
10 |         "\n",
11 |         "In this guide, we will walk you through how to build a GenAI application by customizing `dspy.Module`.\n",
12 |         "\n",
13 |         "A [DSPy module](https://dspy.ai/learn/programming/modules/) is the building block for DSPy programs.\n",
14 |         "\n",
15 |         "- Each built-in module abstracts a prompting technique (like chain of thought or ReAct). Crucially, they are generalized to handle any signature.\n",
16 |         "\n",
17 |         "- A DSPy module has learnable parameters (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.\n",
18 |         "\n",
19 |         "- Multiple modules can be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.\n",
20 |         "\n",
21 |         "Although you can build a DSPy program without implementing a custom module, we highly recommend putting your logic with a custom module so that you can use other DSPy features, like DSPy optimizer or MLflow DSPy tracing."
22 |       ]
23 |     },
24 |     {
25 |       "cell_type": "markdown",
26 |       "metadata": {
27 |         "id": "KBYjBQtv3Cn5"
28 |       },
29 |       "source": [
30 |         "Before getting started, make sure you have DSPy installed:\n",
31 |         "\n",
32 |         "```\n",
33 |         "!pip install dspy\n",
34 |         "```"
35 |       ]
36 |     },
37 |     {
38 |       "cell_type": "markdown",
39 |       "metadata": {},
40 |       "source": [
41 |         "<details>\n",
42 |         "<summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>\n",
43 |         "\n",
44 |         "### MLflow DSPy Integration\n",
45 |         "\n",
46 |         "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
47 |         "\n",
48 |         "![MLflow Trace](./mlflow-tracing-custom-module.png)\n",
49 |         "\n",
50 |         "1. Install MLflow\n",
51 |         "\n",
52 |         "```bash\n",
53 |         "%pip install mlflow>=3.0.0\n",
54 |         "```\n",
55 |         "\n",
56 |         "2. Start MLflow UI in a separate terminal\n",
57 |         "```bash\n",
58 |         "mlflow ui --port 5000 --backend-store-uri sqlite:///mlruns.db\n",
59 |         "```\n",
60 |         "\n",
61 |         "3. Connect the notebook to MLflow\n",
62 |         "```python\n",
63 |         "import mlflow\n",
64 |         "\n",
65 |         "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
66 |         "mlflow.set_experiment(\"DSPy\")\n",
67 |         "```\n",
68 |         "\n",
69 |         "4. Enabling tracing.\n",
70 |         "```python\n",
71 |         "mlflow.dspy.autolog()\n",
72 |         "```\n",
73 |         "\n",
74 |         "\n",
75 |         "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
76 |         "</details>"
77 |       ]
78 |     },
79 |     {
80 |       "cell_type": "markdown",
81 |       "metadata": {
82 |         "id": "reQSTM8a8qMf"
83 |       },
84 |       "source": [
85 |         "## Customize DSPy Module\n",
86 |         "\n",
87 |         "You can implement custom prompting logic and integrate external tools or services by customizing a DSPy module. To achieve this, subclass from `dspy.Module` and implement the following two key methods:\n",
88 |         "\n",
89 |         "- `__init__`: This is the constructor, where you define the attributes and sub-modules of your program.\n",
90 |         "- `forward`: This method contains the core logic of your DSPy program.\n",
91 |         "\n",
92 |         "Within the `forward()` method, you are not limited to calling only other DSPy modules; you can also integrate any standard Python functions, such as those for interacting with Langchain/Agno agents, MCP tools, database handlers, and more.\n",
93 |         "\n",
94 |         "The basic structure for a custom DSPy module looks like this:\n",
95 |         "\n",
96 |         "```python\n",
97 |         "class MyProgram(dspy.Module):\n",
98 |         "    \n",
99 |         "    def __init__(self, ...):\n",
100 |         "        # Define attributes and sub-modules here\n",
101 |         "        {constructor_code}\n",
102 |         "\n",
103 |         "    def forward(self, input_name1, input_name2, ...):\n",
104 |         "        # Implement your program's logic here\n",
105 |         "        {custom_logic_code}\n",
106 |         "```"
107 |       ]
108 |     },
109 |     {
110 |       "cell_type": "markdown",
111 |       "metadata": {
112 |         "id": "DziTWwT8_TrY"
113 |       },
114 |       "source": [
115 |         "Let's illustrate this with a practical code example. We will build a simple Retrieval-Augmented Generation (RAG) application with multiple stages:\n",
116 |         "\n",
117 |         "1.  **Query Generation:** Generate a suitable query based on the user's question to retrieve relevant context.\n",
118 |         "2.  **Context Retrieval:** Fetch context using the generated query.\n",
119 |         "3.  **Answer Generation:** Produce a final answer based on the retrieved context and the original question.\n",
120 |         "\n",
121 |         "The code implementation for this multi-stage program is shown below."
122 |       ]
123 |     },
124 |     {
125 |       "cell_type": "code",
126 |       "execution_count": 3,
127 |       "metadata": {
128 |         "id": "lAoV5_v7YlvN"
129 |       },
130 |       "outputs": [],
131 |       "source": [
132 |         "import dspy\n",
133 |         "\n",
134 |         "class QueryGenerator(dspy.Signature):\n",
135 |         "    \"\"\"Generate a query based on question to fetch relevant context\"\"\"\n",
136 |         "    question: str = dspy.InputField()\n",
137 |         "    query: str = dspy.OutputField()\n",
138 |         "\n",
139 |         "def search_wikipedia(query: str) -> list[str]:\n",
140 |         "    \"\"\"Query ColBERT endpoint, which is a knowledge source based on wikipedia data\"\"\"\n",
141 |         "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=1)\n",
142 |         "    return [x[\"text\"] for x in results]\n",
143 |         "\n",
144 |         "class RAG(dspy.Module):\n",
145 |         "    def __init__(self):\n",
146 |         "        self.query_generator = dspy.Predict(QueryGenerator)\n",
147 |         "        self.answer_generator = dspy.ChainOfThought(\"question,context->answer\")\n",
148 |         "\n",
149 |         "    def forward(self, question, **kwargs):\n",
150 |         "        query = self.query_generator(question=question).query\n",
151 |         "        context = search_wikipedia(query)[0]\n",
152 |         "        return self.answer_generator(question=question, context=context).answer"
153 |       ]
154 |     },
155 |     {
156 |       "cell_type": "markdown",
157 |       "metadata": {},
158 |       "source": [
159 |         "Let's take a look at the `forward` method. We first send the question to `self.query_generator`, which is a `dspy.Predict`, to get the query for context retrieving. Then we use the query to call ColBERT and keep the first context retrieved. Finally, we send the question and context into `self.answer_generator`, which is a `dspy.ChainOfThought` to generate the final answer."
160 |       ]
161 |     },
162 |     {
163 |       "cell_type": "markdown",
164 |       "metadata": {
165 |         "id": "FBq_4e8NamwY"
166 |       },
167 |       "source": [
168 |         "Next, we'll create an instance of our `RAG` module to run the program.\n",
169 |         "\n",
170 |         "**Important:** When invoking a custom DSPy module, you should use the module instance directly (which calls the `__call__` method internally), rather than calling the `forward()` method explicitly. The `__call__` method handles necessary internal processing before executing the `forward` logic."
171 |       ]
172 |     },
173 |     {
174 |       "cell_type": "code",
175 |       "execution_count": 7,
176 |       "metadata": {
177 |         "colab": {
178 |           "base_uri": "https://localhost:8080/"
179 |         },
180 |         "id": "ZR7xcFSTa596",
181 |         "outputId": "f3427754-8a16-48fe-c540-8c9f31d9a30d"
182 |       },
183 |       "outputs": [
184 |         {
185 |           "name": "stdout",
186 |           "output_type": "stream",
187 |           "text": [
188 |             "The question of whether LeBron James is the basketball GOAT is subjective and depends on personal opinions. Many consider him one of the greatest due to his achievements and impact on the game, but others may argue for different players like Michael Jordan.\n"
189 |           ]
190 |         }
191 |       ],
192 |       "source": [
193 |         "import os\n",
194 |         "\n",
195 |         "os.environ[\"OPENAI_API_KEY\"] = \"{your_openai_api_key}\"\n",
196 |         "\n",
197 |         "dspy.configure(lm=dspy.LM(\"openai/gpt-4o-mini\"))\n",
198 |         "rag = RAG()\n",
199 |         "print(rag(question=\"Is Lebron James the basketball GOAT?\"))"
200 |       ]
201 |     },
202 |     {
203 |       "cell_type": "markdown",
204 |       "metadata": {},
205 |       "source": [
206 |         "That's it! In summary, to build your GenAI applications, we just put the custom logic into the `forward()` method, then create a module instance and call the instance itself."
207 |       ]
208 |     },
209 |     {
210 |       "cell_type": "markdown",
211 |       "metadata": {
212 |         "id": "aYAYc-Hg39ri"
213 |       },
214 |       "source": [
215 |         "## Why Customizing Module?\n",
216 |         "\n",
217 |         "DSPy is a lightweight authoring and optimization framework, and our focus is to resolve the mess of prompt engineering by transforming prompting (string in, string out) LLM into programming LLM (structured inputs in, structured outputs out) for robust AI system.\n",
218 |         "\n",
219 |         "While we provide pre-built modules which have custom prompting logic like `dspy.ChainOfThought` for reasoning, `dspy.ReAct` for tool calling agent to facilitate building your AI applications, we don't aim at standardizing how you build agents.\n",
220 |         "\n",
221 |         "In DSPy, your application logic simply goes to the `forward` method of your custom Module, which doesn't have any constraint as long as you are writing python code. With this layout, DSPy is easy to migrate to from other frameworks or vanilla SDK usage, and easy to migrate off because essentially it's just python code.\n"
222 |       ]
223 |     }
224 |   ],
225 |   "metadata": {
226 |     "colab": {
227 |       "provenance": []
228 |     },
229 |     "kernelspec": {
230 |       "display_name": "Python 3",
231 |       "name": "python3"
232 |     },
233 |     "language_info": {
234 |       "name": "python"
235 |     }
236 |   },
237 |   "nbformat": 4,
238 |   "nbformat_minor": 0
239 | }
```

tutorials/customer_service_agent/index.ipynb
```
1 | {
2 |   "cells": [
3 |     {
4 |       "cell_type": "markdown",
5 |       "metadata": {
6 |         "id": "XicO2XPbEBVD"
7 |       },
8 |       "source": [
9 |         "# Build AI Agents with DSPy\n",
10 |         "\n",
11 |         "In this tutorial, we will walk you through how to build an AI agents with DSPy. AI agents refer to the system that can autonomously perceive its environment, make decisions, and take actions to achieve specific goals. Unlike a single model prompt, an agent typically follows a loop of reasoning, planning, and acting, often integrating tools like search engines, APIs, or memory to complete complex tasks.\n",
12 |         "\n",
13 |         "This tutorial focuses on a popular architecture of AI agents called ReAct, standing for **Re**asoning and **Act**ing, which provides a task description along with a list of tools to LM, then lets LM decide whether to call tools for more obseravations, or generate the final output.\n",
14 |         "\n",
15 |         "As the demo, let's build a simple airline customer service agent that can do the following:\n",
16 |         "\n",
17 |         "- Book new trips on behalf of the user.\n",
18 |         "- Modify existing trips, including flight change and cancellation.\n",
19 |         "- On tasks it cannot handle, raise a customer support ticket.\n",
20 |         "\n",
21 |         "We will build it from `dspy.ReAct` module.\n"
22 |       ]
23 |     },
24 |     {
25 |       "cell_type": "markdown",
26 |       "metadata": {
27 |         "id": "8dYvGkoWG3C7"
28 |       },
29 |       "source": [
30 |         "## Install Dependencies\n",
31 |         "\n",
32 |         "Before starting, let's install the required packages:\n",
33 |         "\n",
34 |         "```\n",
35 |         "!pip install -qU dspy pydantic\n",
36 |         "```"
37 |       ]
38 |     },
39 |     {
40 |       "cell_type": "markdown",
41 |       "metadata": {},
42 |       "source": [
43 |         "<details>\n",
44 |         "<summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>\n",
45 |         "\n",
46 |         "### MLflow DSPy Integration\n",
47 |         "\n",
48 |         "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
49 |         "\n",
50 |         "![MLflow Trace](./mlflow-tracing-customer-service-agent.png)\n",
51 |         "\n",
52 |         "1. Install MLflow\n",
53 |         "\n",
54 |         "```bash\n",
55 |         "%pip install mlflow>=3.0.0\n",
56 |         "```\n",
57 |         "\n",
58 |         "2. Start MLflow UI in a separate terminal\n",
59 |         "```bash\n",
60 |         "mlflow ui --port 5000 --backend-store-uri sqlite:///mlruns.db\n",
61 |         "```\n",
62 |         "\n",
63 |         "3. Connect the notebook to MLflow\n",
64 |         "```python\n",
65 |         "import mlflow\n",
66 |         "\n",
67 |         "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
68 |         "mlflow.set_experiment(\"DSPy\")\n",
69 |         "```\n",
70 |         "\n",
71 |         "4. Enabling tracing.\n",
72 |         "```python\n",
73 |         "mlflow.dspy.autolog()\n",
74 |         "```\n",
75 |         "\n",
76 |         "\n",
77 |         "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
78 |         "</details>"
79 |       ]
80 |     },
81 |     {
82 |       "cell_type": "markdown",
83 |       "metadata": {
84 |         "id": "W_rhpcE2HAMO"
85 |       },
86 |       "source": [
87 |         "## Define Tools\n",
88 |         "\n",
89 |         "We need to prepare a list of tools so that the agent can behave like a human airline service agent:\n",
90 |         "\n",
91 |         "- `fetch_flight_info`: get flight information for certain dates.\n",
92 |         "- `pick_flight`: pick the best flight based on some criteria.\n",
93 |         "- `book_flight`: book a flight on behalf of the user.\n",
94 |         "- `fetch_itinerary`: get the information of a booked itinerary.\n",
95 |         "- `cancel_itinerary`: cancel a booked itinerary.\n",
96 |         "- `get_user_info`: get users' information.\n",
97 |         "- `file_ticket`: file a backlog ticket to have human assist.\n"
98 |       ]
99 |     },
100 |     {
101 |       "cell_type": "markdown",
102 |       "metadata": {
103 |         "id": "Zr0r8FVgI8vQ"
104 |       },
105 |       "source": [
106 |         "### Define Data Structure\n",
107 |         "\n",
108 |         "Before defining the tools, we need to define the data structure. In real production, this will be the database schema. As a demo, we just define the data structure as [pydantic models](https://docs.pydantic.dev/latest/concepts/models/) for simplicity."
109 |       ]
110 |     },
111 |     {
112 |       "cell_type": "code",
113 |       "execution_count": 1,
114 |       "metadata": {
115 |         "id": "nKjBVt3MJQR-"
116 |       },
117 |       "outputs": [],
118 |       "source": [
119 |         "from pydantic import BaseModel\n",
120 |         "\n",
121 |         "class Date(BaseModel):\n",
122 |         "    # Somehow LLM is bad at specifying `datetime.datetime`, so\n",
123 |         "    # we define a custom class to represent the date.\n",
124 |         "    year: int\n",
125 |         "    month: int\n",
126 |         "    day: int\n",
127 |         "    hour: int\n",
128 |         "\n",
129 |         "class UserProfile(BaseModel):\n",
130 |         "    user_id: str\n",
131 |         "    name: str\n",
132 |         "    email: str\n",
133 |         "\n",
134 |         "class Flight(BaseModel):\n",
135 |         "    flight_id: str\n",
136 |         "    date_time: Date\n",
137 |         "    origin: str\n",
138 |         "    destination: str\n",
139 |         "    duration: float\n",
140 |         "    price: float\n",
141 |         "\n",
142 |         "class Itinerary(BaseModel):\n",
143 |         "    confirmation_number: str\n",
144 |         "    user_profile: UserProfile\n",
145 |         "    flight: Flight\n",
146 |         "\n",
147 |         "class Ticket(BaseModel):\n",
148 |         "    user_request: str\n",
149 |         "    user_profile: UserProfile"
150 |       ]
151 |     },
152 |     {
153 |       "cell_type": "markdown",
154 |       "metadata": {
155 |         "id": "JrZkTeI4JbcW"
156 |       },
157 |       "source": [
158 |         "### Create Dummy Data\n",
159 |         "\n",
160 |         "Let's also create some dummy data so that the airline agent can do the work. We need to create a few flights and a few users, and initialize empty dictionaries for the itineraries and custom support tickets."
161 |       ]
162 |     },
163 |     {
164 |       "cell_type": "code",
165 |       "execution_count": 2,
166 |       "metadata": {
167 |         "id": "FJDzsHjKJwWb"
168 |       },
169 |       "outputs": [],
170 |       "source": [
171 |         "user_database = {\n",
172 |         "    \"Adam\": UserProfile(user_id=\"1\", name=\"Adam\", email=\"adam@gmail.com\"),\n",
173 |         "    \"Bob\": UserProfile(user_id=\"2\", name=\"Bob\", email=\"bob@gmail.com\"),\n",
174 |         "    \"Chelsie\": UserProfile(user_id=\"3\", name=\"Chelsie\", email=\"chelsie@gmail.com\"),\n",
175 |         "    \"David\": UserProfile(user_id=\"4\", name=\"David\", email=\"david@gmail.com\"),\n",
176 |         "}\n",
177 |         "\n",
178 |         "flight_database = {\n",
179 |         "    \"DA123\": Flight(\n",
180 |         "        flight_id=\"DA123\",  # DSPy Airline 123\n",
181 |         "        origin=\"SFO\",\n",
182 |         "        destination=\"JFK\",\n",
183 |         "        date_time=Date(year=2025, month=9, day=1, hour=1),\n",
184 |         "        duration=3,\n",
185 |         "        price=200,\n",
186 |         "    ),\n",
187 |         "    \"DA125\": Flight(\n",
188 |         "        flight_id=\"DA125\",\n",
189 |         "        origin=\"SFO\",\n",
190 |         "        destination=\"JFK\",\n",
191 |         "        date_time=Date(year=2025, month=9, day=1, hour=7),\n",
192 |         "        duration=9,\n",
193 |         "        price=500,\n",
194 |         "    ),\n",
195 |         "    \"DA456\": Flight(\n",
196 |         "        flight_id=\"DA456\",\n",
197 |         "        origin=\"SFO\",\n",
198 |         "        destination=\"SNA\",\n",
199 |         "        date_time=Date(year=2025, month=10, day=1, hour=1),\n",
200 |         "        duration=2,\n",
201 |         "        price=100,\n",
202 |         "    ),\n",
203 |         "    \"DA460\": Flight(\n",
204 |         "        flight_id=\"DA460\",\n",
205 |         "        origin=\"SFO\",\n",
206 |         "        destination=\"SNA\",\n",
207 |         "        date_time=Date(year=2025, month=10, day=1, hour=9),\n",
208 |         "        duration=2,\n",
209 |         "        price=120,\n",
210 |         "    ),\n",
211 |         "}\n",
212 |         "\n",
213 |         "itinery_database = {}\n",
214 |         "ticket_database = {}"
215 |       ]
216 |     },
217 |     {
218 |       "cell_type": "markdown",
219 |       "metadata": {
220 |         "id": "-iGVa5WiKFp_"
221 |       },
222 |       "source": [
223 |         "### Define the Tools\n",
224 |         "\n",
225 |         "Now we can define the tools. In order to have `dspy.ReAct` function properly, every function should:\n",
226 |         "\n",
227 |         "- Have a docstring which defines what the tool does. If the function name is self-explanable, then you can leave the docstring empty.\n",
228 |         "- Have type hint for the arguments, which is necessary for LM to generate the arguments in the right format."
229 |       ]
230 |     },
231 |     {
232 |       "cell_type": "code",
233 |       "execution_count": 3,
234 |       "metadata": {
235 |         "id": "-wQGzf6qK7P5"
236 |       },
237 |       "outputs": [],
238 |       "source": [
239 |         "import random\n",
240 |         "import string\n",
241 |         "\n",
242 |         "\n",
243 |         "def fetch_flight_info(date: Date, origin: str, destination: str):\n",
244 |         "    \"\"\"Fetch flight information from origin to destination on the given date\"\"\"\n",
245 |         "    flights = []\n",
246 |         "\n",
247 |         "    for flight_id, flight in flight_database.items():\n",
248 |         "        if (\n",
249 |         "            flight.date_time.year == date.year\n",
250 |         "            and flight.date_time.month == date.month\n",
251 |         "            and flight.date_time.day == date.day\n",
252 |         "            and flight.origin == origin\n",
253 |         "            and flight.destination == destination\n",
254 |         "        ):\n",
255 |         "            flights.append(flight)\n",
256 |         "    if len(flights) == 0:\n",
257 |         "        raise ValueError(\"No matching flight found!\")\n",
258 |         "    return flights\n",
259 |         "\n",
260 |         "\n",
261 |         "def fetch_itinerary(confirmation_number: str):\n",
262 |         "    \"\"\"Fetch a booked itinerary information from database\"\"\"\n",
263 |         "    return itinery_database.get(confirmation_number)\n",
264 |         "\n",
265 |         "\n",
266 |         "def pick_flight(flights: list[Flight]):\n",
267 |         "    \"\"\"Pick up the best flight that matches users' request. we pick the shortest, and cheaper one on ties.\"\"\"\n",
268 |         "    sorted_flights = sorted(\n",
269 |         "        flights,\n",
270 |         "        key=lambda x: (\n",
271 |         "            x.get(\"duration\") if isinstance(x, dict) else x.duration,\n",
272 |         "            x.get(\"price\") if isinstance(x, dict) else x.price,\n",
273 |         "        ),\n",
274 |         "    )\n",
275 |         "    return sorted_flights[0]\n",
276 |         "\n",
277 |         "\n",
278 |         "def _generate_id(length=8):\n",
279 |         "    chars = string.ascii_lowercase + string.digits\n",
280 |         "    return \"\".join(random.choices(chars, k=length))\n",
281 |         "\n",
282 |         "\n",
283 |         "def book_flight(flight: Flight, user_profile: UserProfile):\n",
284 |         "    \"\"\"Book a flight on behalf of the user.\"\"\"\n",
285 |         "    confirmation_number = _generate_id()\n",
286 |         "    while confirmation_number in itinery_database:\n",
287 |         "        confirmation_number = _generate_id()\n",
288 |         "    itinery_database[confirmation_number] = Itinerary(\n",
289 |         "        confirmation_number=confirmation_number,\n",
290 |         "        user_profile=user_profile,\n",
291 |         "        flight=flight,\n",
292 |         "    )\n",
293 |         "    return confirmation_number, itinery_database[confirmation_number]\n",
294 |         "\n",
295 |         "\n",
296 |         "def cancel_itinerary(confirmation_number: str, user_profile: UserProfile):\n",
297 |         "    \"\"\"Cancel an itinerary on behalf of the user.\"\"\"\n",
298 |         "    if confirmation_number in itinery_database:\n",
299 |         "        del itinery_database[confirmation_number]\n",
300 |         "        return\n",
301 |         "    raise ValueError(\"Cannot find the itinerary, please check your confirmation number.\")\n",
302 |         "\n",
303 |         "\n",
304 |         "def get_user_info(name: str):\n",
305 |         "    \"\"\"Fetch the user profile from database with given name.\"\"\"\n",
306 |         "    return user_database.get(name)\n",
307 |         "\n",
308 |         "\n",
309 |         "def file_ticket(user_request: str, user_profile: UserProfile):\n",
310 |         "    \"\"\"File a customer support ticket if this is something the agent cannot handle.\"\"\"\n",
311 |         "    ticket_id = _generate_id(length=6)\n",
312 |         "    ticket_database[ticket_id] = Ticket(\n",
313 |         "        user_request=user_request,\n",
314 |         "        user_profile=user_profile,\n",
315 |         "    )\n",
316 |         "    return ticket_id\n"
317 |       ]
318 |     },
319 |     {
320 |       "cell_type": "markdown",
321 |       "metadata": {
322 |         "id": "rzibYLS0LqXc"
323 |       },
324 |       "source": [
325 |         "### Create ReAct Agent\n",
326 |         "\n",
327 |         "Now we can create the ReAct agent via `dspy.ReAct`. We need to provide a signature to `dspy.ReAct` to define task, and the inputs and outputs of the agent, and tell it about the tools it can access."
328 |       ]
329 |     },
330 |     {
331 |       "cell_type": "code",
332 |       "execution_count": 4,
333 |       "metadata": {
334 |         "id": "OZ2qOWLGLk2J"
335 |       },
336 |       "outputs": [],
337 |       "source": [
338 |         "import dspy\n",
339 |         "\n",
340 |         "class DSPyAirlineCustomerService(dspy.Signature):\n",
341 |         "    \"\"\"You are an airline customer service agent that helps user book and manage flights.\n",
342 |         "\n",
343 |         "    You are given a list of tools to handle user request, and you should decide the right tool to use in order to\n",
344 |         "    fulfill users' request.\"\"\"\n",
345 |         "\n",
346 |         "    user_request: str = dspy.InputField()\n",
347 |         "    process_result: str = dspy.OutputField(\n",
348 |         "        desc=(\n",
349 |         "                \"Message that summarizes the process result, and the information users need, e.g., the \"\n",
350 |         "                \"confirmation_number if a new flight is booked.\"\n",
351 |         "            )\n",
352 |         "        )"
353 |       ]
354 |     },
355 |     {
356 |       "cell_type": "code",
357 |       "execution_count": 5,
358 |       "metadata": {
359 |         "id": "ooDb84LIM5Pn"
360 |       },
361 |       "outputs": [],
362 |       "source": [
363 |         "agent = dspy.ReAct(\n",
364 |         "    DSPyAirlineCustomerService,\n",
365 |         "    tools = [\n",
366 |         "        fetch_flight_info,\n",
367 |         "        fetch_itinerary,\n",
368 |         "        pick_flight,\n",
369 |         "        book_flight,\n",
370 |         "        cancel_itinerary,\n",
371 |         "        get_user_info,\n",
372 |         "        file_ticket,\n",
373 |         "    ]\n",
374 |         ")"
375 |       ]
376 |     },
377 |     {
378 |       "cell_type": "markdown",
379 |       "metadata": {
380 |         "id": "4NA2QtQgNyrO"
381 |       },
382 |       "source": [
383 |         "## Use the Agent\n",
384 |         "\n",
385 |         "To interact with the agent, simply provide the request through `user_request`, and the agent will start doing its job."
386 |       ]
387 |     },
388 |     {
389 |       "cell_type": "markdown",
390 |       "metadata": {
391 |         "id": "CXnSE6P1OHbm"
392 |       },
393 |       "source": [
394 |         "Select a language model and set up the API keys. We are using gpt-4o-mini here, but you can change to other models. For how to configure the language model, please refer to [this guide](https://dspy.ai/learn/programming/language_models/)."
395 |       ]
396 |     },
397 |     {
398 |       "cell_type": "code",
399 |       "execution_count": 6,
400 |       "metadata": {
401 |         "id": "5F3TVB1DOW82"
402 |       },
403 |       "outputs": [],
404 |       "source": [
405 |         "import os\n",
406 |         "\n",
407 |         "os.environ[\"OPENAI_API_KEY\"] = \"{your openai key}\"\n",
408 |         "\n",
409 |         "dspy.configure(lm=dspy.LM(\"openai/gpt-4o-mini\"))"
410 |       ]
411 |     },
412 |     {
413 |       "cell_type": "code",
414 |       "execution_count": 7,
415 |       "metadata": {
416 |         "colab": {
417 |           "base_uri": "https://localhost:8080/"
418 |         },
419 |         "id": "hL3q0iS4Nwn8",
420 |         "outputId": "460af1b7-bceb-4edc-8d51-30f87eb7d454"
421 |       },
422 |       "outputs": [
423 |         {
424 |           "name": "stdout",
425 |           "output_type": "stream",
426 |           "text": [
427 |             "Prediction(\n",
428 |             "    trajectory={'thought_0': 'I need to fetch flight information for the route from SFO to JFK on the specified date of 09/01/2025. Once I have the flight options, I can help Adam pick the best flight and proceed with the booking.', 'tool_name_0': 'fetch_flight_info', 'tool_args_0': {'date': {'year': 2025, 'month': 9, 'day': 1, 'hour': 0}, 'origin': 'SFO', 'destination': 'JFK'}, 'observation_0': [Flight(flight_id='DA123', date_time=Date(year=2025, month=9, day=1, hour=1), origin='SFO', destination='JFK', duration=3.0, price=200.0), Flight(flight_id='DA125', date_time=Date(year=2025, month=9, day=1, hour=7), origin='SFO', destination='JFK', duration=9.0, price=500.0)], 'thought_1': 'I have received the flight options from SFO to JFK on 09/01/2025. There are two flights available: DA123 at 1 AM for $200 and DA125 at 7 AM for $500. I will now pick the best flight, which is the cheaper option, DA123.', 'tool_name_1': 'pick_flight', 'tool_args_1': {'flights': [{'flight_id': 'DA123', 'date_time': {'year': 2025, 'month': 9, 'day': 1, 'hour': 1}, 'origin': 'SFO', 'destination': 'JFK', 'duration': 3.0, 'price': 200.0}, {'flight_id': 'DA125', 'date_time': {'year': 2025, 'month': 9, 'day': 1, 'hour': 7}, 'origin': 'SFO', 'destination': 'JFK', 'duration': 9.0, 'price': 500.0}]}, 'observation_1': Flight(flight_id='DA123', date_time=Date(year=2025, month=9, day=1, hour=1), origin='SFO', destination='JFK', duration=3.0, price=200.0), 'thought_2': \"I have successfully picked the best flight, which is DA123. Now, I need to gather Adam's user profile information to proceed with the booking.\", 'tool_name_2': 'get_user_info', 'tool_args_2': {'name': 'Adam'}, 'observation_2': UserProfile(user_id='1', name='Adam', email='adam@gmail.com'), 'thought_3': \"I have gathered Adam's user profile information. Now I can proceed to book the selected flight DA123 for him.\", 'tool_name_3': 'book_flight', 'tool_args_3': {'flight': {'flight_id': 'DA123', 'date_time': {'year': 2025, 'month': 9, 'day': 1, 'hour': 1}, 'origin': 'SFO', 'destination': 'JFK', 'duration': 3.0, 'price': 200.0}, 'user_profile': {'user_id': '1', 'name': 'Adam', 'email': 'adam@gmail.com'}}, 'observation_3': ('94ldyhsx', Itinerary(confirmation_number='94ldyhsx', user_profile=UserProfile(user_id='1', name='Adam', email='adam@gmail.com'), flight=Flight(flight_id='DA123', date_time=Date(year=2025, month=9, day=1, hour=1), origin='SFO', destination='JFK', duration=3.0, price=200.0))), 'thought_4': \"Adam's flight has been successfully booked, and I have received the confirmation number. I will now mark the task as complete.\", 'tool_name_4': 'finish', 'tool_args_4': {}, 'observation_4': 'Completed.'},\n",
429 |             "    reasoning=\"I successfully fetched flight information for the route from SFO to JFK on 09/01/2025. There were two available flights: DA123 at 1 AM for $200 and DA125 at 7 AM for $500. I selected the cheaper option, DA123, and then retrieved Adam's user profile information to proceed with the booking. After booking the flight, I received a confirmation number for the reservation.\",\n",
430 |             "    process_result='Your flight from SFO to JFK on 09/01/2025 has been successfully booked. Your confirmation number is 94ldyhsx.'\n",
431 |             ")\n"
432 |           ]
433 |         }
434 |       ],
435 |       "source": [
436 |         "result = agent(user_request=\"please help me book a flight from SFO to JFK on 09/01/2025, my name is Adam\")\n",
437 |         "print(result)"
438 |       ]
439 |     },
440 |     {
441 |       "cell_type": "markdown",
442 |       "metadata": {
443 |         "id": "5kwbgMNUWQUX"
444 |       },
445 |       "source": [
446 |         "We can see the booked itinerarie in the database."
447 |       ]
448 |     },
449 |     {
450 |       "cell_type": "code",
451 |       "execution_count": 8,
452 |       "metadata": {
453 |         "colab": {
454 |           "base_uri": "https://localhost:8080/"
455 |         },
456 |         "id": "J3fO7G3yVtoa",
457 |         "outputId": "76f665ca-81a4-45df-bf08-d66b946a45c7"
458 |       },
459 |       "outputs": [
460 |         {
461 |           "name": "stdout",
462 |           "output_type": "stream",
463 |           "text": [
464 |             "{'94ldyhsx': Itinerary(confirmation_number='94ldyhsx', user_profile=UserProfile(user_id='1', name='Adam', email='adam@gmail.com'), flight=Flight(flight_id='DA123', date_time=Date(year=2025, month=9, day=1, hour=1), origin='SFO', destination='JFK', duration=3.0, price=200.0))}\n"
465 |           ]
466 |         }
467 |       ],
468 |       "source": [
469 |         "print(itinery_database)"
470 |       ]
471 |     },
472 |     {
473 |       "cell_type": "markdown",
474 |       "metadata": {
475 |         "id": "mxFF1tMrWU_7"
476 |       },
477 |       "source": [
478 |         "### Interpret the Result\n",
479 |         "\n",
480 |         "The result contains the the `process_result` as required by the user, and a `reasoning` field that carries the reasoning behind the answer. In addition, it has a `trajectory` field which contains:\n",
481 |         "\n",
482 |         "- Reasoning (thought) at each step\n",
483 |         "- Tools picked by LM at each step\n",
484 |         "- Arguments for tool calling, determined by LM at each step\n",
485 |         "- Tool execution results at each step\n",
486 |         "\n",
487 |         "Behind scene, the `dspy.ReAct` is executing a loop, which accumulates tool call information along with the task description, and send to the LM until hits `max_iters` or the LM decides to wrap up. To better interpret the process, let's use `dspy.inspect_history()` to see what's happening inside each step.\n"
488 |       ]
489 |     },
490 |     {
491 |       "cell_type": "code",
492 |       "execution_count": 9,
493 |       "metadata": {
494 |         "colab": {
495 |           "base_uri": "https://localhost:8080/"
496 |         },
497 |         "id": "Pm7lZhLZV_ko",
498 |         "outputId": "e4387b2d-7368-4aea-8705-1127c9c8fd31"
499 |       },
500 |       "outputs": [
501 |         {
502 |           "name": "stdout",
503 |           "output_type": "stream",
504 |           "text": [
505 |             "\n",
506 |             "\n",
507 |             "\n",
508 |             "\n",
509 |             "\u001b[34m[2025-05-28T01:06:46.819048]\u001b[0m\n",
510 |             "\n",
511 |             "\u001b[31mSystem message:\u001b[0m\n",
512 |             "\n",
513 |             "Your input fields are:\n",
514 |             "1. `user_request` (str)\n",
515 |             "2. `trajectory` (str)\n",
516 |             "Your output fields are:\n",
517 |             "1. `next_thought` (str)\n",
518 |             "2. `next_tool_name` (Literal['fetch_flight_info', 'fetch_itinerary', 'pick_flight', 'book_flight', 'cancel_itinerary', 'get_user_info', 'file_ticket', 'finish'])\n",
519 |             "3. `next_tool_args` (dict[str, Any])\n",
520 |             "All interactions will be structured in the following way, with the appropriate values filled in.\n",
521 |             "\n",
522 |             "[[ ## user_request ## ]]\n",
523 |             "{user_request}\n",
524 |             "\n",
525 |             "[[ ## trajectory ## ]]\n",
526 |             "{trajectory}\n",
527 |             "\n",
528 |             "[[ ## next_thought ## ]]\n",
529 |             "{next_thought}\n",
530 |             "\n",
531 |             "[[ ## next_tool_name ## ]]\n",
532 |             "{next_tool_name}        # note: the value you produce must exactly match (no extra characters) one of: fetch_flight_info; fetch_itinerary; pick_flight; book_flight; cancel_itinerary; get_user_info; file_ticket; finish\n",
533 |             "\n",
534 |             "[[ ## next_tool_args ## ]]\n",
535 |             "{next_tool_args}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"object\", \"additionalProperties\": true}\n",
536 |             "\n",
537 |             "[[ ## completed ## ]]\n",
538 |             "In adhering to this structure, your objective is: \n",
539 |             "        You are an airline customer service agent that helps user book and manage flights. \n",
540 |             "        \n",
541 |             "        You are given a list of tools to handle user request, and you should decide the right tool to use in order to\n",
542 |             "        fulfill users' request.\n",
543 |             "        \n",
544 |             "        You are an Agent. In each episode, you will be given the fields `user_request` as input. And you can see your past trajectory so far.\n",
545 |             "        Your goal is to use one or more of the supplied tools to collect any necessary information for producing `process_result`.\n",
546 |             "        \n",
547 |             "        To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\n",
548 |             "        After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n",
549 |             "        \n",
550 |             "        When writing next_thought, you may reason about the current situation and plan for future steps.\n",
551 |             "        When selecting the next_tool_name and its next_tool_args, the tool must be one of:\n",
552 |             "        \n",
553 |             "        (1) fetch_flight_info, whose description is <desc>Fetch flight information from origin to destination on the given date</desc>. It takes arguments {'date': {'properties': {'year': {'title': 'Year', 'type': 'integer'}, 'month': {'title': 'Month', 'type': 'integer'}, 'day': {'title': 'Day', 'type': 'integer'}, 'hour': {'title': 'Hour', 'type': 'integer'}}, 'required': ['year', 'month', 'day', 'hour'], 'title': 'Date', 'type': 'object'}, 'origin': {'type': 'string'}, 'destination': {'type': 'string'}} in JSON format.\n",
554 |             "        (2) fetch_itinerary, whose description is <desc>Fetch a booked itinerary information from database</desc>. It takes arguments {'confirmation_number': {'type': 'string'}} in JSON format.\n",
555 |             "        (3) pick_flight, whose description is <desc>Pick up the best flight that matches users' request. we pick the shortest, and cheaper one on ties.</desc>. It takes arguments {'flights': {'$defs': {'Date': {'properties': {'year': {'title': 'Year', 'type': 'integer'}, 'month': {'title': 'Month', 'type': 'integer'}, 'day': {'title': 'Day', 'type': 'integer'}, 'hour': {'title': 'Hour', 'type': 'integer'}}, 'required': ['year', 'month', 'day', 'hour'], 'title': 'Date', 'type': 'object'}, 'Flight': {'properties': {'flight_id': {'title': 'Flight Id', 'type': 'string'}, 'date_time': {'$ref': '#/$defs/Date'}, 'origin': {'title': 'Origin', 'type': 'string'}, 'destination': {'title': 'Destination', 'type': 'string'}, 'duration': {'title': 'Duration', 'type': 'number'}, 'price': {'title': 'Price', 'type': 'number'}}, 'required': ['flight_id', 'date_time', 'origin', 'destination', 'duration', 'price'], 'title': 'Flight', 'type': 'object'}}, 'items': {'$ref': '#/$defs/Flight'}, 'type': 'array'}} in JSON format.\n",
556 |             "        (4) book_flight, whose description is <desc>Book a flight on behalf of the user.</desc>. It takes arguments {'flight': {'properties': {'flight_id': {'title': 'Flight Id', 'type': 'string'}, 'date_time': {'properties': {'year': {'title': 'Year', 'type': 'integer'}, 'month': {'title': 'Month', 'type': 'integer'}, 'day': {'title': 'Day', 'type': 'integer'}, 'hour': {'title': 'Hour', 'type': 'integer'}}, 'required': ['year', 'month', 'day', 'hour'], 'title': 'Date', 'type': 'object'}, 'origin': {'title': 'Origin', 'type': 'string'}, 'destination': {'title': 'Destination', 'type': 'string'}, 'duration': {'title': 'Duration', 'type': 'number'}, 'price': {'title': 'Price', 'type': 'number'}}, 'required': ['flight_id', 'date_time', 'origin', 'destination', 'duration', 'price'], 'title': 'Flight', 'type': 'object'}, 'user_profile': {'properties': {'user_id': {'title': 'User Id', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'email': {'title': 'Email', 'type': 'string'}}, 'required': ['user_id', 'name', 'email'], 'title': 'UserProfile', 'type': 'object'}} in JSON format.\n",
557 |             "        (5) cancel_itinerary, whose description is <desc>Cancel an itinerary on behalf of the user.</desc>. It takes arguments {'confirmation_number': {'type': 'string'}, 'user_profile': {'properties': {'user_id': {'title': 'User Id', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'email': {'title': 'Email', 'type': 'string'}}, 'required': ['user_id', 'name', 'email'], 'title': 'UserProfile', 'type': 'object'}} in JSON format.\n",
558 |             "        (6) get_user_info, whose description is <desc>Fetch the user profile from database with given name.</desc>. It takes arguments {'name': {'type': 'string'}} in JSON format.\n",
559 |             "        (7) file_ticket, whose description is <desc>File a customer support ticket if this is something the agent cannot handle.</desc>. It takes arguments {'user_request': {'type': 'string'}, 'user_profile': {'properties': {'user_id': {'title': 'User Id', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'email': {'title': 'Email', 'type': 'string'}}, 'required': ['user_id', 'name', 'email'], 'title': 'UserProfile', 'type': 'object'}} in JSON format.\n",
560 |             "        (8) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `process_result`, are now available to be extracted.</desc>. It takes arguments {} in JSON format.\n",
561 |             "\n",
562 |             "\n",
563 |             "\u001b[31mUser message:\u001b[0m\n",
564 |             "\n",
565 |             "[[ ## user_request ## ]]\n",
566 |             "please help me book a flight from SFO to JFK on 09/01/2025, my name is Adam\n",
567 |             "\n",
568 |             "[[ ## trajectory ## ]]\n",
569 |             "\n",
570 |             "\n",
571 |             "Respond with the corresponding output fields, starting with the field `[[ ## next_thought ## ]]`, then `[[ ## next_tool_name ## ]]` (must be formatted as a valid Python Literal['fetch_flight_info', 'fetch_itinerary', 'pick_flight', 'book_flight', 'cancel_itinerary', 'get_user_info', 'file_ticket', 'finish']), then `[[ ## next_tool_args ## ]]` (must be formatted as a valid Python dict[str, Any]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
572 |             "\n",
573 |             "\n",
574 |             "\u001b[31mResponse:\u001b[0m\n",
575 |             "\n",
576 |             "\u001b[32m[[ ## next_thought ## ]]\n",
577 |             "I need to fetch flight information for the route from SFO to JFK on the specified date of 09/01/2025. Once I have the flight options, I can help Adam pick the best flight and proceed with the booking.\n",
578 |             "\n",
579 |             "[[ ## next_tool_name ## ]]\n",
580 |             "fetch_flight_info\n",
581 |             "\n",
582 |             "[[ ## next_tool_args ## ]]\n",
583 |             "{\"date\":{\"year\":2025,\"month\":9,\"day\":1,\"hour\":0},\"origin\":\"SFO\",\"destination\":\"JFK\"}\n",
584 |             "\n",
585 |             "[[ ## completed ## ]]\u001b[0m\n",
586 |             "\n",
587 |             "\n",
588 |             "\n",
589 |             "\n",
590 |             "\n",
591 |             "\u001b[34m[2025-05-28T01:06:46.832209]\u001b[0m\n",
592 |             "\n",
593 |             "\u001b[31mSystem message:\u001b[0m\n",
594 |             "\n",
595 |             "Your input fields are:\n",
596 |             "1. `user_request` (str)\n",
597 |             "2. `trajectory` (str)\n",
598 |             "Your output fields are:\n",
599 |             "1. `next_thought` (str)\n",
600 |             "2. `next_tool_name` (Literal['fetch_flight_info', 'fetch_itinerary', 'pick_flight', 'book_flight', 'cancel_itinerary', 'get_user_info', 'file_ticket', 'finish'])\n",
601 |             "3. `next_tool_args` (dict[str, Any])\n",
602 |             "All interactions will be structured in the following way, with the appropriate values filled in.\n",
603 |             "\n",
604 |             "[[ ## user_request ## ]]\n",
605 |             "{user_request}\n",
606 |             "\n",
607 |             "[[ ## trajectory ## ]]\n",
608 |             "{trajectory}\n",
609 |             "\n",
610 |             "[[ ## next_thought ## ]]\n",
611 |             "{next_thought}\n",
612 |             "\n",
613 |             "[[ ## next_tool_name ## ]]\n",
614 |             "{next_tool_name}        # note: the value you produce must exactly match (no extra characters) one of: fetch_flight_info; fetch_itinerary; pick_flight; book_flight; cancel_itinerary; get_user_info; file_ticket; finish\n",
615 |             "\n",
616 |             "[[ ## next_tool_args ## ]]\n",
617 |             "{next_tool_args}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"object\", \"additionalProperties\": true}\n",
618 |             "\n",
619 |             "[[ ## completed ## ]]\n",
620 |             "In adhering to this structure, your objective is: \n",
621 |             "        You are an airline customer service agent that helps user book and manage flights. \n",
622 |             "        \n",
623 |             "        You are given a list of tools to handle user request, and you should decide the right tool to use in order to\n",
624 |             "        fulfill users' request.\n",
625 |             "        \n",
626 |             "        You are an Agent. In each episode, you will be given the fields `user_request` as input. And you can see your past trajectory so far.\n",
627 |             "        Your goal is to use one or more of the supplied tools to collect any necessary information for producing `process_result`.\n",
628 |             "        \n",
629 |             "        To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\n",
630 |             "        After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n",
631 |             "        \n",
632 |             "        When writing next_thought, you may reason about the current situation and plan for future steps.\n",
633 |             "        When selecting the next_tool_name and its next_tool_args, the tool must be one of:\n",
634 |             "        \n",
635 |             "        (1) fetch_flight_info, whose description is <desc>Fetch flight information from origin to destination on the given date</desc>. It takes arguments {'date': {'properties': {'year': {'title': 'Year', 'type': 'integer'}, 'month': {'title': 'Month', 'type': 'integer'}, 'day': {'title': 'Day', 'type': 'integer'}, 'hour': {'title': 'Hour', 'type': 'integer'}}, 'required': ['year', 'month', 'day', 'hour'], 'title': 'Date', 'type': 'object'}, 'origin': {'type': 'string'}, 'destination': {'type': 'string'}} in JSON format.\n",
636 |             "        (2) fetch_itinerary, whose description is <desc>Fetch a booked itinerary information from database</desc>. It takes arguments {'confirmation_number': {'type': 'string'}} in JSON format.\n",
637 |             "        (3) pick_flight, whose description is <desc>Pick up the best flight that matches users' request. we pick the shortest, and cheaper one on ties.</desc>. It takes arguments {'flights': {'$defs': {'Date': {'properties': {'year': {'title': 'Year', 'type': 'integer'}, 'month': {'title': 'Month', 'type': 'integer'}, 'day': {'title': 'Day', 'type': 'integer'}, 'hour': {'title': 'Hour', 'type': 'integer'}}, 'required': ['year', 'month', 'day', 'hour'], 'title': 'Date', 'type': 'object'}, 'Flight': {'properties': {'flight_id': {'title': 'Flight Id', 'type': 'string'}, 'date_time': {'$ref': '#/$defs/Date'}, 'origin': {'title': 'Origin', 'type': 'string'}, 'destination': {'title': 'Destination', 'type': 'string'}, 'duration': {'title': 'Duration', 'type': 'number'}, 'price': {'title': 'Price', 'type': 'number'}}, 'required': ['flight_id', 'date_time', 'origin', 'destination', 'duration', 'price'], 'title': 'Flight', 'type': 'object'}}, 'items': {'$ref': '#/$defs/Flight'}, 'type': 'array'}} in JSON format.\n",
638 |             "        (4) book_flight, whose description is <desc>Book a flight on behalf of the user.</desc>. It takes arguments {'flight': {'properties': {'flight_id': {'title': 'Flight Id', 'type': 'string'}, 'date_time': {'properties': {'year': {'title': 'Year', 'type': 'integer'}, 'month': {'title': 'Month', 'type': 'integer'}, 'day': {'title': 'Day', 'type': 'integer'}, 'hour': {'title': 'Hour', 'type': 'integer'}}, 'required': ['year', 'month', 'day', 'hour'], 'title': 'Date', 'type': 'object'}, 'origin': {'title': 'Origin', 'type': 'string'}, 'destination': {'title': 'Destination', 'type': 'string'}, 'duration': {'title': 'Duration', 'type': 'number'}, 'price': {'title': 'Price', 'type': 'number'}}, 'required': ['flight_id', 'date_time', 'origin', 'destination', 'duration', 'price'], 'title': 'Flight', 'type': 'object'}, 'user_profile': {'properties': {'user_id': {'title': 'User Id', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'email': {'title': 'Email', 'type': 'string'}}, 'required': ['user_id', 'name', 'email'], 'title': 'UserProfile', 'type': 'object'}} in JSON format.\n",
639 |             "        (5) cancel_itinerary, whose description is <desc>Cancel an itinerary on behalf of the user.</desc>. It takes arguments {'confirmation_number': {'type': 'string'}, 'user_profile': {'properties': {'user_id': {'title': 'User Id', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'email': {'title': 'Email', 'type': 'string'}}, 'required': ['user_id', 'name', 'email'], 'title': 'UserProfile', 'type': 'object'}} in JSON format.\n",
640 |             "        (6) get_user_info, whose description is <desc>Fetch the user profile from database with given name.</desc>. It takes arguments {'name': {'type': 'string'}} in JSON format.\n",
641 |             "        (7) file_ticket, whose description is <desc>File a customer support ticket if this is something the agent cannot handle.</desc>. It takes arguments {'user_request': {'type': 'string'}, 'user_profile': {'properties': {'user_id': {'title': 'User Id', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'email': {'title': 'Email', 'type': 'string'}}, 'required': ['user_id', 'name', 'email'], 'title': 'UserProfile', 'type': 'object'}} in JSON format.\n",
642 |             "        (8) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `process_result`, are now available to be extracted.</desc>. It takes arguments {} in JSON format.\n",
643 |             "\n",
644 |             "\n",
645 |             "\u001b[31mUser message:\u001b[0m\n",
646 |             "\n",
647 |             "[[ ## user_request ## ]]\n",
648 |             "please help me book a flight from SFO to JFK on 09/01/2025, my name is Adam\n",
649 |             "\n",
650 |             "[[ ## trajectory ## ]]\n",
651 |             "[[ ## thought_0 ## ]]\n",
652 |             "I need to fetch flight information for the route from SFO to JFK on the specified date of 09/01/2025. Once I have the flight options, I can help Adam pick the best flight and proceed with the booking.\n",
653 |             "\n",
654 |             "[[ ## tool_name_0 ## ]]\n",
655 |             "fetch_flight_info\n",
656 |             "\n",
657 |             "[[ ## tool_args_0 ## ]]\n",
658 |             "{\"date\": {\"year\": 2025, \"month\": 9, \"day\": 1, \"hour\": 0}, \"origin\": \"SFO\", \"destination\": \"JFK\"}\n",
659 |             "\n",
660 |             "[[ ## observation_0 ## ]]\n",
661 |             "[1] «flight_id='DA123' date_time=Date(year=2025, month=9, day=1, hour=1) origin='SFO' destination='JFK' duration=3.0 price=200.0»\n",
662 |             "[2] «flight_id='DA125' date_time=Date(year=2025, month=9, day=1, hour=7) origin='SFO' destination='JFK' duration=9.0 price=500.0»\n",
663 |             "\n",
664 |             "Respond with the corresponding output fields, starting with the field `[[ ## next_thought ## ]]`, then `[[ ## next_tool_name ## ]]` (must be formatted as a valid Python Literal['fetch_flight_info', 'fetch_itinerary', 'pick_flight', 'book_flight', 'cancel_itinerary', 'get_user_info', 'file_ticket', 'finish']), then `[[ ## next_tool_args ## ]]` (must be formatted as a valid Python dict[str, Any]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
665 |             "\n",
666 |             "\n",
667 |             "\u001b[31mResponse:\u001b[0m\n",
668 |             "\n",
669 |             "\u001b[32m[[ ## next_thought ## ]]\n",
670 |             "I have received the flight options from SFO to JFK on 09/01/2025. There are two flights available: DA123 at 1 AM for $200 and DA125 at 7 AM for $500. I will now pick the best flight, which is the cheaper option, DA123.\n",
671 |             "\n",
672 |             "[[ ## next_tool_name ## ]]\n",
673 |             "pick_flight\n",
674 |             "\n",
675 |             "[[ ## next_tool_args ## ]]\n",
676 |             "{\"flights\":[{\"flight_id\":\"DA123\",\"date_time\":{\"year\":2025,\"month\":9,\"day\":1,\"hour\":1},\"origin\":\"SFO\",\"destination\":\"JFK\",\"duration\":3.0,\"price\":200.0},{\"flight_id\":\"DA125\",\"date_time\":{\"year\":2025,\"month\":9,\"day\":1,\"hour\":7},\"origin\":\"SFO\",\"destination\":\"JFK\",\"duration\":9.0,\"price\":500.0}]}\n",
677 |             "\n",
678 |             "[[ ## completed ## ]]\u001b[0m\n",
679 |             "\n",
680 |             "\n",
681 |             "\n",
682 |             "\n",
683 |             "\n",
684 |             "\u001b[34m[2025-05-28T01:06:46.844268]\u001b[0m\n",
685 |             "\n",
686 |             "\u001b[31mSystem message:\u001b[0m\n",
687 |             "\n",
688 |             "Your input fields are:\n",
689 |             "1. `user_request` (str)\n",
690 |             "2. `trajectory` (str)\n",
691 |             "Your output fields are:\n",
692 |             "1. `next_thought` (str)\n",
693 |             "2. `next_tool_name` (Literal['fetch_flight_info', 'fetch_itinerary', 'pick_flight', 'book_flight', 'cancel_itinerary', 'get_user_info', 'file_ticket', 'finish'])\n",
694 |             "3. `next_tool_args` (dict[str, Any])\n",
695 |             "All interactions will be structured in the following way, with the appropriate values filled in.\n",
696 |             "\n",
697 |             "[[ ## user_request ## ]]\n",
698 |             "{user_request}\n",
699 |             "\n",
700 |             "[[ ## trajectory ## ]]\n",
701 |             "{trajectory}\n",
702 |             "\n",
703 |             "[[ ## next_thought ## ]]\n",
704 |             "{next_thought}\n",
705 |             "\n",
706 |             "[[ ## next_tool_name ## ]]\n",
707 |             "{next_tool_name}        # note: the value you produce must exactly match (no extra characters) one of: fetch_flight_info; fetch_itinerary; pick_flight; book_flight; cancel_itinerary; get_user_info; file_ticket; finish\n",
708 |             "\n",
709 |             "[[ ## next_tool_args ## ]]\n",
710 |             "{next_tool_args}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"object\", \"additionalProperties\": true}\n",
711 |             "\n",
712 |             "[[ ## completed ## ]]\n",
713 |             "In adhering to this structure, your objective is: \n",
714 |             "        You are an airline customer service agent that helps user book and manage flights. \n",
715 |             "        \n",
716 |             "        You are given a list of tools to handle user request, and you should decide the right tool to use in order to\n",
717 |             "        fulfill users' request.\n",
718 |             "        \n",
719 |             "        You are an Agent. In each episode, you will be given the fields `user_request` as input. And you can see your past trajectory so far.\n",
720 |             "        Your goal is to use one or more of the supplied tools to collect any necessary information for producing `process_result`.\n",
721 |             "        \n",
722 |             "        To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\n",
723 |             "        After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n",
724 |             "        \n",
725 |             "        When writing next_thought, you may reason about the current situation and plan for future steps.\n",
726 |             "        When selecting the next_tool_name and its next_tool_args, the tool must be one of:\n",
727 |             "        \n",
728 |             "        (1) fetch_flight_info, whose description is <desc>Fetch flight information from origin to destination on the given date</desc>. It takes arguments {'date': {'properties': {'year': {'title': 'Year', 'type': 'integer'}, 'month': {'title': 'Month', 'type': 'integer'}, 'day': {'title': 'Day', 'type': 'integer'}, 'hour': {'title': 'Hour', 'type': 'integer'}}, 'required': ['year', 'month', 'day', 'hour'], 'title': 'Date', 'type': 'object'}, 'origin': {'type': 'string'}, 'destination': {'type': 'string'}} in JSON format.\n",
729 |             "        (2) fetch_itinerary, whose description is <desc>Fetch a booked itinerary information from database</desc>. It takes arguments {'confirmation_number': {'type': 'string'}} in JSON format.\n",
730 |             "        (3) pick_flight, whose description is <desc>Pick up the best flight that matches users' request. we pick the shortest, and cheaper one on ties.</desc>. It takes arguments {'flights': {'$defs': {'Date': {'properties': {'year': {'title': 'Year', 'type': 'integer'}, 'month': {'title': 'Month', 'type': 'integer'}, 'day': {'title': 'Day', 'type': 'integer'}, 'hour': {'title': 'Hour', 'type': 'integer'}}, 'required': ['year', 'month', 'day', 'hour'], 'title': 'Date', 'type': 'object'}, 'Flight': {'properties': {'flight_id': {'title': 'Flight Id', 'type': 'string'}, 'date_time': {'$ref': '#/$defs/Date'}, 'origin': {'title': 'Origin', 'type': 'string'}, 'destination': {'title': 'Destination', 'type': 'string'}, 'duration': {'title': 'Duration', 'type': 'number'}, 'price': {'title': 'Price', 'type': 'number'}}, 'required': ['flight_id', 'date_time', 'origin', 'destination', 'duration', 'price'], 'title': 'Flight', 'type': 'object'}}, 'items': {'$ref': '#/$defs/Flight'}, 'type': 'array'}} in JSON format.\n",
731 |             "        (4) book_flight, whose description is <desc>Book a flight on behalf of the user.</desc>. It takes arguments {'flight': {'properties': {'flight_id': {'title': 'Flight Id', 'type': 'string'}, 'date_time': {'properties': {'year': {'title': 'Year', 'type': 'integer'}, 'month': {'title': 'Month', 'type': 'integer'}, 'day': {'title': 'Day', 'type': 'integer'}, 'hour': {'title': 'Hour', 'type': 'integer'}}, 'required': ['year', 'month', 'day', 'hour'], 'title': 'Date', 'type': 'object'}, 'origin': {'title': 'Origin', 'type': 'string'}, 'destination': {'title': 'Destination', 'type': 'string'}, 'duration': {'title': 'Duration', 'type': 'number'}, 'price': {'title': 'Price', 'type': 'number'}}, 'required': ['flight_id', 'date_time', 'origin', 'destination', 'duration', 'price'], 'title': 'Flight', 'type': 'object'}, 'user_profile': {'properties': {'user_id': {'title': 'User Id', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'email': {'title': 'Email', 'type': 'string'}}, 'required': ['user_id', 'name', 'email'], 'title': 'UserProfile', 'type': 'object'}} in JSON format.\n",
732 |             "        (5) cancel_itinerary, whose description is <desc>Cancel an itinerary on behalf of the user.</desc>. It takes arguments {'confirmation_number': {'type': 'string'}, 'user_profile': {'properties': {'user_id': {'title': 'User Id', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'email': {'title': 'Email', 'type': 'string'}}, 'required': ['user_id', 'name', 'email'], 'title': 'UserProfile', 'type': 'object'}} in JSON format.\n",
733 |             "        (6) get_user_info, whose description is <desc>Fetch the user profile from database with given name.</desc>. It takes arguments {'name': {'type': 'string'}} in JSON format.\n",
734 |             "        (7) file_ticket, whose description is <desc>File a customer support ticket if this is something the agent cannot handle.</desc>. It takes arguments {'user_request': {'type': 'string'}, 'user_profile': {'properties': {'user_id': {'title': 'User Id', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'email': {'title': 'Email', 'type': 'string'}}, 'required': ['user_id', 'name', 'email'], 'title': 'UserProfile', 'type': 'object'}} in JSON format.\n",
735 |             "        (8) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `process_result`, are now available to be extracted.</desc>. It takes arguments {} in JSON format.\n",
736 |             "\n",
737 |             "\n",
738 |             "\u001b[31mUser message:\u001b[0m\n",
739 |             "\n",
740 |             "[[ ## user_request ## ]]\n",
741 |             "please help me book a flight from SFO to JFK on 09/01/2025, my name is Adam\n",
742 |             "\n",
743 |             "[[ ## trajectory ## ]]\n",
744 |             "[[ ## thought_0 ## ]]\n",
745 |             "I need to fetch flight information for the route from SFO to JFK on the specified date of 09/01/2025. Once I have the flight options, I can help Adam pick the best flight and proceed with the booking.\n",
746 |             "\n",
747 |             "[[ ## tool_name_0 ## ]]\n",
748 |             "fetch_flight_info\n",
749 |             "\n",
750 |             "[[ ## tool_args_0 ## ]]\n",
[TRUNCATED]
```

tutorials/deployment/index.md
```
1 | # Tutorial: Deploying your DSPy program
2 | 
3 | This guide demonstrates two potential ways to deploy your DSPy program in production: FastAPI for lightweight deployments and MLflow for more production-grade deployments with program versioning and management.
4 | 
5 | Below, we'll assume you have the following simple DSPy program that you want to deploy. You can replace this with something more sophisticated.
6 | 
7 | ```python
8 | import dspy
9 | 
10 | dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))
11 | dspy_program = dspy.ChainOfThought("question -> answer")
12 | ```
13 | 
14 | ## Deploying with FastAPI
15 | 
16 | FastAPI offers a straightforward way to serve your DSPy program as a REST API. This is ideal when you have direct access to your program code and need a lightweight deployment solution.
17 | 
18 | ```bash
19 | > pip install fastapi uvicorn
20 | > export OPENAI_API_KEY="your-openai-api-key"
21 | ```
22 | 
23 | Let's create a FastAPI application to serve your `dspy_program` defined above.
24 | 
25 | ```python
26 | from fastapi import FastAPI, HTTPException
27 | from pydantic import BaseModel
28 | 
29 | import dspy
30 | 
31 | app = FastAPI(
32 |     title="DSPy Program API",
33 |     description="A simple API serving a DSPy Chain of Thought program",
34 |     version="1.0.0"
35 | )
36 | 
37 | # Define request model for better documentation and validation
38 | class Question(BaseModel):
39 |     text: str
40 | 
41 | # Configure your language model and 'asyncify' your DSPy program.
42 | lm = dspy.LM("openai/gpt-4o-mini")
43 | dspy.settings.configure(lm=lm, async_max_workers=4) # default is 8
44 | dspy_program = dspy.ChainOfThought("question -> answer")
45 | dspy_program = dspy.asyncify(dspy_program)
46 | 
47 | @app.post("/predict")
48 | async def predict(question: Question):
49 |     try:
50 |         result = await dspy_program(question=question.text)
51 |         return {
52 |             "status": "success",
53 |             "data": result.toDict()
54 |         }
55 |     except Exception as e:
56 |         raise HTTPException(status_code=500, detail=str(e))
57 | ```
58 | 
59 | In the code above, we call `dspy.asyncify` to convert the dspy program to run in async mode for high-throughput FastAPI
60 | deployments. Currently, this runs the dspy program in a separate thread and awaits its result.
61 | 
62 | By default, the limit of spawned threads is 8. Think of this like a worker pool.
63 | If you have 8 in-flight programs and call it once more, the 9th call will wait until one of the 8 returns.
64 | You can configure the async capacity using the new `async_max_workers` setting.
65 | 
66 | ??? "Streaming, in DSPy 2.6.0+"
67 | 
68 |     Streaming is also supported in DSPy 2.6.0+, which can be installed via `pip install -U dspy`.
69 | 
70 |     We can use `dspy.streamify` to convert the dspy program to a streaming mode. This is useful when you want to stream
71 |     the intermediate outputs (i.e. O1-style reasoning) to the client before the final prediction is ready. This uses
72 |     asyncify under the hood and inherits the execution semantics.
73 | 
74 |     ```python
75 |     dspy_program = dspy.asyncify(dspy.ChainOfThought("question -> answer"))
76 |     streaming_dspy_program = dspy.streamify(dspy_program)
77 | 
78 |     @app.post("/predict/stream")
79 |     async def stream(question: Question):
80 |         async def generate():
81 |             async for value in streaming_dspy_program(question=question.text):
82 |                 if isinstance(value, dspy.Prediction):
83 |                     data = {"prediction": value.labels().toDict()}
84 |                 elif isinstance(value, litellm.ModelResponse):
85 |                     data = {"chunk": value.json()}
86 |                 yield f"data: {ujson.dumps(data)}\n\n"
87 |             yield "data: [DONE]\n\n"
88 | 
89 |         return StreamingResponse(generate(), media_type="text/event-stream")
90 | 
91 |     # Since you're often going to want to stream the result of a DSPy program as server-sent events,
92 |     # we've included a helper function for that, which is equivalent to the code above.
93 | 
94 |     from dspy.utils.streaming import streaming_response
95 | 
96 |     @app.post("/predict/stream")
97 |     async def stream(question: Question):
98 |         stream = streaming_dspy_program(question=question.text)
99 |         return StreamingResponse(streaming_response(stream), media_type="text/event-stream")
100 |     ```
101 | 
102 | Write your code to a file, e.g., `fastapi_dspy.py`. Then you can serve the app with:
103 | 
104 | ```bash
105 | > uvicorn fastapi_dspy:app --reload
106 | ```
107 | 
108 | It will start a local server at `http://127.0.0.1:8000/`. You can test it with the python code below:
109 | 
110 | ```python
111 | import requests
112 | 
113 | response = requests.post(
114 |     "http://127.0.0.1:8000/predict",
115 |     json={"text": "What is the capital of France?"}
116 | )
117 | print(response.json())
118 | ```
119 | 
120 | You should see the response like below:
121 | 
122 | ```json
123 | {
124 |   "status": "success",
125 |   "data": {
126 |     "reasoning": "The capital of France is a well-known fact, commonly taught in geography classes and referenced in various contexts. Paris is recognized globally as the capital city, serving as the political, cultural, and economic center of the country.",
127 |     "answer": "The capital of France is Paris."
128 |   }
129 | }
130 | ```
131 | 
132 | ## Deploying with MLflow
133 | 
134 | We recommend deploying with MLflow if you are looking to package your DSPy program and deploy in an isolated environment.
135 | MLflow is a popular platform for managing machine learning workflows, including versioning, tracking, and deployment.
136 | 
137 | ```bash
138 | > pip install mlflow>=2.18.0
139 | ```
140 | 
141 | Let's spin up the MLflow tracking server, where we will store our DSPy program. The command below will start a local server at
142 | `http://127.0.0.1:5000/`.
143 | 
144 | ```bash
145 | > mlflow ui
146 | ```
147 | 
148 | Then we can define the DSPy program and log it to the MLflow server. "log" is an overloaded term in MLflow, basically it means
149 | we store the program information along with environment requirements in the MLflow server. This is done via the `mlflow.dspy.log_model()`
150 | function, please see the code below:
151 | 
152 | > [!NOTE]
153 | > As of MLflow 2.22.0, there is a caveat that you must wrap your DSPy program in a custom DSPy Module class when deploying with MLflow.
154 | > This is because MLflow requires positional arguments while DSPy pre-built modules disallow positional arguments, e.g., `dspy.Predict`
155 | > or `dspy.ChainOfThought`. To work around this, create a wrapper class that inherits from `dspy.Module` and implement your program's
156 | > logic in the `forward()` method, as shown in the example below.
157 | 
158 | ```python
159 | import dspy
160 | import mlflow
161 | 
162 | mlflow.set_tracking_uri("http://127.0.0.1:5000/")
163 | mlflow.set_experiment("deploy_dspy_program")
164 | 
165 | lm = dspy.LM("openai/gpt-4o-mini")
166 | dspy.settings.configure(lm=lm)
167 | 
168 | class MyProgram(dspy.Module):
169 |     def __init__(self):
170 |         super().__init__()
171 |         self.cot = dspy.ChainOfThought("question -> answer")
172 | 
173 |     def forward(self, messages):
174 |         return self.cot(question=messages[0]["content"])
175 | 
176 | dspy_program = MyProgram()
177 | 
178 | with mlflow.start_run():
179 |     mlflow.dspy.log_model(
180 |         dspy_program,
181 |         "dspy_program",
182 |         input_example={"messages": [{"role": "user", "content": "What is LLM agent?"}]},
183 |         task="llm/v1/chat",
184 |     )
185 | ```
186 | 
187 | We recommend you to set `task="llm/v1/chat"` so that the deployed program automatically takes input and generate output in
188 | the same format as the OpenAI chat API, which is a common interface for LM applications. Write the code above into
189 | a file, e.g. `mlflow_dspy.py`, and run it.
190 | 
191 | After you logged the program, you can view the saved information in MLflow UI. Open `http://127.0.0.1:5000/` and select
192 | the `deploy_dspy_program` experiment, then select the run your just created, under the `Artifacts` tab, you should see the
193 | logged program information, similar to the following screenshot:
194 | 
195 | ![MLflow UI](./dspy_mlflow_ui.png)
196 | 
197 | Grab your run id from UI (or the console print when you execute `mlflow_dspy.py`), now you can deploy the logged program
198 | with the following command:
199 | 
200 | ```bash
201 | > mlflow models serve -m runs:/{run_id}/model -p 6000
202 | ```
203 | 
204 | After the program is deployed, you can test it with the following command:
205 | 
206 | ```bash
207 | > curl http://127.0.0.1:6000/invocations -H "Content-Type:application/json"  --data '{"messages": [{"content": "what is 2 + 2?", "role": "user"}]}'
208 | ```
209 | 
210 | You should see the response like below:
211 | 
212 | ```json
213 | {
214 |   "choices": [
215 |     {
216 |       "index": 0,
217 |       "message": {
218 |         "role": "assistant",
219 |         "content": "{\"reasoning\": \"The question asks for the sum of 2 and 2. To find the answer, we simply add the two numbers together: 2 + 2 = 4.\", \"answer\": \"4\"}"
220 |       },
221 |       "finish_reason": "stop"
222 |     }
223 |   ]
224 | }
225 | ```
226 | 
227 | For complete guide on how to deploy a DSPy program with MLflow, and how to customize the deployment, please refer to the
228 | [MLflow documentation](https://mlflow.org/docs/latest/llms/dspy/index.html).
229 | 
230 | ### Best Practices for MLflow Deployment
231 | 
232 | 1. **Environment Management**: Always specify your Python dependencies in a `conda.yaml` or `requirements.txt` file.
233 | 2. **Versioning**: Use meaningful tags and descriptions for your model versions.
234 | 3. **Input Validation**: Define clear input schemas and examples.
235 | 4. **Monitoring**: Set up proper logging and monitoring for production deployments.
236 | 
237 | For production deployments, consider using MLflow with containerization:
238 | 
239 | ```bash
240 | > mlflow models build-docker -m "runs:/{run_id}/model" -n "dspy-program"
241 | > docker run -p 6000:8080 dspy-program
242 | ```
243 | 
244 | For a complete guide on production deployment options and best practices, refer to the
245 | [MLflow documentation](https://mlflow.org/docs/latest/llms/dspy/index.html).
```

tutorials/email_extraction/index.md
```
1 | # Extracting Information from Emails with DSPy
2 | 
3 | This tutorial demonstrates how to build an intelligent email processing system using DSPy. We'll create a system that can automatically extract key information from various types of emails, classify their intent, and structure the data for further processing.
4 | 
5 | ## What You'll Build
6 | 
7 | By the end of this tutorial, you'll have a DSPy-powered email processing system that can:
8 | 
9 | - **Classify email types** (order confirmation, support request, meeting invitation, etc.)
10 | - **Extract key entities** (dates, amounts, product names, contact info)
11 | - **Determine urgency levels** and required actions
12 | - **Structure extracted data** into consistent formats
13 | - **Handle multiple email formats** robustly
14 | 
15 | ## Prerequisites
16 | 
17 | - Basic understanding of DSPy modules and signatures
18 | - Python 3.9+ installed
19 | - OpenAI API key (or access to another supported LLM)
20 | 
21 | ## Installation and Setup
22 | 
23 | ```bash
24 | pip install dspy
25 | ```
26 | 
27 | <details>
28 | <summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>
29 | 
30 | ### MLflow DSPy Integration
31 | 
32 | <a href="https://mlflow.org/">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.
33 | 
34 | ![MLflow Trace](./mlflow-tracing-email-extraction.png)
35 | 
36 | 1. Install MLflow
37 | 
38 | ```bash
39 | %pip install mlflow>=3.0.0
40 | ```
41 | 
42 | 2. Start MLflow UI in a separate terminal
43 | ```bash
44 | mlflow ui --port 5000 --backend-store-uri sqlite:///mlruns.db
45 | ```
46 | 
47 | 3. Connect the notebook to MLflow
48 | ```python
49 | import mlflow
50 | 
51 | mlflow.set_tracking_uri("http://localhost:5000")
52 | mlflow.set_experiment("DSPy")
53 | ```
54 | 
55 | 4. Enabling tracing.
56 | ```python
57 | mlflow.dspy.autolog()
58 | ```
59 | 
60 | 
61 | To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.
62 | </details>
63 | 
64 | ## Step 1: Define Our Data Structures
65 | 
66 | First, let's define the types of information we want to extract from emails:
67 | 
68 | ```python
69 | import dspy
70 | from typing import List, Optional, Literal
71 | from datetime import datetime
72 | from pydantic import BaseModel
73 | from enum import Enum
74 | 
75 | class EmailType(str, Enum):
76 |     ORDER_CONFIRMATION = "order_confirmation"
77 |     SUPPORT_REQUEST = "support_request"
78 |     MEETING_INVITATION = "meeting_invitation"
79 |     NEWSLETTER = "newsletter"
80 |     PROMOTIONAL = "promotional"
81 |     INVOICE = "invoice"
82 |     SHIPPING_NOTIFICATION = "shipping_notification"
83 |     OTHER = "other"
84 | 
85 | class UrgencyLevel(str, Enum):
86 |     LOW = "low"
87 |     MEDIUM = "medium"
88 |     HIGH = "high"
89 |     CRITICAL = "critical"
90 | 
91 | class ExtractedEntity(BaseModel):
92 |     entity_type: str
93 |     value: str
94 |     confidence: float
95 | ```
96 | 
97 | ## Step 2: Create DSPy Signatures
98 | 
99 | Now let's define the signatures for our email processing pipeline:
100 | 
101 | ```python
102 | class ClassifyEmail(dspy.Signature):
103 |     """Classify the type and urgency of an email based on its content."""
104 | 
105 |     email_subject: str = dspy.InputField(desc="The subject line of the email")
106 |     email_body: str = dspy.InputField(desc="The main content of the email")
107 |     sender: str = dspy.InputField(desc="Email sender information")
108 | 
109 |     email_type: EmailType = dspy.OutputField(desc="The classified type of email")
110 |     urgency: UrgencyLevel = dspy.OutputField(desc="The urgency level of the email")
111 |     reasoning: str = dspy.OutputField(desc="Brief explanation of the classification")
112 | 
113 | class ExtractEntities(dspy.Signature):
114 |     """Extract key entities and information from email content."""
115 | 
116 |     email_content: str = dspy.InputField(desc="The full email content including subject and body")
117 |     email_type: EmailType = dspy.InputField(desc="The classified type of email")
118 | 
119 |     key_entities: list[ExtractedEntity] = dspy.OutputField(desc="List of extracted entities with type, value, and confidence")
120 |     financial_amount: Optional[float] = dspy.OutputField(desc="Any monetary amounts found (e.g., '$99.99')")
121 |     important_dates: list[str] = dspy.OutputField(desc="List of important dates found in the email")
122 |     contact_info: list[str] = dspy.OutputField(desc="Relevant contact information extracted")
123 | 
124 | class GenerateActionItems(dspy.Signature):
125 |     """Determine what actions are needed based on the email content and extracted information."""
126 | 
127 |     email_type: EmailType = dspy.InputField()
128 |     urgency: UrgencyLevel = dspy.InputField()
129 |     email_summary: str = dspy.InputField(desc="Brief summary of the email content")
130 |     extracted_entities: list[ExtractedEntity] = dspy.InputField(desc="Key entities found in the email")
131 | 
132 |     action_required: bool = dspy.OutputField(desc="Whether any action is required")
133 |     action_items: list[str] = dspy.OutputField(desc="List of specific actions needed")
134 |     deadline: Optional[str] = dspy.OutputField(desc="Deadline for action if applicable")
135 |     priority_score: int = dspy.OutputField(desc="Priority score from 1-10")
136 | 
137 | class SummarizeEmail(dspy.Signature):
138 |     """Create a concise summary of the email content."""
139 | 
140 |     email_subject: str = dspy.InputField()
141 |     email_body: str = dspy.InputField()
142 |     key_entities: list[ExtractedEntity] = dspy.InputField()
143 | 
144 |     summary: str = dspy.OutputField(desc="A 2-3 sentence summary of the email's main points")
145 | ```
146 | 
147 | ## Step 3: Build the Email Processing Module
148 | 
149 | Now let's create our main email processing module:
150 | 
151 | ```python
152 | class EmailProcessor(dspy.Module):
153 |     """A comprehensive email processing system using DSPy."""
154 | 
155 |     def __init__(self):
156 |         super().__init__()
157 | 
158 |         # Initialize our processing components
159 |         self.classifier = dspy.ChainOfThought(ClassifyEmail)
160 |         self.entity_extractor = dspy.ChainOfThought(ExtractEntities)
161 |         self.action_generator = dspy.ChainOfThought(GenerateActionItems)
162 |         self.summarizer = dspy.ChainOfThought(SummarizeEmail)
163 | 
164 |     def forward(self, email_subject: str, email_body: str, sender: str = ""):
165 |         """Process an email and extract structured information."""
166 | 
167 |         # Step 1: Classify the email
168 |         classification = self.classifier(
169 |             email_subject=email_subject,
170 |             email_body=email_body,
171 |             sender=sender
172 |         )
173 | 
174 |         # Step 2: Extract entities
175 |         full_content = f"Subject: {email_subject}\n\nFrom: {sender}\n\n{email_body}"
176 |         entities = self.entity_extractor(
177 |             email_content=full_content,
178 |             email_type=classification.email_type
179 |         )
180 | 
181 |         # Step 3: Generate summary
182 |         summary = self.summarizer(
183 |             email_subject=email_subject,
184 |             email_body=email_body,
185 |             key_entities=entities.key_entities
186 |         )
187 | 
188 |         # Step 4: Determine actions
189 |         actions = self.action_generator(
190 |             email_type=classification.email_type,
191 |             urgency=classification.urgency,
192 |             email_summary=summary.summary,
193 |             extracted_entities=entities.key_entities
194 |         )
195 | 
196 |         # Step 5: Structure the results
197 |         return dspy.Prediction(
198 |             email_type=classification.email_type,
199 |             urgency=classification.urgency,
200 |             summary=summary.summary,
201 |             key_entities=entities.key_entities,
202 |             financial_amount=entities.financial_amount,
203 |             important_dates=entities.important_dates,
204 |             action_required=actions.action_required,
205 |             action_items=actions.action_items,
206 |             deadline=actions.deadline,
207 |             priority_score=actions.priority_score,
208 |             reasoning=classification.reasoning,
209 |             contact_info=entities.contact_info
210 |         )
211 | ```
212 | 
213 | ## Step 4: Running the Email Processing System
214 | 
215 | Let's create a simple function to test our email processing system:
216 | 
217 | ```python
218 | import os
219 | def run_email_processing_demo():
220 |     """Demonstration of the email processing system."""
221 |     
222 |     # Configure DSPy
223 |     lm = dspy.LM(model='openai/gpt-4o-mini')
224 |     dspy.configure(lm=lm)
225 |     os.environ["OPENAI_API_KEY"] = "<YOUR OPENAI KEY>"
226 |     
227 |     # Create our email processor
228 |     processor = EmailProcessor()
229 |     
230 |     # Sample emails for testing
231 |     sample_emails = [
232 |         {
233 |             "subject": "Order Confirmation #12345 - Your MacBook Pro is on the way!",
234 |             "body": """Dear John Smith,
235 | 
236 | Thank you for your order! We're excited to confirm that your order #12345 has been processed.
237 | 
238 | Order Details:
239 | - MacBook Pro 14-inch (Space Gray)
240 | - Order Total: $2,399.00
241 | - Estimated Delivery: December 15, 2024
242 | - Tracking Number: 1Z999AA1234567890
243 | 
244 | If you have any questions, please contact our support team at support@techstore.com.
245 | 
246 | Best regards,
247 | TechStore Team""",
248 |             "sender": "orders@techstore.com"
249 |         },
250 |         {
251 |             "subject": "URGENT: Server Outage - Immediate Action Required",
252 |             "body": """Hi DevOps Team,
253 | 
254 | We're experiencing a critical server outage affecting our production environment.
255 | 
256 | Impact: All users unable to access the platform
257 | Started: 2:30 PM EST
258 | 
259 | Please join the emergency call immediately: +1-555-123-4567
260 | 
261 | This is our highest priority.
262 | 
263 | Thanks,
264 | Site Reliability Team""",
265 |             "sender": "alerts@company.com"
266 |         },
267 |         {
268 |             "subject": "Meeting Invitation: Q4 Planning Session",
269 |             "body": """Hello team,
270 | 
271 | You're invited to our Q4 planning session.
272 | 
273 | When: Friday, December 20, 2024 at 2:00 PM - 4:00 PM EST
274 | Where: Conference Room A
275 | 
276 | Please confirm your attendance by December 18th.
277 | 
278 | Best,
279 | Sarah Johnson""",
280 |             "sender": "sarah.johnson@company.com"
281 |         }
282 |     ]
283 |     
284 |     # Process each email and display results
285 |     print("🚀 Email Processing Demo")
286 |     print("=" * 50)
287 |     
288 |     for i, email in enumerate(sample_emails):
289 |         print(f"\n📧 EMAIL {i+1}: {email['subject'][:50]}...")
290 |         
291 |         # Process the email
292 |         result = processor(
293 |             email_subject=email["subject"],
294 |             email_body=email["body"],
295 |             sender=email["sender"]
296 |         )
297 |         
298 |         # Display key results
299 |         print(f"   📊 Type: {result.email_type}")
300 |         print(f"   🚨 Urgency: {result.urgency}")
301 |         print(f"   📝 Summary: {result.summary}")
302 |         
303 |         if result.financial_amount:
304 |             print(f"   💰 Amount: ${result.financial_amount:,.2f}")
305 |         
306 |         if result.action_required:
307 |             print(f"   ✅ Action Required: Yes")
308 |             if result.deadline:
309 |                 print(f"   ⏰ Deadline: {result.deadline}")
310 |         else:
311 |             print(f"   ✅ Action Required: No")
312 | 
313 | # Run the demo
314 | if __name__ == "__main__":
315 |     run_email_processing_demo()
316 | ```
317 | 
318 | ## Expected Output
319 | ```
320 | 🚀 Email Processing Demo
321 | ==================================================
322 | 
323 | 📧 EMAIL 1: Order Confirmation #12345 - Your MacBook Pro is on...
324 |    📊 Type: order_confirmation
325 |    🚨 Urgency: low
326 |    📝 Summary: The email confirms John Smith's order #12345 for a MacBook Pro 14-inch in Space Gray, totaling $2,399.00, with an estimated delivery date of December 15, 2024. It includes a tracking number and contact information for customer support.
327 |    💰 Amount: $2,399.00
328 |    ✅ Action Required: No
329 | 
330 | 📧 EMAIL 2: URGENT: Server Outage - Immediate Action Required...
331 |    📊 Type: other
332 |    🚨 Urgency: critical
333 |    📝 Summary: The Site Reliability Team has reported a critical server outage that began at 2:30 PM EST, preventing all users from accessing the platform. They have requested the DevOps Team to join an emergency call immediately to address the issue.
334 |    ✅ Action Required: Yes
335 |    ⏰ Deadline: Immediately
336 | 
337 | 📧 EMAIL 3: Meeting Invitation: Q4 Planning Session...
338 |    📊 Type: meeting_invitation
339 |    🚨 Urgency: medium
340 |    📝 Summary: Sarah Johnson has invited the team to a Q4 planning session on December 20, 2024, from 2:00 PM to 4:00 PM EST in Conference Room A. Attendees are asked to confirm their participation by December 18th.
341 |    ✅ Action Required: Yes
342 |    ⏰ Deadline: December 18th
343 | ```
344 | 
345 | ## Next Steps
346 | 
347 | - **Add more email types** and refine classification (newsletter, promotional, etc.)
348 | - **Add integration** with email providers (Gmail API, Outlook, IMAP)
349 | - **Experiment with different LLMs** and optimization strategies
350 | - **Add multilingual support** for international email processing
351 | - **Optimization** for increasing the performance of your program
```

tutorials/entity_extraction/index.ipynb
```
1 | {
2 |   "cells": [
3 |     {
4 |       "cell_type": "markdown",
5 |       "metadata": {},
6 |       "source": [
7 |         "# Tutorial: Entity Extraction\n",
8 |         "\n",
9 |         "This tutorial demonstrates how to perform **entity extraction** using the CoNLL-2003 dataset with DSPy. The focus is on extracting entities referring to people. We will:\n",
10 |         "\n",
11 |         "- Extract and label entities from the CoNLL-2003 dataset that refer to people\n",
12 |         "- Define a DSPy program for extracting entities that refer to people\n",
13 |         "- Optimize and evaluate the program on a subset of the CoNLL-2003 dataset\n",
14 |         "\n",
15 |         "By the end of this tutorial, you'll understand how to structure tasks in DSPy using signatures and modules, evaluate your system's performance, and improve its quality with optimizers.\n",
16 |         "\n",
17 |         "Install the latest version of DSPy and follow along. If you're looking instead for a conceptual overview of DSPy, this [recent lecture](https://www.youtube.com/live/JEMYuzrKLUw) is a good place to start."
18 |       ]
19 |     },
20 |     {
21 |       "cell_type": "code",
22 |       "execution_count": 1,
23 |       "metadata": {},
24 |       "outputs": [],
25 |       "source": [
26 |         "# Install the latest version of DSPy\n",
27 |         "%pip install -U dspy\n",
28 |         "# Install the Hugging Face datasets library to load the CoNLL-2003 dataset\n",
29 |         "%pip install datasets"
30 |       ]
31 |     },
32 |     {
33 |       "cell_type": "markdown",
34 |       "metadata": {},
35 |       "source": [
36 |         "<details>\n",
37 |         "<summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>\n",
38 |         "\n",
39 |         "### MLflow DSPy Integration\n",
40 |         "\n",
41 |         "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
42 |         "\n",
43 |         "![MLflow Trace](./mlflow-tracing-entity-extraction.png)\n",
44 |         "\n",
45 |         "1. Install MLflow\n",
46 |         "\n",
47 |         "```bash\n",
48 |         "%pip install mlflow>=2.20\n",
49 |         "```\n",
50 |         "\n",
51 |         "2. Start MLflow UI in a separate terminal\n",
52 |         "```bash\n",
53 |         "mlflow ui --port 5000\n",
54 |         "```\n",
55 |         "\n",
56 |         "3. Connect the notebook to MLflow\n",
57 |         "```python\n",
58 |         "import mlflow\n",
59 |         "\n",
60 |         "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
61 |         "mlflow.set_experiment(\"DSPy\")\n",
62 |         "```\n",
63 |         "\n",
64 |         "4. Enabling tracing.\n",
65 |         "```python\n",
66 |         "mlflow.dspy.autolog()\n",
67 |         "```\n",
68 |         "\n",
69 |         "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
70 |         "</details>"
71 |       ]
72 |     },
73 |     {
74 |       "cell_type": "markdown",
75 |       "metadata": {},
76 |       "source": [
77 |         "## Load and Prepare the Dataset\n",
78 |         "\n",
79 |         "In this section, we prepare the CoNLL-2003 dataset, which is commonly used for entity extraction tasks. The dataset includes tokens annotated with entity labels such as persons, organizations, and locations.\n",
80 |         "\n",
81 |         "We will:\n",
82 |         "1. Load the dataset using the Hugging Face `datasets` library.\n",
83 |         "2. Define a function to extract tokens referring to people.\n",
84 |         "3. Slice the dataset to create smaller subsets for training and testing.\n",
85 |         "\n",
86 |         "DSPy expects examples in a structured format, so we'll also transform the dataset into DSPy `Examples` for easy integration."
87 |       ]
88 |     },
89 |     {
90 |       "cell_type": "code",
91 |       "execution_count": 2,
92 |       "metadata": {},
93 |       "outputs": [],
94 |       "source": [
95 |         "import os\n",
96 |         "import tempfile\n",
97 |         "from datasets import load_dataset\n",
98 |         "from typing import Dict, Any, List\n",
99 |         "import dspy\n",
100 |         "\n",
101 |         "def load_conll_dataset() -> dict:\n",
102 |         "    \"\"\"\n",
103 |         "    Loads the CoNLL-2003 dataset into train, validation, and test splits.\n",
104 |         "    \n",
105 |         "    Returns:\n",
106 |         "        dict: Dataset splits with keys 'train', 'validation', and 'test'.\n",
107 |         "    \"\"\"\n",
108 |         "    with tempfile.TemporaryDirectory() as temp_dir:\n",
109 |         "        # Use a temporary Hugging Face cache directory for compatibility with certain hosted notebook\n",
110 |         "        # environments that don't support the default Hugging Face cache directory\n",
111 |         "        os.environ[\"HF_DATASETS_CACHE\"] = temp_dir\n",
112 |         "        return load_dataset(\"conll2003\", trust_remote_code=True)\n",
113 |         "\n",
114 |         "def extract_people_entities(data_row: dict[str, Any]) -> list[str]:\n",
115 |         "    \"\"\"\n",
116 |         "    Extracts entities referring to people from a row of the CoNLL-2003 dataset.\n",
117 |         "    \n",
118 |         "    Args:\n",
119 |         "        data_row (dict[str, Any]): A row from the dataset containing tokens and NER tags.\n",
120 |         "    \n",
121 |         "    Returns:\n",
122 |         "        list[str]: List of tokens tagged as people.\n",
123 |         "    \"\"\"\n",
124 |         "    return [\n",
125 |         "        token\n",
126 |         "        for token, ner_tag in zip(data_row[\"tokens\"], data_row[\"ner_tags\"])\n",
127 |         "        if ner_tag in (1, 2)  # CoNLL entity codes 1 and 2 refer to people\n",
128 |         "    ]\n",
129 |         "\n",
130 |         "def prepare_dataset(data_split, start: int, end: int) -> list[dspy.Example]:\n",
131 |         "    \"\"\"\n",
132 |         "    Prepares a sliced dataset split for use with DSPy.\n",
133 |         "    \n",
134 |         "    Args:\n",
135 |         "        data_split: The dataset split (e.g., train or test).\n",
136 |         "        start (int): Starting index of the slice.\n",
137 |         "        end (int): Ending index of the slice.\n",
138 |         "    \n",
139 |         "    Returns:\n",
140 |         "        list[dspy.Example]: List of DSPy Examples with tokens and expected labels.\n",
141 |         "    \"\"\"\n",
142 |         "    return [\n",
143 |         "        dspy.Example(\n",
144 |         "            tokens=row[\"tokens\"],\n",
145 |         "            expected_extracted_people=extract_people_entities(row)\n",
146 |         "        ).with_inputs(\"tokens\")\n",
147 |         "        for row in data_split.select(range(start, end))\n",
148 |         "    ]\n",
149 |         "\n",
150 |         "# Load the dataset\n",
151 |         "dataset = load_conll_dataset()\n",
152 |         "\n",
153 |         "# Prepare the training and test sets\n",
154 |         "train_set = prepare_dataset(dataset[\"train\"], 0, 50)\n",
155 |         "test_set = prepare_dataset(dataset[\"test\"], 0, 200)"
156 |       ]
157 |     },
158 |     {
159 |       "cell_type": "markdown",
160 |       "metadata": {},
161 |       "source": [
162 |         "## Configure DSPy and create an Entity Extraction Program\n",
163 |         "\n",
164 |         "Here, we define a DSPy program for extracting entities referring to people from tokenized text.\n",
165 |         "\n",
166 |         "Then, we configure DSPy to use a particular language model (`gpt-4o-mini`) for all invocations of the program.\n",
167 |         "\n",
168 |         "**Key DSPy Concepts Introduced:**\n",
169 |         "- **Signatures:** Define structured input/output schemas for your program.\n",
170 |         "- **Modules:** Encapsulate program logic in reusable, composable units.\n",
171 |         "\n",
172 |         "Specifically, we'll:\n",
173 |         "- Create a `PeopleExtraction` DSPy Signature to specify the input (`tokens`) and output (`extracted_people`) fields.\n",
174 |         "- Define a `people_extractor` program that uses DSPy's built-in `dspy.ChainOfThought` module to implement the `PeopleExtraction` signature. The program extracts entities referring to people from a list of input tokens using language model (LM) prompting.\n",
175 |         "- Use the `dspy.LM` class and `dspy.settings.configure()` method to configure the language model that DSPy will use when invoking the program."
176 |       ]
177 |     },
178 |     {
179 |       "cell_type": "code",
180 |       "execution_count": 3,
181 |       "metadata": {},
182 |       "outputs": [],
183 |       "source": [
184 |         "from typing import List\n",
185 |         "\n",
186 |         "class PeopleExtraction(dspy.Signature):\n",
187 |         "    \"\"\"\n",
188 |         "    Extract contiguous tokens referring to specific people, if any, from a list of string tokens.\n",
189 |         "    Output a list of tokens. In other words, do not combine multiple tokens into a single value.\n",
190 |         "    \"\"\"\n",
191 |         "    tokens: list[str] = dspy.InputField(desc=\"tokenized text\")\n",
192 |         "    extracted_people: list[str] = dspy.OutputField(desc=\"all tokens referring to specific people extracted from the tokenized text\")\n",
193 |         "\n",
194 |         "people_extractor = dspy.ChainOfThought(PeopleExtraction)"
195 |       ]
196 |     },
197 |     {
198 |       "cell_type": "markdown",
199 |       "metadata": {},
200 |       "source": [
201 |         "Here, we tell DSPy to use OpenAI's `gpt-4o-mini` model in our program. To authenticate, DSPy reads your `OPENAI_API_KEY`. You can easily swap this out for [other providers or local models](https://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb)."
202 |       ]
203 |     },
204 |     {
205 |       "cell_type": "code",
206 |       "execution_count": 4,
207 |       "metadata": {},
208 |       "outputs": [],
209 |       "source": [
210 |         "lm = dspy.LM(model=\"openai/gpt-4o-mini\")\n",
211 |         "dspy.settings.configure(lm=lm)"
212 |       ]
213 |     },
214 |     {
215 |       "cell_type": "markdown",
216 |       "metadata": {},
217 |       "source": [
218 |         "## Define Metric and Evaluation Functions\n",
219 |         "\n",
220 |         "In DSPy, evaluating a program's performance is critical for iterative development. A good evaluation framework allows us to:\n",
221 |         "- Measure the quality of our program's outputs.\n",
222 |         "- Compare outputs against ground-truth labels.\n",
223 |         "- Identify areas for improvement.\n",
224 |         "\n",
225 |         "**What We'll Do:**\n",
226 |         "- Define a custom metric (`extraction_correctness_metric`) to evaluate whether the extracted entities match the ground truth.\n",
227 |         "- Create an evaluation function (`evaluate_correctness`) to apply this metric to a training or test dataset and compute the overall accuracy.\n",
228 |         "\n",
229 |         "The evaluation function uses DSPy's `Evaluate` utility to handle parallelism and visualization of results."
230 |       ]
231 |     },
232 |     {
233 |       "cell_type": "code",
234 |       "execution_count": 5,
235 |       "metadata": {},
236 |       "outputs": [],
237 |       "source": [
238 |         "def extraction_correctness_metric(example: dspy.Example, prediction: dspy.Prediction, trace=None) -> bool:\n",
239 |         "    \"\"\"\n",
240 |         "    Computes correctness of entity extraction predictions.\n",
241 |         "    \n",
242 |         "    Args:\n",
243 |         "        example (dspy.Example): The dataset example containing expected people entities.\n",
244 |         "        prediction (dspy.Prediction): The prediction from the DSPy people extraction program.\n",
245 |         "        trace: Optional trace object for debugging.\n",
246 |         "    \n",
247 |         "    Returns:\n",
248 |         "        bool: True if predictions match expectations, False otherwise.\n",
249 |         "    \"\"\"\n",
250 |         "    return prediction.extracted_people == example.expected_extracted_people\n",
251 |         "\n",
252 |         "evaluate_correctness = dspy.Evaluate(\n",
253 |         "    devset=test_set,\n",
254 |         "    metric=extraction_correctness_metric,\n",
255 |         "    num_threads=24,\n",
256 |         "    display_progress=True,\n",
257 |         "    display_table=True\n",
258 |         ")"
259 |       ]
260 |     },
261 |     {
262 |       "cell_type": "markdown",
263 |       "metadata": {},
264 |       "source": [
265 |         "## Evaluate Initial Extractor\n",
266 |         "\n",
267 |         "Before optimizing our program, we need a baseline evaluation to understand its current performance. This helps us:\n",
268 |         "- Establish a reference point for comparison after optimization.\n",
269 |         "- Identify potential weaknesses in the initial implementation.\n",
270 |         "\n",
271 |         "In this step, we'll run our `people_extractor` program on the test set and measure its accuracy using the evaluation framework defined earlier."
272 |       ]
273 |     },
274 |     {
275 |       "cell_type": "code",
276 |       "execution_count": 6,
277 |       "metadata": {},
278 |       "outputs": [
279 |         {
280 |           "name": "stdout",
281 |           "output_type": "stream",
282 |           "text": [
283 |             "Average Metric: 172.00 / 200 (86.0%): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:16<00:00, 11.94it/s]"
284 |           ]
285 |         },
286 |         {
287 |           "name": "stderr",
288 |           "output_type": "stream",
289 |           "text": [
290 |             "2024/11/18 21:08:04 INFO dspy.evaluate.evaluate: Average Metric: 172 / 200 (86.0%)\n"
291 |           ]
292 |         },
293 |         {
294 |           "name": "stdout",
295 |           "output_type": "stream",
296 |           "text": [
297 |             "\n"
298 |           ]
299 |         },
300 |         {
301 |           "data": {
302 |             "text/html": [
303 |               "<div>\n",
304 |               "<style scoped>\n",
305 |               "    .dataframe tbody tr th:only-of-type {\n",
306 |               "        vertical-align: middle;\n",
307 |               "    }\n",
308 |               "\n",
309 |               "    .dataframe tbody tr th {\n",
310 |               "        vertical-align: top;\n",
311 |               "    }\n",
312 |               "\n",
313 |               "    .dataframe thead th {\n",
314 |               "        text-align: right;\n",
315 |               "    }\n",
316 |               "</style>\n",
317 |               "<table border=\"1\" class=\"dataframe\">\n",
318 |               "  <thead>\n",
319 |               "    <tr style=\"text-align: right;\">\n",
320 |               "      <th></th>\n",
321 |               "      <th>tokens</th>\n",
322 |               "      <th>expected_extracted_people</th>\n",
323 |               "      <th>rationale</th>\n",
324 |               "      <th>extracted_people</th>\n",
325 |               "      <th>extraction_correctness_metric</th>\n",
326 |               "    </tr>\n",
327 |               "  </thead>\n",
328 |               "  <tbody>\n",
329 |               "    <tr>\n",
330 |               "      <th>0</th>\n",
331 |               "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT...</td>\n",
332 |               "      <td>[CHINA]</td>\n",
333 |               "      <td>We extracted \"JAPAN\" and \"CHINA\" as they refer to specific countri...</td>\n",
334 |               "      <td>[JAPAN, CHINA]</td>\n",
335 |               "      <td></td>\n",
336 |               "    </tr>\n",
337 |               "    <tr>\n",
338 |               "      <th>1</th>\n",
339 |               "      <td>[Nadim, Ladki]</td>\n",
340 |               "      <td>[Nadim, Ladki]</td>\n",
341 |               "      <td>We extracted the tokens \"Nadim\" and \"Ladki\" as they refer to speci...</td>\n",
342 |               "      <td>[Nadim, Ladki]</td>\n",
343 |               "      <td>✔️ [True]</td>\n",
344 |               "    </tr>\n",
345 |               "    <tr>\n",
346 |               "      <th>2</th>\n",
347 |               "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
348 |               "      <td>[]</td>\n",
349 |               "      <td>There are no tokens referring to specific people in the provided l...</td>\n",
350 |               "      <td>[]</td>\n",
351 |               "      <td>✔️ [True]</td>\n",
352 |               "    </tr>\n",
353 |               "    <tr>\n",
354 |               "      <th>3</th>\n",
355 |               "      <td>[Japan, began, the, defence, of, their, Asian, Cup, title, with, a...</td>\n",
356 |               "      <td>[]</td>\n",
357 |               "      <td>We did not find any tokens referring to specific people in the pro...</td>\n",
358 |               "      <td>[]</td>\n",
359 |               "      <td>✔️ [True]</td>\n",
360 |               "    </tr>\n",
361 |               "    <tr>\n",
362 |               "      <th>4</th>\n",
363 |               "      <td>[But, China, saw, their, luck, desert, them, in, the, second, matc...</td>\n",
364 |               "      <td>[]</td>\n",
365 |               "      <td>The extracted tokens referring to specific people are \"China\" and ...</td>\n",
366 |               "      <td>[China, Uzbekistan]</td>\n",
367 |               "      <td></td>\n",
368 |               "    </tr>\n",
369 |               "    <tr>\n",
370 |               "      <th>...</th>\n",
371 |               "      <td>...</td>\n",
372 |               "      <td>...</td>\n",
373 |               "      <td>...</td>\n",
374 |               "      <td>...</td>\n",
375 |               "      <td>...</td>\n",
376 |               "    </tr>\n",
377 |               "    <tr>\n",
378 |               "      <th>195</th>\n",
379 |               "      <td>['The', 'Wallabies', 'have', 'their', 'sights', 'set', 'on', 'a', ...</td>\n",
380 |               "      <td>[David, Campese]</td>\n",
381 |               "      <td>The extracted_people includes \"David Campese\" as it refers to a sp...</td>\n",
382 |               "      <td>[David, Campese]</td>\n",
383 |               "      <td>✔️ [True]</td>\n",
384 |               "    </tr>\n",
385 |               "    <tr>\n",
386 |               "      <th>196</th>\n",
387 |               "      <td>['The', 'Wallabies', 'currently', 'have', 'no', 'plans', 'to', 'ma...</td>\n",
388 |               "      <td>[]</td>\n",
389 |               "      <td>The extracted_people includes \"Wallabies\" as it refers to a specif...</td>\n",
390 |               "      <td>[]</td>\n",
391 |               "      <td>✔️ [True]</td>\n",
392 |               "    </tr>\n",
393 |               "    <tr>\n",
394 |               "      <th>197</th>\n",
395 |               "      <td>['Campese', 'will', 'be', 'up', 'against', 'a', 'familiar', 'foe',...</td>\n",
396 |               "      <td>[Campese, Rob, Andrew]</td>\n",
397 |               "      <td>The extracted tokens refer to specific people mentioned in the tex...</td>\n",
398 |               "      <td>[Campese, Rob, Andrew]</td>\n",
399 |               "      <td>✔️ [True]</td>\n",
400 |               "    </tr>\n",
401 |               "    <tr>\n",
402 |               "      <th>198</th>\n",
403 |               "      <td>['\"', 'Campo', 'has', 'a', 'massive', 'following', 'in', 'this', '...</td>\n",
404 |               "      <td>[Campo, Andrew]</td>\n",
405 |               "      <td>The extracted tokens referring to specific people include \"Campo\" ...</td>\n",
406 |               "      <td>[Campo, Andrew]</td>\n",
407 |               "      <td>✔️ [True]</td>\n",
408 |               "    </tr>\n",
409 |               "    <tr>\n",
410 |               "      <th>199</th>\n",
411 |               "      <td>['On', 'tour', ',', 'Australia', 'have', 'won', 'all', 'four', 'te...</td>\n",
412 |               "      <td>[]</td>\n",
413 |               "      <td>We extracted the names of specific people from the tokenized text....</td>\n",
414 |               "      <td>[]</td>\n",
415 |               "      <td>✔️ [True]</td>\n",
416 |               "    </tr>\n",
417 |               "  </tbody>\n",
418 |               "</table>\n",
419 |               "<p>200 rows × 5 columns</p>\n",
420 |               "</div>"
421 |             ],
422 |             "text/plain": [
423 |               "                                                                    tokens  \\\n",
424 |               "0    [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT...   \n",
425 |               "1                                                           [Nadim, Ladki]   \n",
426 |               "2                          [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
427 |               "3    [Japan, began, the, defence, of, their, Asian, Cup, title, with, a...   \n",
428 |               "4    [But, China, saw, their, luck, desert, them, in, the, second, matc...   \n",
429 |               "..                                                                     ...   \n",
430 |               "195  ['The', 'Wallabies', 'have', 'their', 'sights', 'set', 'on', 'a', ...   \n",
431 |               "196  ['The', 'Wallabies', 'currently', 'have', 'no', 'plans', 'to', 'ma...   \n",
432 |               "197  ['Campese', 'will', 'be', 'up', 'against', 'a', 'familiar', 'foe',...   \n",
433 |               "198  ['\"', 'Campo', 'has', 'a', 'massive', 'following', 'in', 'this', '...   \n",
434 |               "199  ['On', 'tour', ',', 'Australia', 'have', 'won', 'all', 'four', 'te...   \n",
435 |               "\n",
436 |               "    expected_extracted_people  \\\n",
437 |               "0                     [CHINA]   \n",
438 |               "1              [Nadim, Ladki]   \n",
439 |               "2                          []   \n",
440 |               "3                          []   \n",
441 |               "4                          []   \n",
442 |               "..                        ...   \n",
443 |               "195          [David, Campese]   \n",
444 |               "196                        []   \n",
445 |               "197    [Campese, Rob, Andrew]   \n",
446 |               "198           [Campo, Andrew]   \n",
447 |               "199                        []   \n",
448 |               "\n",
449 |               "                                                                 rationale  \\\n",
450 |               "0    We extracted \"JAPAN\" and \"CHINA\" as they refer to specific countri...   \n",
451 |               "1    We extracted the tokens \"Nadim\" and \"Ladki\" as they refer to speci...   \n",
452 |               "2    There are no tokens referring to specific people in the provided l...   \n",
453 |               "3    We did not find any tokens referring to specific people in the pro...   \n",
454 |               "4    The extracted tokens referring to specific people are \"China\" and ...   \n",
455 |               "..                                                                     ...   \n",
456 |               "195  The extracted_people includes \"David Campese\" as it refers to a sp...   \n",
457 |               "196  The extracted_people includes \"Wallabies\" as it refers to a specif...   \n",
458 |               "197  The extracted tokens refer to specific people mentioned in the tex...   \n",
459 |               "198  The extracted tokens referring to specific people include \"Campo\" ...   \n",
460 |               "199  We extracted the names of specific people from the tokenized text....   \n",
461 |               "\n",
462 |               "           extracted_people extraction_correctness_metric  \n",
463 |               "0            [JAPAN, CHINA]                                \n",
464 |               "1            [Nadim, Ladki]                     ✔️ [True]  \n",
465 |               "2                        []                     ✔️ [True]  \n",
466 |               "3                        []                     ✔️ [True]  \n",
467 |               "4       [China, Uzbekistan]                                \n",
468 |               "..                      ...                           ...  \n",
469 |               "195        [David, Campese]                     ✔️ [True]  \n",
470 |               "196                      []                     ✔️ [True]  \n",
471 |               "197  [Campese, Rob, Andrew]                     ✔️ [True]  \n",
472 |               "198         [Campo, Andrew]                     ✔️ [True]  \n",
473 |               "199                      []                     ✔️ [True]  \n",
474 |               "\n",
475 |               "[200 rows x 5 columns]"
476 |             ]
477 |           },
478 |           "metadata": {},
479 |           "output_type": "display_data"
480 |         },
481 |         {
482 |           "data": {
483 |             "text/plain": [
484 |               "86.0"
485 |             ]
486 |           },
487 |           "execution_count": 6,
488 |           "metadata": {},
489 |           "output_type": "execute_result"
490 |         }
491 |       ],
492 |       "source": [
493 |         "evaluate_correctness(people_extractor, devset=test_set)"
494 |       ]
495 |     },
496 |     {
497 |       "cell_type": "markdown",
498 |       "metadata": {},
499 |       "source": [
500 |         "<details>\n",
501 |         "<summary>Tracking Evaluation Results in MLflow Experiment</summary>\n",
502 |         "\n",
503 |         "<br/>\n",
504 |         "\n",
505 |         "To track and visualize the evaluation results over time, you can record the results in MLflow Experiment.\n",
506 |         "\n",
507 |         "\n",
508 |         "```python\n",
509 |         "import mlflow\n",
510 |         "\n",
511 |         "with mlflow.start_run(run_name=\"extractor_evaluation\"):\n",
512 |         "    evaluate_correctness = dspy.Evaluate(\n",
513 |         "        devset=test_set,\n",
514 |         "        metric=extraction_correctness_metric,\n",
515 |         "        num_threads=24,\n",
516 |         "        display_progress=True,\n",
517 |         "    )\n",
518 |         "\n",
519 |         "    # Evaluate the program as usual\n",
520 |         "    result = evaluate_correctness(people_extractor)\n",
521 |         "\n",
522 |         "    # Log the aggregated score\n",
523 |         "    mlflow.log_metric(\"exact_match\", result.score)\n",
524 |         "    # Log the detailed evaluation results as a table\n",
525 |         "    mlflow.log_table(\n",
526 |         "        {\n",
527 |         "            \"Tokens\": [example.tokens for example in test_set],\n",
528 |         "            \"Expected\": [example.expected_extracted_people for example in test_set],\n",
529 |         "            \"Predicted\": [output[1] for output in result.results],\n",
530 |         "            \"Exact match\": [output[2] for output in result.results],\n",
531 |         "        },\n",
532 |         "        artifact_file=\"eval_results.json\",\n",
533 |         "    )\n",
534 |         "```\n",
535 |         "\n",
536 |         "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
537 |         "\n",
538 |         "</details>"
539 |       ]
540 |     },
541 |     {
542 |       "cell_type": "markdown",
543 |       "metadata": {},
544 |       "source": [
545 |         "## Optimize the Model\n",
546 |         "\n",
547 |         "DSPy includes powerful optimizers that can improve the quality of your system.\n",
548 |         "\n",
549 |         "Here, we use DSPy's `MIPROv2` optimizer to:\n",
550 |         "- Automatically tune the program's language model (LM) prompt by 1. using the LM to adjust the prompt's instructions and 2. building few-shot examples from the training dataset that are augmented with reasoning generated from `dspy.ChainOfThought`.\n",
551 |         "- Maximize correctness on the training set.\n",
552 |         "\n",
553 |         "This optimization process is automated, saving time and effort while improving accuracy."
554 |       ]
555 |     },
556 |     {
557 |       "cell_type": "code",
558 |       "execution_count": null,
559 |       "metadata": {},
560 |       "outputs": [],
561 |       "source": [
562 |         "mipro_optimizer = dspy.MIPROv2(\n",
563 |         "    metric=extraction_correctness_metric,\n",
564 |         "    auto=\"medium\",\n",
565 |         ")\n",
566 |         "optimized_people_extractor = mipro_optimizer.compile(\n",
567 |         "    people_extractor,\n",
568 |         "    trainset=train_set,\n",
569 |         "    max_bootstrapped_demos=4,\n",
570 |         "    minibatch=False\n",
571 |         ")"
572 |       ]
573 |     },
574 |     {
575 |       "cell_type": "markdown",
576 |       "metadata": {},
577 |       "source": [
578 |         "## Evaluate Optimized Program\n",
579 |         "\n",
580 |         "After optimization, we re-evaluate the program on the test set to measure improvements. Comparing the optimized and initial results allows us to:\n",
581 |         "- Quantify the benefits of optimization.\n",
582 |         "- Validate that the program generalizes well to unseen data.\n",
583 |         "\n",
584 |         "In this case, we see that accuracy of the program on the test dataset has improved significantly."
585 |       ]
586 |     },
587 |     {
588 |       "cell_type": "code",
589 |       "execution_count": 8,
590 |       "metadata": {},
591 |       "outputs": [
592 |         {
593 |           "name": "stdout",
594 |           "output_type": "stream",
595 |           "text": [
596 |             "Average Metric: 186.00 / 200 (93.0%): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:23<00:00,  8.58it/s]"
597 |           ]
598 |         },
599 |         {
600 |           "name": "stderr",
601 |           "output_type": "stream",
602 |           "text": [
603 |             "2024/11/18 21:15:00 INFO dspy.evaluate.evaluate: Average Metric: 186 / 200 (93.0%)\n"
604 |           ]
605 |         },
606 |         {
607 |           "name": "stdout",
608 |           "output_type": "stream",
609 |           "text": [
610 |             "\n"
611 |           ]
612 |         },
613 |         {
614 |           "data": {
615 |             "text/html": [
616 |               "<div>\n",
617 |               "<style scoped>\n",
618 |               "    .dataframe tbody tr th:only-of-type {\n",
619 |               "        vertical-align: middle;\n",
620 |               "    }\n",
621 |               "\n",
622 |               "    .dataframe tbody tr th {\n",
623 |               "        vertical-align: top;\n",
624 |               "    }\n",
625 |               "\n",
626 |               "    .dataframe thead th {\n",
627 |               "        text-align: right;\n",
628 |               "    }\n",
629 |               "</style>\n",
630 |               "<table border=\"1\" class=\"dataframe\">\n",
631 |               "  <thead>\n",
632 |               "    <tr style=\"text-align: right;\">\n",
633 |               "      <th></th>\n",
634 |               "      <th>tokens</th>\n",
635 |               "      <th>expected_extracted_people</th>\n",
636 |               "      <th>rationale</th>\n",
637 |               "      <th>extracted_people</th>\n",
638 |               "      <th>extraction_correctness_metric</th>\n",
639 |               "    </tr>\n",
640 |               "  </thead>\n",
641 |               "  <tbody>\n",
642 |               "    <tr>\n",
643 |               "      <th>0</th>\n",
644 |               "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT...</td>\n",
645 |               "      <td>[CHINA]</td>\n",
646 |               "      <td>There are no specific people mentioned in the provided tokens. The...</td>\n",
647 |               "      <td>[]</td>\n",
648 |               "      <td></td>\n",
649 |               "    </tr>\n",
650 |               "    <tr>\n",
651 |               "      <th>1</th>\n",
652 |               "      <td>[Nadim, Ladki]</td>\n",
653 |               "      <td>[Nadim, Ladki]</td>\n",
654 |               "      <td>The tokens \"Nadim Ladki\" refer to a specific individual. Both toke...</td>\n",
655 |               "      <td>[Nadim, Ladki]</td>\n",
656 |               "      <td>✔️ [True]</td>\n",
657 |               "    </tr>\n",
658 |               "    <tr>\n",
659 |               "      <th>2</th>\n",
660 |               "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
661 |               "      <td>[]</td>\n",
662 |               "      <td>There are no tokens referring to specific people in the provided l...</td>\n",
663 |               "      <td>[]</td>\n",
664 |               "      <td>✔️ [True]</td>\n",
665 |               "    </tr>\n",
666 |               "    <tr>\n",
667 |               "      <th>3</th>\n",
668 |               "      <td>[Japan, began, the, defence, of, their, Asian, Cup, title, with, a...</td>\n",
669 |               "      <td>[]</td>\n",
670 |               "      <td>There are no specific people mentioned in the provided tokens. The...</td>\n",
671 |               "      <td>[]</td>\n",
672 |               "      <td>✔️ [True]</td>\n",
673 |               "    </tr>\n",
674 |               "    <tr>\n",
675 |               "      <th>4</th>\n",
676 |               "      <td>[But, China, saw, their, luck, desert, them, in, the, second, matc...</td>\n",
677 |               "      <td>[]</td>\n",
678 |               "      <td>There are no tokens referring to specific people in the provided l...</td>\n",
679 |               "      <td>[]</td>\n",
680 |               "      <td>✔️ [True]</td>\n",
681 |               "    </tr>\n",
682 |               "    <tr>\n",
683 |               "      <th>...</th>\n",
684 |               "      <td>...</td>\n",
685 |               "      <td>...</td>\n",
686 |               "      <td>...</td>\n",
687 |               "      <td>...</td>\n",
688 |               "      <td>...</td>\n",
689 |               "    </tr>\n",
690 |               "    <tr>\n",
691 |               "      <th>195</th>\n",
692 |               "      <td>['The', 'Wallabies', 'have', 'their', 'sights', 'set', 'on', 'a', ...</td>\n",
693 |               "      <td>[David, Campese]</td>\n",
694 |               "      <td>The extracted tokens refer to a specific person mentioned in the t...</td>\n",
695 |               "      <td>[David, Campese]</td>\n",
696 |               "      <td>✔️ [True]</td>\n",
697 |               "    </tr>\n",
698 |               "    <tr>\n",
699 |               "      <th>196</th>\n",
700 |               "      <td>['The', 'Wallabies', 'currently', 'have', 'no', 'plans', 'to', 'ma...</td>\n",
701 |               "      <td>[]</td>\n",
702 |               "      <td>There are no specific individuals mentioned in the provided tokens...</td>\n",
703 |               "      <td>[]</td>\n",
704 |               "      <td>✔️ [True]</td>\n",
705 |               "    </tr>\n",
706 |               "    <tr>\n",
707 |               "      <th>197</th>\n",
708 |               "      <td>['Campese', 'will', 'be', 'up', 'against', 'a', 'familiar', 'foe',...</td>\n",
709 |               "      <td>[Campese, Rob, Andrew]</td>\n",
710 |               "      <td>The tokens include the names \"Campese\" and \"Rob Andrew,\" both of w...</td>\n",
711 |               "      <td>[Campese, Rob, Andrew]</td>\n",
712 |               "      <td>✔️ [True]</td>\n",
713 |               "    </tr>\n",
714 |               "    <tr>\n",
715 |               "      <th>198</th>\n",
716 |               "      <td>['\"', 'Campo', 'has', 'a', 'massive', 'following', 'in', 'this', '...</td>\n",
717 |               "      <td>[Campo, Andrew]</td>\n",
718 |               "      <td>The extracted tokens refer to specific people mentioned in the tex...</td>\n",
719 |               "      <td>[Campo, Andrew]</td>\n",
720 |               "      <td>✔️ [True]</td>\n",
721 |               "    </tr>\n",
722 |               "    <tr>\n",
723 |               "      <th>199</th>\n",
724 |               "      <td>['On', 'tour', ',', 'Australia', 'have', 'won', 'all', 'four', 'te...</td>\n",
725 |               "      <td>[]</td>\n",
726 |               "      <td>There are no specific people mentioned in the provided tokens. The...</td>\n",
727 |               "      <td>[]</td>\n",
728 |               "      <td>✔️ [True]</td>\n",
729 |               "    </tr>\n",
730 |               "  </tbody>\n",
731 |               "</table>\n",
732 |               "<p>200 rows × 5 columns</p>\n",
733 |               "</div>"
734 |             ],
735 |             "text/plain": [
736 |               "                                                                    tokens  \\\n",
737 |               "0    [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT...   \n",
738 |               "1                                                           [Nadim, Ladki]   \n",
739 |               "2                          [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
740 |               "3    [Japan, began, the, defence, of, their, Asian, Cup, title, with, a...   \n",
741 |               "4    [But, China, saw, their, luck, desert, them, in, the, second, matc...   \n",
742 |               "..                                                                     ...   \n",
743 |               "195  ['The', 'Wallabies', 'have', 'their', 'sights', 'set', 'on', 'a', ...   \n",
744 |               "196  ['The', 'Wallabies', 'currently', 'have', 'no', 'plans', 'to', 'ma...   \n",
745 |               "197  ['Campese', 'will', 'be', 'up', 'against', 'a', 'familiar', 'foe',...   \n",
746 |               "198  ['\"', 'Campo', 'has', 'a', 'massive', 'following', 'in', 'this', '...   \n",
747 |               "199  ['On', 'tour', ',', 'Australia', 'have', 'won', 'all', 'four', 'te...   \n",
748 |               "\n",
749 |               "    expected_extracted_people  \\\n",
750 |               "0                     [CHINA]   \n",
751 |               "1              [Nadim, Ladki]   \n",
752 |               "2                          []   \n",
753 |               "3                          []   \n",
754 |               "4                          []   \n",
755 |               "..                        ...   \n",
756 |               "195          [David, Campese]   \n",
757 |               "196                        []   \n",
758 |               "197    [Campese, Rob, Andrew]   \n",
759 |               "198           [Campo, Andrew]   \n",
760 |               "199                        []   \n",
761 |               "\n",
762 |               "                                                                 rationale  \\\n",
763 |               "0    There are no specific people mentioned in the provided tokens. The...   \n",
764 |               "1    The tokens \"Nadim Ladki\" refer to a specific individual. Both toke...   \n",
765 |               "2    There are no tokens referring to specific people in the provided l...   \n",
766 |               "3    There are no specific people mentioned in the provided tokens. The...   \n",
767 |               "4    There are no tokens referring to specific people in the provided l...   \n",
768 |               "..                                                                     ...   \n",
769 |               "195  The extracted tokens refer to a specific person mentioned in the t...   \n",
770 |               "196  There are no specific individuals mentioned in the provided tokens...   \n",
771 |               "197  The tokens include the names \"Campese\" and \"Rob Andrew,\" both of w...   \n",
772 |               "198  The extracted tokens refer to specific people mentioned in the tex...   \n",
773 |               "199  There are no specific people mentioned in the provided tokens. The...   \n",
774 |               "\n",
775 |               "           extracted_people extraction_correctness_metric  \n",
776 |               "0                        []                                \n",
777 |               "1            [Nadim, Ladki]                     ✔️ [True]  \n",
778 |               "2                        []                     ✔️ [True]  \n",
779 |               "3                        []                     ✔️ [True]  \n",
780 |               "4                        []                     ✔️ [True]  \n",
781 |               "..                      ...                           ...  \n",
782 |               "195        [David, Campese]                     ✔️ [True]  \n",
783 |               "196                      []                     ✔️ [True]  \n",
784 |               "197  [Campese, Rob, Andrew]                     ✔️ [True]  \n",
785 |               "198         [Campo, Andrew]                     ✔️ [True]  \n",
786 |               "199                      []                     ✔️ [True]  \n",
787 |               "\n",
788 |               "[200 rows x 5 columns]"
789 |             ]
790 |           },
791 |           "metadata": {},
792 |           "output_type": "display_data"
793 |         },
794 |         {
795 |           "data": {
796 |             "text/plain": [
797 |               "93.0"
798 |             ]
799 |           },
800 |           "execution_count": 8,
801 |           "metadata": {},
802 |           "output_type": "execute_result"
803 |         }
804 |       ],
805 |       "source": [
806 |         "evaluate_correctness(optimized_people_extractor, devset=test_set)"
807 |       ]
808 |     },
809 |     {
810 |       "cell_type": "markdown",
811 |       "metadata": {},
812 |       "source": [
813 |         "## Inspect Optimized Program's Prompt\n",
814 |         "\n",
815 |         "After optimizing the program, we can inspect the history of interactions to see how DSPy has augmented the program's prompt with few-shot examples. This step demonstrates:\n",
816 |         "- The structure of the prompt used by the program.\n",
817 |         "- How few-shot examples are added to guide the model's behavior.\n",
818 |         "\n",
819 |         "Use `inspect_history(n=1)` to view the last interaction and analyze the generated prompt."
820 |       ]
821 |     },
822 |     {
823 |       "cell_type": "code",
824 |       "execution_count": 9,
825 |       "metadata": {},
826 |       "outputs": [
827 |         {
828 |           "name": "stdout",
829 |           "output_type": "stream",
830 |           "text": [
831 |             "\n",
832 |             "\n",
833 |             "\n",
834 |             "\n",
835 |             "\u001b[34m[2024-11-18T21:15:00.584497]\u001b[0m\n",
836 |             "\n",
837 |             "\u001b[31mSystem message:\u001b[0m\n",
838 |             "\n",
839 |             "Your input fields are:\n",
840 |             "1. `tokens` (list[str]): tokenized text\n",
841 |             "\n",
842 |             "Your output fields are:\n",
843 |             "1. `rationale` (str): ${produce the extracted_people}. We ...\n",
844 |             "2. `extracted_people` (list[str]): all tokens referring to specific people extracted from the tokenized text\n",
845 |             "\n",
846 |             "All interactions will be structured in the following way, with the appropriate values filled in.\n",
847 |             "\n",
848 |             "[[ ## tokens ## ]]\n",
849 |             "{tokens}\n",
850 |             "\n",
851 |             "[[ ## rationale ## ]]\n",
852 |             "{rationale}\n",
853 |             "\n",
854 |             "[[ ## extracted_people ## ]]\n",
855 |             "{extracted_people}        # note: the value you produce must be pareseable according to the following JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
856 |             "\n",
857 |             "[[ ## completed ## ]]\n",
858 |             "\n",
859 |             "In adhering to this structure, your objective is: \n",
860 |             "        In a high-stakes situation where accurate identification of individuals is critical for regulatory compliance and public health communication, extract contiguous tokens referring to specific people from the provided list of string tokens. Ensure that you output each identified individual as separate tokens without combining multiple tokens into a single value. This task is essential for ensuring clarity and accountability in communications pertaining to EU regulations and health matters.\n",
861 |             "\n",
862 |             "\n",
863 |             "\u001b[31mUser message:\u001b[0m\n",
864 |             "\n",
865 |             "[[ ## tokens ## ]]\n",
866 |             "[\"He\", \"said\", \"further\", \"scientific\", \"study\", \"was\", \"required\", \"and\", \"if\", \"it\", \"was\", \"found\", \"that\", \"action\", \"was\", \"needed\", \"it\", \"should\", \"be\", \"taken\", \"by\", \"the\", \"European\", \"Union\", \".\"]\n",
867 |             "\n",
868 |             "Respond with the corresponding output fields, starting with the field `[[ ## rationale ## ]]`, then `[[ ## extracted_people ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
869 |             "\n",
870 |             "\n",
871 |             "\u001b[31mAssistant message:\u001b[0m\n",
872 |             "\n",
873 |             "[[ ## rationale ## ]]\n",
874 |             "There are no specific people mentioned in the provided tokens. The only reference is to the \"European Union,\" which is an organization rather than a specific person. We therefore conclude that there are no extracted people.\n",
875 |             "\n",
876 |             "[[ ## extracted_people ## ]]\n",
877 |             "[]\n",
878 |             "\n",
879 |             "[[ ## completed ## ]]\n",
880 |             "\n",
881 |             "\n",
882 |             "\u001b[31mUser message:\u001b[0m\n",
883 |             "\n",
884 |             "[[ ## tokens ## ]]\n",
885 |             "[\"BRUSSELS\", \"1996-08-22\"]\n",
886 |             "\n",
887 |             "Respond with the corresponding output fields, starting with the field `[[ ## rationale ## ]]`, then `[[ ## extracted_people ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
888 |             "\n",
889 |             "\n",
890 |             "\u001b[31mAssistant message:\u001b[0m\n",
891 |             "\n",
892 |             "[[ ## rationale ## ]]\n",
893 |             "There are no tokens referring to specific people in the provided list. We only have a location and a date.\n",
894 |             "\n",
895 |             "[[ ## extracted_people ## ]]\n",
896 |             "[]\n",
897 |             "\n",
898 |             "[[ ## completed ## ]]\n",
899 |             "\n",
900 |             "\n",
901 |             "\u001b[31mUser message:\u001b[0m\n",
902 |             "\n",
903 |             "[[ ## tokens ## ]]\n",
904 |             "[\"Fischler\", \"proposed\", \"EU-wide\", \"measures\", \"after\", \"reports\", \"from\", \"Britain\", \"and\", \"France\", \"that\", \"under\", \"laboratory\", \"conditions\", \"sheep\", \"could\", \"contract\", \"Bovine\", \"Spongiform\", \"Encephalopathy\", \"(\", \"BSE\", \")\", \"--\", \"mad\", \"cow\", \"disease\", \".\"]\n",
905 |             "\n",
906 |             "Respond with the corresponding output fields, starting with the field `[[ ## rationale ## ]]`, then `[[ ## extracted_people ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
907 |             "\n",
908 |             "\n",
909 |             "\u001b[31mAssistant message:\u001b[0m\n",
910 |             "\n",
911 |             "[[ ## rationale ## ]]\n",
912 |             "The extracted tokens refer to specific people mentioned in the text. In this case, \"Fischler\" is the only token that refers to a specific person. We extract this token as it is contiguous and represents an individual involved in the proposed measures.\n",
913 |             "\n",
914 |             "[[ ## extracted_people ## ]]\n",
915 |             "[\"Fischler\"]\n",
916 |             "\n",
917 |             "[[ ## completed ## ]]\n",
918 |             "\n",
919 |             "\n",
920 |             "\u001b[31mUser message:\u001b[0m\n",
921 |             "\n",
922 |             "[[ ## tokens ## ]]\n",
923 |             "[\"Germany\", \"'s\", \"representative\", \"to\", \"the\", \"European\", \"Union\", \"'s\", \"veterinary\", \"committee\", \"Werner\", \"Zwingmann\", \"said\", \"on\", \"Wednesday\", \"consumers\", \"should\", \"buy\", \"sheepmeat\", \"from\", \"countries\", \"other\", \"than\", \"Britain\", \"until\", \"the\", \"scientific\", \"advice\", \"was\", \"clearer\", \".\"]\n",
924 |             "\n",
925 |             "Respond with the corresponding output fields, starting with the field `[[ ## rationale ## ]]`, then `[[ ## extracted_people ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
926 |             "\n",
927 |             "\n",
928 |             "\u001b[31mAssistant message:\u001b[0m\n",
929 |             "\n",
930 |             "[[ ## rationale ## ]]\n",
931 |             "We extracted the name \"Werner Zwingmann\" from the tokenized text as it refers to a specific person mentioned in the context of a statement regarding sheepmeat consumption.\n",
932 |             "\n",
933 |             "[[ ## extracted_people ## ]]\n",
934 |             "[\"Werner\", \"Zwingmann\"]\n",
935 |             "\n",
936 |             "[[ ## completed ## ]]\n",
937 |             "\n",
938 |             "\n",
939 |             "\u001b[31mUser message:\u001b[0m\n",
940 |             "\n",
941 |             "[[ ## tokens ## ]]\n",
942 |             "[\"LONDON\", \"1996-12-06\"]\n",
943 |             "\n",
944 |             "Respond with the corresponding output fields, starting with the field `[[ ## rationale ## ]]`, then `[[ ## extracted_people ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
945 |             "\n",
946 |             "\n",
947 |             "\u001b[31mResponse:\u001b[0m\n",
948 |             "\n",
949 |             "\u001b[32m[[ ## rationale ## ]]\n",
950 |             "There are no tokens referring to specific people in the provided list. The tokens only include a location and a date.\n",
951 |             "\n",
952 |             "[[ ## extracted_people ## ]]\n",
953 |             "[]\n",
954 |             "\n",
955 |             "[[ ## completed ## ]]\u001b[0m\n",
956 |             "\n",
957 |             "\n",
958 |             "\n",
959 |             "\n",
960 |             "\n"
961 |           ]
962 |         }
963 |       ],
964 |       "source": [
965 |         "dspy.inspect_history(n=1)"
966 |       ]
967 |     },
968 |     {
969 |       "cell_type": "markdown",
970 |       "metadata": {},
971 |       "source": [
972 |         "## Keeping an eye on cost\n",
973 |         "\n",
974 |         "DSPy allows you to track the cost of your programs. The following code demonstrates how to obtain the cost of all LM calls made by the DSPy extractor program so far."
975 |       ]
976 |     },
977 |     {
978 |       "cell_type": "code",
979 |       "execution_count": 10,
980 |       "metadata": {},
981 |       "outputs": [
982 |         {
983 |           "data": {
984 |             "text/plain": [
985 |               "0.26362742999999983"
986 |             ]
987 |           },
988 |           "execution_count": 10,
989 |           "metadata": {},
990 |           "output_type": "execute_result"
991 |         }
992 |       ],
993 |       "source": [
994 |         "cost = sum([x['cost'] for x in lm.history if x['cost'] is not None])  # cost in USD, as calculated by LiteLLM for certain providers\n",
995 |         "cost"
996 |       ]
997 |     },
998 |     {
999 |       "cell_type": "markdown",
1000 |       "metadata": {},
1001 |       "source": [
1002 |         "## Saving and Loading Optimized Programs\n",
1003 |         "\n",
1004 |         "DSPy supports saving and loading programs, enabling you to reuse optimized systems without the need to re-optimize from scratch. This feature is especially useful for deploying your programs in production environments or sharing them with collaborators.\n",
1005 |         "\n",
1006 |         "In this step, we'll save the optimized program to a file and demonstrate how to load it back for future use."
1007 |       ]
1008 |     },
1009 |     {
1010 |       "cell_type": "code",
1011 |       "execution_count": 11,
1012 |       "metadata": {},
1013 |       "outputs": [
1014 |         {
1015 |           "data": {
1016 |             "text/plain": [
1017 |               "['Marcello', 'Cuttitta']"
1018 |             ]
1019 |           },
1020 |           "execution_count": 11,
1021 |           "metadata": {},
1022 |           "output_type": "execute_result"
1023 |         }
1024 |       ],
1025 |       "source": [
1026 |         "optimized_people_extractor.save(\"optimized_extractor.json\")\n",
1027 |         "\n",
1028 |         "loaded_people_extractor = dspy.ChainOfThought(PeopleExtraction)\n",
1029 |         "loaded_people_extractor.load(\"optimized_extractor.json\")\n",
1030 |         "\n",
1031 |         "loaded_people_extractor(tokens=[\"Italy\", \"recalled\", \"Marcello\", \"Cuttitta\"]).extracted_people"
1032 |       ]
1033 |     },
1034 |     {
1035 |       "cell_type": "markdown",
1036 |       "metadata": {},
1037 |       "source": [
1038 |         "<details>\n",
1039 |         "<summary>Saving programs in MLflow Experiment</summary>\n",
1040 |         "\n",
1041 |         "<br/>\n",
1042 |         "\n",
1043 |         "Instead of saving the program to a local file, you can track it in MLflow for better reproducibility and collaboration.\n",
1044 |         "\n",
1045 |         "1. **Dependency Management**: MLflow automatically save the frozen environment metadata along with the program to ensure reproducibility.\n",
1046 |         "2. **Experiment Tracking**: With MLflow, you can track the program's performance and cost along with the program itself.\n",
1047 |         "3. **Collaboration**: You can share the program and results with your team members by sharing the MLflow experiment.\n",
1048 |         "\n",
1049 |         "To save the program in MLflow, run the following code:\n",
1050 |         "\n",
1051 |         "```python\n",
1052 |         "import mlflow\n",
1053 |         "\n",
1054 |         "# Start an MLflow Run and save the program\n",
[TRUNCATED]
```

tutorials/gepa_ai_program/index.md
```
1 | # Reflective Prompt Evolution with GEPA
2 | 
3 | This section introduces GEPA, a reflective prompt optimizer for DSPy. GEPA works by leveraging LM's ability to reflect on the DSPy program's trajectory, identifying what went well, what didn't, and what can be improved. Based on this reflection, GEPA proposes new prompts, building a tree of evolved prompt candidates, accumulating improvements as the optimization progresses. Since GEPA can leverage domain-specific text feedback (as opposed to only the scalar metric), GEPA can often propose high performing prompts in very few rollouts. GEPA was introduced in the paper [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457) and available as `dspy.GEPA` which internally uses the GEPA implementation provided in [gepa-ai/gepa](https://github.com/gepa-ai/gepa).
4 | 
5 | ## `dspy.GEPA` Tutorials
6 | 
7 | ### [GEPA for AIME (Math)](../gepa_aime/index.ipynb)
8 | 
9 | This tutorial explores how GEPA can optimize a single `dspy.ChainOfThought` based program to achieve 10% gains on AIME 2025 with GPT-4.1 Mini!
10 | 
11 | ### [GEPA for Structured Information Extraction for Enterprise Tasks](../gepa_facilitysupportanalyzer/index.ipynb)
12 | 
13 | This tutorial explores how GEPA leverages predictor-level feedback to improve GPT-4.1 Nano's performance on a three-part task for structured information extraction and classification in an enterprise setting.
14 | 
15 | ### [GEPA for Privacy-Conscious Delegation](../gepa_papillon/index.ipynb)
16 | 
17 | This tutorial explores how GEPA can improve rapidly in as few as 1 iteration, while leveraging a simple feedback provided by a LLM-as-a-judge metric. The tutorial also explores how GEPA benefits from the textual feedback showing a breakdown of aggregate metrics into sub-components, allowing the reflection LM to identify what aspects of the task need improvement.
```

tutorials/gepa_aime/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "id": "23b9ab7f",
6 |    "metadata": {},
7 |    "source": [
8 |     "# Tutorial: GEPA for AIME (Math)\n",
9 |     "In this tutorial, we optimize GPT-4.1 Mini's Chain of Thought (`dspy.ChainOfThought`) for solving math problems (AIME) using the `dspy.GEPA` optimizer!"
10 |    ]
11 |   },
12 |   {
13 |    "cell_type": "markdown",
14 |    "id": "782f0cf1",
15 |    "metadata": {},
16 |    "source": [
17 |     "<details>\n",
18 |     "<summary>Recommended: Set up MLflow Autologging to understand what's happening under the hood.</summary>\n",
19 |     "\n",
20 |     "### MLflow DSPy Integration\n",
21 |     "\n",
22 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. MLflow's autologging capability automatically tracks progress of GEPA optimization, as well as visualizes prompts and module executions as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
23 |     "\n",
24 |     "**Visualize module executions as traces**\n",
25 |     "\n",
26 |     "![MLflow Trace](./mlflow-tracing-gepa-aime.png)\n",
27 |     "\n",
28 |     "**Automatically track optimization progress and results**\n",
29 |     "\n",
30 |     "![MLflow Tracking](./mlflow-tracking-gepa-aime-optimization.png)\n",
31 |     "\n",
32 |     "\n",
33 |     "**Setup MLflow**\n",
34 |     "\n",
35 |     "1. Install MLflow\n",
36 |     "\n",
37 |     "```bash\n",
38 |     "%pip install mlflow>=3.0.0\n",
39 |     "```\n",
40 |     "\n",
41 |     "2. Start MLflow UI in a separate terminal\n",
42 |     "```bash\n",
43 |     "mlflow ui --port 5000 --backend-store-uri sqlite:///mlruns.db\n",
44 |     "```\n",
45 |     "\n",
46 |     "3. Connect the notebook to MLflow\n",
47 |     "```python\n",
48 |     "import mlflow\n",
49 |     "\n",
50 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
51 |     "mlflow.set_experiment(\"DSPy\")\n",
52 |     "```\n",
53 |     "\n",
54 |     "4. Enabling autologging.\n",
55 |     "\n",
56 |     "```python\n",
57 |     "mlflow.dspy.autolog(\n",
58 |     "    # Log the optimization progress\n",
59 |     "    log_compiles=True,\n",
60 |     "    # Log the evaluation results\n",
61 |     "    log_evals=True,\n",
62 |     "    # Log traces from module executions\n",
63 |     "    log_traces=True\n",
64 |     ")\n",
65 |     "```\n",
66 |     "\n",
67 |     "\n",
68 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
69 |     "</details>"
70 |    ]
71 |   },
72 |   {
73 |    "cell_type": "code",
74 |    "execution_count": 1,
75 |    "id": "283588ae",
76 |    "metadata": {},
77 |    "outputs": [],
78 |    "source": [
79 |     "api_key = input(\"Enter your OpenAI API key: \")\n",
80 |     "import dspy\n",
81 |     "lm = dspy.LM(\"openai/gpt-4.1-mini\", temperature=1, api_key=api_key, max_tokens=32000)\n",
82 |     "dspy.configure(lm=lm)"
83 |    ]
84 |   },
85 |   {
86 |    "cell_type": "markdown",
87 |    "id": "fb853f2c",
88 |    "metadata": {},
89 |    "source": [
90 |     "### Loading the AIME dataset\n",
91 |     "\n",
92 |     "The AIME exam consists of 2 problem sets of size 15 for each year. For this tutorial, we will use AIME problem sets from previous years (2022-2024) for optimization (amounting to total 3 years x 2 sets x 15 problems = 90 problems, split equally between train and validation sets), and test the performance on AIME 2025 (2 sets x 15 problems = 30 problems). Since AIME 2025 is a small set, we repeat it 5 times for statistical stability in evaluation."
93 |    ]
94 |   },
95 |   {
96 |    "cell_type": "code",
97 |    "execution_count": null,
98 |    "id": "c9e78285",
99 |    "metadata": {},
100 |    "outputs": [],
101 |    "source": [
102 |     "import dspy\n",
103 |     "from datasets import load_dataset\n",
104 |     "\n",
105 |     "def init_dataset():\n",
106 |     "    train_split = load_dataset(\"AI-MO/aimo-validation-aime\")['train']\n",
107 |     "    train_split = [\n",
108 |     "        dspy.Example({\n",
109 |     "            \"problem\": x['problem'],\n",
110 |     "            'solution': x['solution'],\n",
111 |     "            'answer': x['answer'],\n",
112 |     "        }).with_inputs(\"problem\")\n",
113 |     "        for x in train_split\n",
114 |     "    ]\n",
115 |     "    import random\n",
116 |     "    random.Random(0).shuffle(train_split)\n",
117 |     "    tot_num = len(train_split)\n",
118 |     "\n",
119 |     "    test_split = load_dataset(\"MathArena/aime_2025\")['train']\n",
120 |     "    test_split = [\n",
121 |     "        dspy.Example({\n",
122 |     "            \"problem\": x['problem'],\n",
123 |     "            'answer': x['answer'],\n",
124 |     "        }).with_inputs(\"problem\")\n",
125 |     "        for x in test_split\n",
126 |     "    ]\n",
127 |     "\n",
128 |     "    train_set = train_split[:int(0.5 * tot_num)]\n",
129 |     "    val_set = train_split[int(0.5 * tot_num):]\n",
130 |     "    test_set = test_split * 5\n",
131 |     "\n",
132 |     "    return train_set, val_set, test_set"
133 |    ]
134 |   },
135 |   {
136 |    "cell_type": "code",
137 |    "execution_count": 3,
138 |    "id": "51822ab6",
139 |    "metadata": {},
140 |    "outputs": [
141 |     {
142 |      "data": {
143 |       "text/plain": [
144 |        "(45, 45, 150)"
145 |       ]
146 |      },
147 |      "execution_count": 3,
148 |      "metadata": {},
149 |      "output_type": "execute_result"
150 |     }
151 |    ],
152 |    "source": [
153 |     "train_set, val_set, test_set = init_dataset()\n",
154 |     "\n",
155 |     "len(train_set), len(val_set), len(test_set)"
156 |    ]
157 |   },
158 |   {
159 |    "cell_type": "markdown",
160 |    "id": "d9f36cd4",
161 |    "metadata": {},
162 |    "source": [
163 |     "Let's view an example task input"
164 |    ]
165 |   },
166 |   {
167 |    "cell_type": "code",
168 |    "execution_count": 4,
169 |    "id": "96f1e064",
170 |    "metadata": {},
171 |    "outputs": [
172 |     {
173 |      "name": "stdout",
174 |      "output_type": "stream",
175 |      "text": [
176 |       "Problem:\n",
177 |       "In isosceles trapezoid $ABCD$, parallel bases $\\overline{AB}$ and $\\overline{CD}$ have lengths $500$ and $650$, respectively, and $AD=BC=333$. The angle bisectors of $\\angle{A}$ and $\\angle{D}$ meet at $P$, and the angle bisectors of $\\angle{B}$ and $\\angle{C}$ meet at $Q$. Find $PQ$.\n",
178 |       "\n",
179 |       "\n",
180 |       "Solution:\n",
181 |       "We have the following diagram:\n",
182 |       "\n",
183 |       "Let $X$ and $W$ be the points where $AP$ and $BQ$ extend to meet $CD$, and $YZ$ be the height of $\\triangle AZB$. As proven in Solution 2, triangles $APD$ and $DPW$ are congruent right triangles. Therefore, $AD = DW = 333$. We can apply this logic to triangles $BCQ$ and $XCQ$ as well, giving us $BC = CX = 333$. Since $CD = 650$, $XW = DW + CX - CD = 16$.\n",
184 |       "Additionally, we can see that $\\triangle XZW$ is similar to $\\triangle PQZ$ and $\\triangle AZB$. We know that $\\frac{XW}{AB} = \\frac{16}{500}$. So, we can say that the height of the triangle $AZB$ is $500u$ while the height of the triangle $XZW$ is $16u$. After that, we can figure out the distance from $Y$ to $PQ: \\frac{500+16}{2} = 258u$ and the height of triangle $PZQ: 500-258 = 242u$.\n",
185 |       "Finally, since the ratio between the height of $PZQ$ to the height of $AZB$ is $242:500$ and $AB$ is $500$, $PQ = \\boxed{242}.$\n",
186 |       "~Cytronical\n",
187 |       "Extend line $PQ$ to meet $AD$ at $P'$ and $BC$ at $Q'$. The diagram looks like this:\n",
188 |       "[asy] /* Made by MRENTHUSIASM */ size(300); pair A, B, C, D, A1, B1, C1, D1, P, Q, P1, Q1; A = (-250,6*sqrt(731)); B = (250,6*sqrt(731)); C = (325,-6*sqrt(731)); D = (-325,-6*sqrt(731)); A1 = bisectorpoint(B,A,D); B1 = bisectorpoint(A,B,C); C1 = bisectorpoint(B,C,D); D1 = bisectorpoint(A,D,C); P = intersectionpoint(A--300*(A1-A)+A,D--300*(D1-D)+D); Q = intersectionpoint(B--300*(B1-B)+B,C--300*(C1-C)+C); P1 = intersectionpoint(A--D,P--(-300)*(Q-P)+P); Q1 = intersectionpoint(B--C,Q--300*(Q-P)+Q); draw(anglemark(P,A,B,1000),red); draw(anglemark(D,A,P,1000),red); draw(anglemark(A,B,Q,1000),red); draw(anglemark(Q,B,C,1000),red); draw(anglemark(P,D,A,1000),red); draw(anglemark(C,D,P,1000),red); draw(anglemark(Q,C,D,1000),red); draw(anglemark(B,C,Q,1000),red); add(pathticks(anglemark(P,A,B,1000), n = 1, r = 0.15, s = 750, red)); add(pathticks(anglemark(D,A,P,1000), n = 1, r = 0.15, s = 750, red)); add(pathticks(anglemark(A,B,Q,1000), n = 1, r = 0.15, s = 750, red)); add(pathticks(anglemark(Q,B,C,1000), n = 1, r = 0.15, s = 750, red)); add(pathticks(anglemark(P,D,A,1000), n = 2, r = 0.12, spacing = 150, s = 750, red)); add(pathticks(anglemark(C,D,P,1000), n = 2, r = 0.12, spacing = 150, s = 750, red)); add(pathticks(anglemark(Q,C,D,1000), n = 2, r = 0.12, spacing = 150, s = 750, red)); add(pathticks(anglemark(B,C,Q,1000), n = 2, r = 0.12, spacing = 150, s = 750, red)); dot(\"$A$\",A,1.5*dir(A),linewidth(4)); dot(\"$B$\",B,1.5*dir(B),linewidth(4)); dot(\"$C$\",C,1.5*dir(C),linewidth(4)); dot(\"$D$\",D,1.5*dir(D),linewidth(4)); dot(\"$P$\",P,1.5*NE,linewidth(4)); dot(\"$Q$\",Q,1.5*NW,linewidth(4)); dot(\"$P'$\",P1,1.5*W,linewidth(4)); dot(\"$Q'$\",Q1,1.5*E,linewidth(4)); draw(A--B--C--D--cycle^^A--P--D^^B--Q--C^^P--Q); draw(P--P1^^Q--Q1,dashed); [/asy]\n",
189 |       "Because the trapezoid is isosceles, by symmetry $PQ$ is parallel to $AB$ and $CD$. Therefore, $\\angle PAB \\cong \\angle APP'$ by interior angles and $\\angle PAB \\cong \\angle PAD$ by the problem statement. Thus, $\\triangle P'AP$ is isosceles with $P'P = P'A$. By symmetry, $P'DP$ is also isosceles, and thus $P'A = \\frac{AD}{2}$. Similarly, the same thing is happening on the right side of the trapezoid, and thus $P'Q'$ is the midline of the trapezoid. Then, $PQ = P'Q' - (P'P + Q'Q)$.\n",
190 |       "Since $P'P = P'A = \\frac{AD}{2}, Q'Q = Q'B = \\frac{BC}{2}$ and $AD = BC = 333$, we have $P'P + Q'Q = \\frac{333}{2} + \\frac{333}{2} = 333$. The length of the midline of a trapezoid is the average of their bases, so $P'Q' = \\frac{500+650}{2} = 575$. Finally, $PQ = 575 - 333 = \\boxed{242}$.\n",
191 |       "~KingRavi\n",
192 |       "We have the following diagram:\n",
193 |       "\n",
194 |       "Extend lines $AP$ and $BQ$ to meet line $DC$ at points $W$ and $X$, respectively, and extend lines $DP$ and $CQ$ to meet $AB$ at points $Z$ and $Y$, respectively.\n",
195 |       "Claim: quadrilaterals $AZWD$ and $BYXC$ are rhombuses.\n",
196 |       "Proof: Since $\\angle DAB + \\angle ADC = 180^{\\circ}$, $\\angle ADP + \\angle PAD = 90^{\\circ}$. Therefore, triangles $APD$, $APZ$, $DPW$ and $PZW$ are all right triangles. By SAA congruence, the first three triangles are congruent; by SAS congruence, $\\triangle PZW$ is congruent to the other three. Therefore, $AD = DW = WZ = AZ$, so $AZWD$ is a rhombus. By symmetry, $BYXC$ is also a rhombus.\n",
197 |       "Extend line $PQ$ to meet $\\overline{AD}$ and $\\overline{BC}$ at $R$ and $S$, respectively. Because of rhombus properties, $RP = QS = \\frac{333}{2}$. Also, by rhombus properties, $R$ and $S$ are the midpoints of segments $AD$ and $BC$, respectively; therefore, by trapezoid properties, $RS = \\frac{AB + CD}{2} = 575$. Finally, $PQ = RS - RP - QS = \\boxed{242}$.\n",
198 |       "~ihatemath123\n",
199 |       "Let $X$ and $Y$ be the feet of the altitudes from $P$ and $Q$, respectively, to $AB$, and let $Z$ and $W$ be the feet of the altitudes from $P$ and $Q$, respectively, to $CD$. Side $AB$ is parallel to side $CD$, so $XYWZ$ is a rectangle with width $PQ$. Furthermore, because $CD - AB = 650-500 = 150$ and trapezoid $ABCD$ is isosceles, $WC - YB = ZD - XA = 75$. \n",
200 |       "Also because $ABCD$ is isosceles, $\\angle ABC + \\angle BCD$ is half the total sum of angles in $ABCD$, or $180^{\\circ}$. Since $BQ$ and $CQ$ bisect $\\angle ABC$ and $\\angle BCD$, respectively, we have $\\angle QBC + \\angle QCB = 90^{\\circ}$, so $\\angle BQC = 90^{\\circ}$. \n",
201 |       "Letting $BQ = 333k$, applying Pythagoras to $\\triangle BQC$ yields $QC = 333\\sqrt{1-k^2}$. We then proceed using similar triangles: $\\angle BYQ = \\angle BQC = 90^{\\circ}$ and $\\angle YBQ = \\angle QBC$, so by AA similarity $YB = 333k^2$. Likewise, $\\angle CWQ = \\angle BQC = 90^{\\circ}$ and $\\angle WCQ = \\angle QCB$, so by AA similarity $WC = 333(1 - k^2)$. Thus $WC + YB = 333$.\n",
202 |       "Adding our two equations for $WC$ and $YB$ gives $2WC = 75 + 333 = 408$. Therefore, the answer is $PQ = ZW = CD - 2WC = 650 - 408 = \\boxed{242}$.\n",
203 |       "~Orange_Quail_9\n",
204 |       "This will be my first solution on AoPS. My apologies in advance for any errors. \n",
205 |       "Angle bisectors can be thought of as the locus of all points equidistant from the lines whose angle they bisect. It can thus be seen that $P$ is equidistant from  $AB, AD,$ and $CD$ and $Q$ is equidistant from  $AB, BC,$ and $CD.$ If we let the feet of the altitudes from $P$ to $AB, AD,$ and $CD$ be called $E, F,$ and $G$ respectively, we can say that $PE = PF = PG.$ Analogously, we let the feet of the altitudes from $Q$ to $AB, BC,$ and $CD$ be $H, I,$ and $J$ respectively. Thus, $QH = QI = QJ.$ Because $ABCD$ is an isosceles trapezoid, we can say that all of the altitudes are equal to each other. \n",
206 |       "By SA as well as SS congruence for right triangles, we find that triangles $AEP, AFP, BHQ,$ and $BIQ$ are congruent. Similarly, $DFP, DGP, CJQ,$ and $CIQ$ by the same reasoning. Additionally, $EH = GJ = PQ$ since $EHQP$ and $GJQP$ are congruent rectangles. \n",
207 |       "If we then let $x = AE = AF = BH = BI,$ let $y = CI = CJ = DG = DF,$ and let $z = EH = GJ = PQ,$ we can create the following system of equations with the given side length information:\n",
208 |       "\\begin{align*} 2x + z &= 500, \\\\ 2y + z &= 650, \\\\ x + y &= 333. \\end{align*}\n",
209 |       "Adding the first two equations, subtracting by twice the second, and dividing by $2$ yields $z = PQ = \\boxed{242}.$\n",
210 |       "~regular\n",
211 |       "Extend line $PQ$ to meet $AD$ at $P'$ and $BC$ at $Q'$. The diagram looks like this: \n",
212 |       "[asy] /* Made by MRENTHUSIASM */ size(300); pair A, B, C, D, A1, B1, C1, D1, P, Q, P1, Q1; A = (-250,6*sqrt(731)); B = (250,6*sqrt(731)); C = (325,-6*sqrt(731)); D = (-325,-6*sqrt(731)); A1 = bisectorpoint(B,A,D); B1 = bisectorpoint(A,B,C); C1 = bisectorpoint(B,C,D); D1 = bisectorpoint(A,D,C); P = intersectionpoint(A--300*(A1-A)+A,D--300*(D1-D)+D); Q = intersectionpoint(B--300*(B1-B)+B,C--300*(C1-C)+C); P1 = intersectionpoint(A--D,P--(-300)*(Q-P)+P); Q1 = intersectionpoint(B--C,Q--300*(Q-P)+Q); draw(anglemark(P,A,B,1000),red); draw(anglemark(D,A,P,1000),red); draw(anglemark(A,B,Q,1000),red); draw(anglemark(Q,B,C,1000),red); draw(anglemark(P,D,A,1000),red); draw(anglemark(C,D,P,1000),red); draw(anglemark(Q,C,D,1000),red); draw(anglemark(B,C,Q,1000),red); add(pathticks(anglemark(P,A,B,1000), n = 1, r = 0.15, s = 750, red)); add(pathticks(anglemark(D,A,P,1000), n = 1, r = 0.15, s = 750, red)); add(pathticks(anglemark(A,B,Q,1000), n = 1, r = 0.15, s = 750, red)); add(pathticks(anglemark(Q,B,C,1000), n = 1, r = 0.15, s = 750, red)); add(pathticks(anglemark(P,D,A,1000), n = 2, r = 0.12, spacing = 150, s = 750, red)); add(pathticks(anglemark(C,D,P,1000), n = 2, r = 0.12, spacing = 150, s = 750, red)); add(pathticks(anglemark(Q,C,D,1000), n = 2, r = 0.12, spacing = 150, s = 750, red)); add(pathticks(anglemark(B,C,Q,1000), n = 2, r = 0.12, spacing = 150, s = 750, red)); dot(\"$A$\",A,1.5*dir(A),linewidth(4)); dot(\"$B$\",B,1.5*dir(B),linewidth(4)); dot(\"$C$\",C,1.5*dir(C),linewidth(4)); dot(\"$D$\",D,1.5*dir(D),linewidth(4)); dot(\"$P$\",P,1.5*NE,linewidth(4)); dot(\"$Q$\",Q,1.5*NW,linewidth(4)); dot(\"$P'$\",P1,1.5*W,linewidth(4)); dot(\"$Q'$\",Q1,1.5*E,linewidth(4)); draw(A--B--C--D--cycle^^A--P--D^^B--Q--C^^P--Q); draw(P--P1^^Q--Q1,dashed); [/asy]\n",
213 |       "Since $\\angle A + \\angle D=\\angle B + \\angle C = 180^{\\circ}$, it follows that $\\angle P'AP+\\angle P'DP = \\angle Q'BQ + \\angle Q'CQ = 90^{\\circ}$. Thus, $\\angle APD = \\angle BQC = 90^{\\circ}$, implying that $\\triangle APD$ and $\\triangle BQC$ are right triangles. Since $P'P$ and $Q'Q$ are medians, $P'P+Q'Q=\\frac{333\\times2}{2}=333$. Since $P'Q'=\\frac{500+650}{2}=575$, we have $PQ+P'P+Q'Q=575$, or $PQ=575-333=\\boxed{242}$.\n",
214 |       "~sigma\n",
215 |       "Let $PQ = x$. Note that since $AP$ bisects $\\angle{A}$ and $DP$ bisects $\\angle{D}$, we have \\[\\angle{APD} = 180^{\\circ}-\\tfrac12 \\angle{A}-\\tfrac12 \\angle{D}=90^{\\circ}.\\] Let $\\angle{ADP}=\\theta$. We have that $\\angle{ADC} = 2\\theta.$ Now, drop an altitude from $A$ to $CD$ at $E$. Notice that $DE=\\tfrac{650-500}{2}=75$. By the definition of cosine, we have \\[\\cos{2\\theta}=1-2\\cos^2{\\theta}=\\tfrac{75}{333}=\\tfrac{25}{111} \\implies \\cos{\\theta}=\\tfrac{2\\sqrt{1887}}{111}.\\] Notice, however, that we can also apply this to $\\triangle{APD}$; we have \\[\\cos{\\theta}=\\tfrac{DP}{333} \\implies DP=6\\sqrt{1887}.\\] By the Pythagorean Theorem, we get \\[AP=\\sqrt{333^2-(6\\sqrt{1887})^2}=3\\sqrt{4773}.\\] Then, drop an altitude from $P$ to $AB$ at $F$; if $AF=y$, then $PQ=x=500-2y$. Because $AP$ is an angle bisector, we see that $\\angle{BAP}=\\angle{DAP}=90^{\\circ}-\\theta$. Again, by the definition of cosine, we have \\[\\cos{(90^{\\circ}-\\theta)}=\\sin{\\theta}=\\tfrac{\\sqrt{4773}}{111}=\\tfrac{y}{3\\sqrt{4773}} \\implies y=129.\\] Finally, $PQ=500-2y=\\boxed{242}$.\n",
216 |       "~pqr.\n",
217 |       "As in solution 4, $\\angle APD = 90^{\\circ}$. Set $k = AX$ and $x = DP$.\n",
218 |       "We know that $DZ = AX + \\frac{DC-AB}{2}$, so $DZ = k + \\frac{650-500}{2} = k + 75$.\n",
219 |       "$\\triangle DPZ \\sim \\triangle APD$ by AA, so we have $\\frac{PD}{AD} = \\frac{ZD}{PD}$, resulting in\n",
220 |       "\\[\\frac{x}{333} = \\frac{k+75}{x} \\text{ (1)}\\]\n",
221 |       "$\\triangle APX \\sim \\triangle ADP$ by AA, so we have $\\frac{AP}{AD} = \\frac{AX}{AP}$, resulting in\n",
222 |       "\\[\\frac{\\sqrt{333^2-x^2}}{333} = \\frac{k}{\\sqrt{333^2-k^2}} \\text{ (2)}\\]\n",
223 |       "From $\\text{(1)}$, we have $x^2 = 333k + 333(75) = 333k + 24975$. From $\\text{(2)}$, we have $333^2 - x^2 = 333k$, or $x^2 = 333^2 - 333k$. Thus, $333k + 24975 = 333^2 - 333k$. Solving for $k$ yields $k = 129$.\n",
224 |       "By symmetry, $YB = AX = 129$. Thus, $PQ = XY = AB - 2AX = 500 - 2(129) = \\boxed{242}$.\n",
225 |       "~ adam_zheng\n",
226 |       "\n",
227 |       "\n",
228 |       "Answer:\n",
229 |       "242\n"
230 |      ]
231 |     }
232 |    ],
233 |    "source": [
234 |     "print(\"Problem:\")\n",
235 |     "print(train_set[0]['problem'])\n",
236 |     "print(\"\\n\\nSolution:\")\n",
237 |     "print(train_set[0]['solution'])\n",
238 |     "print(\"\\n\\nAnswer:\")\n",
239 |     "print(train_set[0]['answer'])"
240 |    ]
241 |   },
242 |   {
243 |    "cell_type": "markdown",
244 |    "id": "552769fd",
245 |    "metadata": {},
246 |    "source": [
247 |     "### Let's define the program: A simple `dspy.ChainOfThought`"
248 |    ]
249 |   },
250 |   {
251 |    "cell_type": "code",
252 |    "execution_count": 5,
253 |    "id": "71598add",
254 |    "metadata": {},
255 |    "outputs": [],
256 |    "source": [
257 |     "class GenerateResponse(dspy.Signature):\n",
258 |     "    \"\"\"Solve the problem and provide the answer in the correct format.\"\"\"\n",
259 |     "    problem = dspy.InputField()\n",
260 |     "    answer = dspy.OutputField()\n",
261 |     "\n",
262 |     "program = dspy.ChainOfThought(GenerateResponse)"
263 |    ]
264 |   },
265 |   {
266 |    "cell_type": "markdown",
267 |    "id": "5e795e1b",
268 |    "metadata": {},
269 |    "source": [
270 |     "### Defining the evaluation metric\n",
271 |     "We simply check exact match between the predicted answer and the correct answer."
272 |    ]
273 |   },
274 |   {
275 |    "cell_type": "code",
276 |    "execution_count": 6,
277 |    "id": "1c0d2c5a",
278 |    "metadata": {},
279 |    "outputs": [],
280 |    "source": [
281 |     "def metric(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
282 |     "    correct_answer = int(example['answer'])\n",
283 |     "    try:\n",
284 |     "        llm_answer = int(prediction.answer)\n",
285 |     "    except ValueError as e:\n",
286 |     "        return 0\n",
287 |     "    return int(correct_answer == llm_answer)"
288 |    ]
289 |   },
290 |   {
291 |    "cell_type": "markdown",
292 |    "id": "785e4e74",
293 |    "metadata": {},
294 |    "source": [
295 |     "### Evaluating unoptimized Chain Of Thought"
296 |    ]
297 |   },
298 |   {
299 |    "cell_type": "code",
300 |    "execution_count": 7,
301 |    "id": "e52d3e50",
302 |    "metadata": {},
303 |    "outputs": [
304 |     {
305 |      "name": "stdout",
306 |      "output_type": "stream",
307 |      "text": [
308 |       "Average Metric: 70.00 / 150 (46.7%): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:01<00:00, 119.75it/s]\n",
309 |       "2025/08/12 21:49:36 INFO dspy.evaluate.evaluate: Average Metric: 70 / 150 (46.7%)\n",
310 |       "\n"
311 |      ]
312 |     },
313 |     {
314 |      "name": "stderr",
315 |      "output_type": "stream",
316 |      "text": [
317 |       "2025/08/12 21:49:36 INFO dspy.evaluate.evaluate: Average Metric: 70 / 150 (46.7%)\n"
318 |      ]
319 |     },
320 |     {
321 |      "name": "stdout",
322 |      "output_type": "stream",
323 |      "text": [
324 |       "\n"
325 |      ]
326 |     },
327 |     {
328 |      "data": {
329 |       "text/html": [
330 |        "<div>\n",
331 |        "<style scoped>\n",
332 |        "    .dataframe tbody tr th:only-of-type {\n",
333 |        "        vertical-align: middle;\n",
334 |        "    }\n",
335 |        "\n",
336 |        "    .dataframe tbody tr th {\n",
337 |        "        vertical-align: top;\n",
338 |        "    }\n",
339 |        "\n",
340 |        "    .dataframe thead th {\n",
341 |        "        text-align: right;\n",
342 |        "    }\n",
343 |        "</style>\n",
344 |        "<table border=\"1\" class=\"dataframe\">\n",
345 |        "  <thead>\n",
346 |        "    <tr style=\"text-align: right;\">\n",
347 |        "      <th></th>\n",
348 |        "      <th>problem</th>\n",
349 |        "      <th>example_answer</th>\n",
350 |        "      <th>reasoning</th>\n",
351 |        "      <th>pred_answer</th>\n",
352 |        "      <th>metric</th>\n",
353 |        "    </tr>\n",
354 |        "  </thead>\n",
355 |        "  <tbody>\n",
356 |        "    <tr>\n",
357 |        "      <th>0</th>\n",
358 |        "      <td>Find the sum of all integer bases $b&gt;9$ for which $17_b$ is a divi...</td>\n",
359 |        "      <td>70</td>\n",
360 |        "      <td>We are looking for integer bases \\( b &gt; 9 \\) such that \\( 17_b \\) ...</td>\n",
361 |        "      <td>70</td>\n",
362 |        "      <td>✔️ [1]</td>\n",
363 |        "    </tr>\n",
364 |        "    <tr>\n",
365 |        "      <th>1</th>\n",
366 |        "      <td>On $\\triangle ABC$ points $A, D, E$, and $B$ lie in that order on ...</td>\n",
367 |        "      <td>588</td>\n",
368 |        "      <td>Let's analyze the problem step-by-step. We have triangle \\( ABC \\)...</td>\n",
369 |        "      <td>588</td>\n",
370 |        "      <td>✔️ [1]</td>\n",
371 |        "    </tr>\n",
372 |        "    <tr>\n",
373 |        "      <th>2</th>\n",
374 |        "      <td>The 9 members of a baseball team went to an ice-cream parlor after...</td>\n",
375 |        "      <td>16</td>\n",
376 |        "      <td>We have 9 players, each choosing one of three flavors: chocolate (...</td>\n",
377 |        "      <td>16</td>\n",
378 |        "      <td>✔️ [1]</td>\n",
379 |        "    </tr>\n",
380 |        "    <tr>\n",
381 |        "      <th>3</th>\n",
382 |        "      <td>Find the number of ordered pairs $(x,y)$, where both $x$ and $y$ a...</td>\n",
383 |        "      <td>117</td>\n",
384 |        "      <td>We start with the given equation: \\[12x^2 - xy - 6y^2 = 0.\\] Our g...</td>\n",
385 |        "      <td>117</td>\n",
386 |        "      <td>✔️ [1]</td>\n",
387 |        "    </tr>\n",
388 |        "    <tr>\n",
389 |        "      <th>4</th>\n",
390 |        "      <td>There are $8!= 40320$ eight-digit positive integers that use each ...</td>\n",
391 |        "      <td>279</td>\n",
392 |        "      <td>We are given that there are \\(8! = 40320\\) eight-digit numbers tha...</td>\n",
393 |        "      <td>279</td>\n",
394 |        "      <td>✔️ [1]</td>\n",
395 |        "    </tr>\n",
396 |        "    <tr>\n",
397 |        "      <th>...</th>\n",
398 |        "      <td>...</td>\n",
399 |        "      <td>...</td>\n",
400 |        "      <td>...</td>\n",
401 |        "      <td>...</td>\n",
402 |        "      <td>...</td>\n",
403 |        "    </tr>\n",
404 |        "    <tr>\n",
405 |        "      <th>145</th>\n",
406 |        "      <td>Let $S$ be the set of vertices of a regular $24$-gon. Find the num...</td>\n",
407 |        "      <td>113</td>\n",
408 |        "      <td>We have a regular 24-gon with vertex set \\( S \\) of size 24. We wa...</td>\n",
409 |        "      <td>1666</td>\n",
410 |        "      <td></td>\n",
411 |        "    </tr>\n",
412 |        "    <tr>\n",
413 |        "      <th>146</th>\n",
414 |        "      <td>Let $A_1 A_2 A_3 \\ldots A_{11}$ be an $11$-sided non-convex simple...</td>\n",
415 |        "      <td>19</td>\n",
416 |        "      <td>Let's analyze the problem step-by-step. We are given an 11-sided p...</td>\n",
417 |        "      <td>19</td>\n",
418 |        "      <td>✔️ [1]</td>\n",
419 |        "    </tr>\n",
420 |        "    <tr>\n",
421 |        "      <th>147</th>\n",
422 |        "      <td>Let $x_1, x_2, x_3, \\ldots$ be a sequence of rational numbers defi...</td>\n",
423 |        "      <td>248</td>\n",
424 |        "      <td>We have the sequence: \\[ x_1 = \\frac{25}{11}, \\quad x_{k+1} = \\fra...</td>\n",
425 |        "      <td>589</td>\n",
426 |        "      <td></td>\n",
427 |        "    </tr>\n",
428 |        "    <tr>\n",
429 |        "      <th>148</th>\n",
430 |        "      <td>Let $\\triangle ABC$ be a right triangle with $\\angle A = 90^\\circ$...</td>\n",
431 |        "      <td>104</td>\n",
432 |        "      <td>Given a right triangle \\(\\triangle ABC\\) with \\(\\angle A = 90^\\cir...</td>\n",
433 |        "      <td>98</td>\n",
434 |        "      <td></td>\n",
435 |        "    </tr>\n",
436 |        "    <tr>\n",
437 |        "      <th>149</th>\n",
438 |        "      <td>There are exactly three positive real numbers $k$ such that the fu...</td>\n",
439 |        "      <td>240</td>\n",
440 |        "      <td>We are given the function \\[ f(x) = \\frac{(x - 18)(x - 72)(x - 98)...</td>\n",
441 |        "      <td>240</td>\n",
442 |        "      <td>✔️ [1]</td>\n",
443 |        "    </tr>\n",
444 |        "  </tbody>\n",
445 |        "</table>\n",
446 |        "<p>150 rows × 5 columns</p>\n",
447 |        "</div>"
448 |       ],
449 |       "text/plain": [
450 |        "                                                                   problem  \\\n",
451 |        "0    Find the sum of all integer bases $b>9$ for which $17_b$ is a divi...   \n",
452 |        "1    On $\\triangle ABC$ points $A, D, E$, and $B$ lie in that order on ...   \n",
453 |        "2    The 9 members of a baseball team went to an ice-cream parlor after...   \n",
454 |        "3    Find the number of ordered pairs $(x,y)$, where both $x$ and $y$ a...   \n",
455 |        "4    There are $8!= 40320$ eight-digit positive integers that use each ...   \n",
456 |        "..                                                                     ...   \n",
457 |        "145  Let $S$ be the set of vertices of a regular $24$-gon. Find the num...   \n",
458 |        "146  Let $A_1 A_2 A_3 \\ldots A_{11}$ be an $11$-sided non-convex simple...   \n",
459 |        "147  Let $x_1, x_2, x_3, \\ldots$ be a sequence of rational numbers defi...   \n",
460 |        "148  Let $\\triangle ABC$ be a right triangle with $\\angle A = 90^\\circ$...   \n",
461 |        "149  There are exactly three positive real numbers $k$ such that the fu...   \n",
462 |        "\n",
463 |        "     example_answer  \\\n",
464 |        "0                70   \n",
465 |        "1               588   \n",
466 |        "2                16   \n",
467 |        "3               117   \n",
468 |        "4               279   \n",
469 |        "..              ...   \n",
470 |        "145             113   \n",
471 |        "146              19   \n",
472 |        "147             248   \n",
473 |        "148             104   \n",
474 |        "149             240   \n",
475 |        "\n",
476 |        "                                                                 reasoning  \\\n",
477 |        "0    We are looking for integer bases \\( b > 9 \\) such that \\( 17_b \\) ...   \n",
478 |        "1    Let's analyze the problem step-by-step. We have triangle \\( ABC \\)...   \n",
479 |        "2    We have 9 players, each choosing one of three flavors: chocolate (...   \n",
480 |        "3    We start with the given equation: \\[12x^2 - xy - 6y^2 = 0.\\] Our g...   \n",
481 |        "4    We are given that there are \\(8! = 40320\\) eight-digit numbers tha...   \n",
482 |        "..                                                                     ...   \n",
483 |        "145  We have a regular 24-gon with vertex set \\( S \\) of size 24. We wa...   \n",
484 |        "146  Let's analyze the problem step-by-step. We are given an 11-sided p...   \n",
485 |        "147  We have the sequence: \\[ x_1 = \\frac{25}{11}, \\quad x_{k+1} = \\fra...   \n",
486 |        "148  Given a right triangle \\(\\triangle ABC\\) with \\(\\angle A = 90^\\cir...   \n",
487 |        "149  We are given the function \\[ f(x) = \\frac{(x - 18)(x - 72)(x - 98)...   \n",
488 |        "\n",
489 |        "    pred_answer  metric  \n",
490 |        "0            70  ✔️ [1]  \n",
491 |        "1           588  ✔️ [1]  \n",
492 |        "2            16  ✔️ [1]  \n",
493 |        "3           117  ✔️ [1]  \n",
494 |        "4           279  ✔️ [1]  \n",
495 |        "..          ...     ...  \n",
496 |        "145        1666          \n",
497 |        "146          19  ✔️ [1]  \n",
498 |        "147         589          \n",
499 |        "148          98          \n",
500 |        "149         240  ✔️ [1]  \n",
501 |        "\n",
502 |        "[150 rows x 5 columns]"
503 |       ]
504 |      },
505 |      "metadata": {},
506 |      "output_type": "display_data"
507 |     },
508 |     {
509 |      "data": {
510 |       "text/plain": [
511 |        "EvaluationResult(score=46.67, results=<list of 150 results>)"
512 |       ]
513 |      },
514 |      "execution_count": 7,
515 |      "metadata": {},
516 |      "output_type": "execute_result"
517 |     }
518 |    ],
519 |    "source": [
520 |     "import dspy\n",
521 |     "evaluate = dspy.Evaluate(\n",
522 |     "    devset=test_set,\n",
523 |     "    metric=metric,\n",
524 |     "    num_threads=32,\n",
525 |     "    display_table=True,\n",
526 |     "    display_progress=True\n",
527 |     ")\n",
528 |     "\n",
529 |     "evaluate(program)"
530 |    ]
531 |   },
532 |   {
533 |    "cell_type": "markdown",
534 |    "id": "0aeb9bae",
535 |    "metadata": {},
536 |    "source": [
537 |     "### Optimize the program with `dspy.GEPA`\n",
538 |     "\n",
539 |     "GEPA is a _reflective_ prompt optimizer, and it's strength lies in being able to leverage additional sources of information, like the DSPy program's execution and evaluation pipelines, which provides GEPA more visibility into why the system got the score that it did, and then GEPA can introspect to identify how to improve the score. GEPA can also leverage additional supervision provided in this manner. For example, during optimization, we can return the correct solution's to the problems the program failed to solve.\n",
540 |     "\n",
541 |     "We note that while such explicit supervision is not available in all scenarios, GEPA can work very flexibly with different forms of feedback (for example, using LLM-as-a-judge feedback shown in the PAPILLON tutorial, or just using answer labels, as shown in the facility-support tutorial).\n",
542 |     "\n",
543 |     "Let's quickly modify the evaluation metric to become an optimization metric for GEPA, that can provide this additional supervision!"
544 |    ]
545 |   },
546 |   {
547 |    "cell_type": "code",
548 |    "execution_count": 8,
549 |    "id": "e21e86df",
550 |    "metadata": {},
551 |    "outputs": [],
552 |    "source": [
553 |     "def metric_with_feedback(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
554 |     "    correct_answer = int(example['answer'])\n",
555 |     "    written_solution = example.get('solution', '')\n",
556 |     "    try:\n",
557 |     "        llm_answer = int(prediction.answer)\n",
558 |     "    except ValueError as e:\n",
559 |     "        feedback_text = f\"The final answer must be a valid integer and nothing else. You responded with '{prediction.answer}', which couldn't be parsed as a python integer. Please ensure your answer is a valid integer without any additional text or formatting.\"\n",
560 |     "        feedback_text += f\" The correct answer is '{correct_answer}'.\"\n",
561 |     "        if written_solution:\n",
562 |     "            feedback_text += f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems and ensure your final answer is a valid integer.\"\n",
563 |     "        return dspy.Prediction(score=0, feedback=feedback_text)\n",
564 |     "\n",
565 |     "    score = int(correct_answer == llm_answer)\n",
566 |     "\n",
567 |     "    feedback_text = \"\"\n",
568 |     "    if score == 1:\n",
569 |     "        feedback_text = f\"Your answer is correct. The correct answer is '{correct_answer}'.\"\n",
570 |     "    else:\n",
571 |     "        feedback_text = f\"Your answer is incorrect. The correct answer is '{correct_answer}'.\"\n",
572 |     "    \n",
573 |     "    if written_solution:\n",
574 |     "        feedback_text += f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems.\"\n",
575 |     "\n",
576 |     "    return dspy.Prediction(score=score, feedback=feedback_text)"
577 |    ]
578 |   },
579 |   {
580 |    "cell_type": "code",
581 |    "execution_count": 9,
582 |    "id": "b1404077",
583 |    "metadata": {},
584 |    "outputs": [
585 |     {
586 |      "name": "stderr",
587 |      "output_type": "stream",
588 |      "text": [
589 |       "2025/08/12 21:49:36 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 560 metric calls of the program. This amounts to 6.22 full evals on the train+val set.\n",
590 |       "3\n",
591 |       "2025/08/12 21:49:36 INFO dspy.teleprompt.gepa.gepa: Using 45 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n",
592 |       "2025/08/12 21:52:15 INFO dspy.evaluate.evaluate: Average Metric: 17.0 / 45 (37.8%)\n",
593 |       "2025/08/12 21:52:15 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.37777777777777777\n",
594 |       "2025/08/12 21:52:15 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.37777777777777777\n",
595 |       "Average Metric: 2.00 / 3 (66.7%): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:10<00:00, 63.51s/it]\n",
596 |       "2025/08/12 21:55:26 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
597 |       "\n",
598 |       "2025/08/12 21:56:50 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for predict: You will be given a single math problem as plain text under an input key like “problem.” Your task is to solve it correctly and return:\n",
599 |       "\n",
600 |       "- reasoning: a concise, logically ordered solution that uses appropriate identities/constraints to minimize brute force and ends with a quick verification.\n",
601 |       "- answer: the final result only (just the number or expression requested, no extra words).\n",
602 |       "\n",
603 |       "Formatting:\n",
604 |       "- Use two top-level fields exactly named “reasoning” and “answer.”\n",
605 |       "- Keep the reasoning succinct but complete. Avoid heavy markup. Bullet points are fine.\n",
606 |       "- The answer field must contain only the final value requested (e.g., 227, 585, 601).\n",
607 |       "\n",
608 |       "General problem-solving guidance:\n",
609 |       "- Parse the problem type (e.g., base representation, palindrome constraints across bases, symmetric sums with constraints, counting ordered triples).\n",
610 |       "- Always enforce domain constraints (e.g., base-b digits are 0..b-1; leading digit in base-10 three-digit numbers is 1..9; ordered triples count permutations unless otherwise specified).\n",
611 |       "- Use algebraic identities and modular arithmetic to reduce the search space and ensure correctness.\n",
612 |       "\n",
613 |       "Domain-specific strategies (from common contest problems):\n",
614 |       "\n",
615 |       "1) Base-conversion/digit rearrangement:\n",
616 |       "- Translate positional notation correctly:\n",
617 |       "  For base 10: abc = 100a + 10b + c.\n",
618 |       "  For base 9 (or b): (b c a)_9 = b·9^2 + c·9 + a.\n",
619 |       "- Enforce digit ranges: in base 9, digits a, b, c ∈ {0,…,8}; if the base-10 number is three-digit, a ∈ {1,…,9} and also must satisfy the base-9 digit limit (so a ∈ {1,…,8}).\n",
620 |       "- Set up equality and simplify. Example pattern: 100a + 10b + c = 81b + 9c + a ⇒ 99a = 71b + 8c.\n",
621 |       "- Use modular constraints to prune:\n",
622 |       "  • Mod 9: 99a ≡ 0 and 71 ≡ 8 ⇒ 0 ≡ 8(b + c) ⇒ b + c ≡ 0 (mod 9), so b + c ∈ {0, 9} within digit bounds.\n",
623 |       "  • Mod 8: 99 ≡ 3, 71 ≡ 7 ⇒ 3a ≡ 7b (mod 8) ⇒ b ≡ −3a (mod 8).\n",
624 |       "- Solve within digit bounds and verify.\n",
625 |       "\n",
626 |       "2) Palindromes across bases (e.g., base 10 and base 8):\n",
627 |       "- Determine the base-8 length given the magnitude (n < 1000 ⇒ base-8 has 3 or 4 digits).\n",
628 |       "- Characterize base-8 palindromes:\n",
629 |       "  • 3-digit: (A B A)_8 = 64A + 8B + A = 65A + 8B.\n",
630 |       "  • 4-digit: (A B B A)_8 = 512A + 64B + 8B + A = 513A + 72B.\n",
631 |       "- For n < 1000 and 4-digit octal palindrome, A must be 1. Enumerate B ∈ {0,…,7} to get candidates 513 + 72B and test which are decimal palindromes. Choose the greatest valid n.\n",
632 |       "\n",
633 |       "3) Symmetric sums with a + b + c fixed (ordered triples of nonnegative integers):\n",
634 |       "- Convert sums like a^2b + a^2c + b^2a + b^2c + c^2a + c^2b using:\n",
635 |       "  S = ab(a + b) + bc(b + c) + ca(c + a) = (a + b + c)(ab + bc + ca) − 3abc.\n",
636 |       "- With a + b + c given (e.g., 300), plug into S = given constant to derive:\n",
637 |       "  100(ab + bc + ca) − abc = constant.\n",
638 |       "- Use the shift a = 100 + x, b = 100 + y, c = 100 + z, so x + y + z = 0. Then:\n",
639 |       "  (a − 100)(b − 100)(c − 100) = abc − 100(ab + bc + ca) + 2,000,000.\n",
640 |       "  Setting S correctly yields (a − 100)(b − 100)(c − 100) = 0.\n",
641 |       "- Count solutions:\n",
642 |       "  • If exactly one equals 100, the other two sum to 200 with both ≠ 100. Count all integer splits respecting nonnegativity; multiply by 3 for which variable is 100.\n",
643 |       "  • Include the case a = b = c = 100 once.\n",
644 |       "  • Treat (a, b, c) as ordered unless the problem explicitly asks for unordered.\n",
645 |       "\n",
646 |       "Quality checks:\n",
647 |       "- Verify digit/base constraints and final equality numerically.\n",
648 |       "- For “greatest/least” questions, justify why your candidate is optimal (structural argument or bounded enumeration).\n",
649 |       "- For counting, avoid over/undercounting; be explicit about ordered vs unordered.\n",
650 |       "\n",
651 |       "Finally:\n",
652 |       "- Place the clean final numeric result in the “answer” field.\n",
653 |       "2025/08/12 21:57:19 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
[TRUNCATED]
```

tutorials/gepa_papillon/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: GEPA for Privacy-Conscious Delegation\n",
8 |     "\n",
9 |     "In this tutorial, we optimize the [PAPILLON](https://dspy.ai/tutorials/papillon/) program with `dspy.GEPA`, a novel optimizer that uses LLM's to reflect on its own approach and mistakes, and proposes new prompts based on the reflection.\n",
10 |     "\n",
11 |     "PAPILLON is a system for privacy-preserving delegation, a small LM (typically local-hosted) to use a larger \"untrusted\" external LLM, which is more powerful but may save your private data, to balance high-quality and private chat.\n",
12 |     "\n",
13 |     "For simplicity, we will use \"gpt-4.1-nano\" as the small LM, and \"gpt-4.1-mini\" as the large, \"untrusted\" LM."
14 |    ]
15 |   },
16 |   {
17 |    "cell_type": "markdown",
18 |    "metadata": {},
19 |    "source": [
20 |     "<details>\n",
21 |     "<summary>Recommended: Set up MLflow Autologging to understand what's happening under the hood.</summary>\n",
22 |     "\n",
23 |     "### MLflow DSPy Integration\n",
24 |     "\n",
25 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. MLflow's autologging capability automatically tracks progress of GEPA optimization, as well as visualizes prompts and module executions as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
26 |     "\n",
27 |     "**Visualize module executions as traces**\n",
28 |     "\n",
29 |     "![MLflow Trace](./mlflow-tracing-gepa-papilon.png)\n",
30 |     "\n",
31 |     "**Automatically track optimization progress and results**\n",
32 |     "\n",
33 |     "![MLflow Tracking](./mlflow-tracking-gepa-papilon-optimization.png)\n",
34 |     "\n",
35 |     "\n",
36 |     "**Setup MLflow**\n",
37 |     "\n",
38 |     "1. Install MLflow\n",
39 |     "\n",
40 |     "```bash\n",
41 |     "%pip install mlflow>=3.0.0\n",
42 |     "```\n",
43 |     "\n",
44 |     "2. Start MLflow UI in a separate terminal\n",
45 |     "```bash\n",
46 |     "mlflow ui --port 5000 --backend-store-uri sqlite:///mlruns.db\n",
47 |     "```\n",
48 |     "\n",
49 |     "3. Connect the notebook to MLflow\n",
50 |     "```python\n",
51 |     "import mlflow\n",
52 |     "\n",
53 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
54 |     "mlflow.set_experiment(\"DSPy\")\n",
55 |     "```\n",
56 |     "\n",
57 |     "4. Enabling autologging.\n",
58 |     "\n",
59 |     "```python\n",
60 |     "mlflow.dspy.autolog(\n",
61 |     "    # Log the optimization progress\n",
62 |     "    log_compiles=True,\n",
63 |     "    # Log the evaluation results\n",
64 |     "    log_evals=True,\n",
65 |     "    # Log traces from module executions\n",
66 |     "    log_traces=True\n",
67 |     ")\n",
68 |     "```\n",
69 |     "\n",
70 |     "\n",
71 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
72 |     "</details>"
73 |    ]
74 |   },
75 |   {
76 |    "cell_type": "code",
77 |    "execution_count": 1,
78 |    "metadata": {},
79 |    "outputs": [],
80 |    "source": [
81 |     "import dspy\n",
82 |     "api_key = input(\"Enter your OpenAI API key: \")\n",
83 |     "local_lm = dspy.LM(model=\"openai/gpt-4.1-nano\", api_key=api_key)\n",
84 |     "large_lm = dspy.LM(model=\"openai/gpt-4.1-mini\", api_key=api_key)\n",
85 |     "dspy.configure(lm=local_lm)"
86 |    ]
87 |   },
88 |   {
89 |    "cell_type": "markdown",
90 |    "metadata": {},
91 |    "source": [
92 |     "### The PAPILLON Program"
93 |    ]
94 |   },
95 |   {
96 |    "cell_type": "code",
97 |    "execution_count": 2,
98 |    "metadata": {},
99 |    "outputs": [],
100 |    "source": [
101 |     "class CraftRedactedRequest(dspy.Signature):\n",
102 |     "    \"\"\"\n",
103 |     "    Given a private user query, create a privacy-preserving request for a powerful external LLM.\n",
104 |     "    The LLM may assist without learning private information about the user.\n",
105 |     "    \"\"\"\n",
106 |     "\n",
107 |     "    user_query = dspy.InputField()\n",
108 |     "    llm_request = dspy.OutputField()\n",
109 |     "\n",
110 |     "\n",
111 |     "class RespondToQuery(dspy.Signature):\n",
112 |     "    \"\"\"\n",
113 |     "    Respond to a user query.\n",
114 |     "    For inspiration, we found a potentially related request to a powerful external LLM and its response.\n",
115 |     "    \"\"\"\n",
116 |     "\n",
117 |     "    related_llm_request = dspy.InputField()\n",
118 |     "    related_llm_response = dspy.InputField(desc=\"information from a powerful LLM responding to a related request\")\n",
119 |     "    user_query = dspy.InputField(desc=\"the user's request you need to fulfill\")\n",
120 |     "    response = dspy.OutputField(desc=\"your final response to the user's request\")\n",
121 |     "\n",
122 |     "\n",
123 |     "class PAPILLON(dspy.Module):\n",
124 |     "    def __init__(self, untrusted_model):\n",
125 |     "        self.craft_redacted_request = dspy.ChainOfThought(CraftRedactedRequest)\n",
126 |     "        self.respond_to_query = dspy.Predict(RespondToQuery)\n",
127 |     "        self.untrusted_model = untrusted_model\n",
128 |     "\n",
129 |     "    def forward(self, user_query):\n",
130 |     "        try:\n",
131 |     "            llm_request = self.craft_redacted_request(user_query=user_query).llm_request\n",
132 |     "            llm_response = self.untrusted_model(llm_request)[0]\n",
133 |     "            response = self.respond_to_query(\n",
134 |     "                related_llm_request=llm_request, related_llm_response=llm_response, user_query=user_query\n",
135 |     "            ).response\n",
136 |     "        except Exception:\n",
137 |     "            return dspy.Prediction(llm_request=\"\", llm_response=\"\", response=\"\")\n",
138 |     "\n",
139 |     "        return dspy.Prediction(llm_request=llm_request, llm_response=llm_response, response=response)"
140 |    ]
141 |   },
142 |   {
143 |    "cell_type": "code",
144 |    "execution_count": 3,
145 |    "metadata": {},
146 |    "outputs": [
147 |     {
148 |      "name": "stdout",
149 |      "output_type": "stream",
150 |      "text": [
151 |       "Loaded 225 training examples, 225 dev examples, and 214 test examples.\n"
152 |      ]
153 |     }
154 |    ],
155 |    "source": [
156 |     "from datasets import load_dataset\n",
157 |     "\n",
158 |     "pupa_tnb = load_dataset(\"Columbia-NLP/PUPA\", \"pupa_tnb\")\n",
159 |     "pupa_new = load_dataset(\"Columbia-NLP/PUPA\", \"pupa_new\")\n",
160 |     "\n",
161 |     "examples = [\n",
162 |     "    dspy.Example(\n",
163 |     "        {\"target_response\": x[\"target_response\"], \"user_query\": x[\"user_query\"], \"pii_str\": x[\"pii_units\"]}\n",
164 |     "    ).with_inputs(\"user_query\")\n",
165 |     "    for x in pupa_new[\"train\"]\n",
166 |     "]\n",
167 |     "\n",
168 |     "trainset, devset, testset = examples[:225], examples[225:450], examples[450:]\n",
169 |     "print(f\"Loaded {len(trainset)} training examples, {len(devset)} dev examples, and {len(testset)} test examples.\")"
170 |    ]
171 |   },
172 |   {
173 |    "cell_type": "markdown",
174 |    "metadata": {},
175 |    "source": [
176 |     "### Define metrics for success in this task\n",
177 |     "\n",
178 |     "What does it mean for a PAPILLON system to be successful?\n",
179 |     "\n",
180 |     "1. The responses of the local model should be as good as (or better than) the `target_response` from a large LM.\n",
181 |     "2. The local model should leak as few `pii_units` to the remote model as possible.\n",
182 |     "\n",
183 |     "For benchmarking, we will judge both of these using our `openai_lm` and the annotation in PUPA."
184 |    ]
185 |   },
186 |   {
187 |    "cell_type": "code",
188 |    "execution_count": 4,
189 |    "metadata": {},
190 |    "outputs": [],
191 |    "source": [
192 |     "class JudgeQuality(dspy.Signature):\n",
193 |     "    \"\"\"\n",
194 |     "    You are comparing the quality of two responses, given a user query.\n",
195 |     "    Is response A at least as good as response B?\n",
196 |     "    \"\"\"\n",
197 |     "\n",
198 |     "    user_query = dspy.InputField(desc=\"The user's request to be fulfilled.\")\n",
199 |     "    response_A = dspy.InputField()\n",
200 |     "    response_B = dspy.InputField()\n",
201 |     "    judgment: bool = dspy.OutputField()\n",
202 |     "\n",
203 |     "\n",
204 |     "class JudgeLeakage(dspy.Signature):\n",
205 |     "    \"\"\"\n",
206 |     "    Count the number information pieces in `pii` that (i) are forms of PII *and* (ii) are leaked into the prompt given.\n",
207 |     "    Don't count the cases where the leaked information is just a celebrity name.\n",
208 |     "    Don't count the cases where the `pii` information does not appear in the prompt.\n",
209 |     "    \"\"\"\n",
210 |     "\n",
211 |     "    pii = dspy.InputField()\n",
212 |     "    prompt = dspy.InputField()\n",
213 |     "    num_pii_leaked: int = dspy.OutputField()\n",
214 |     "\n",
215 |     "\n",
216 |     "class LLMJudge(dspy.Module):\n",
217 |     "    def __init__(self):\n",
218 |     "        self.quality_judge = dspy.ChainOfThought(JudgeQuality)\n",
219 |     "        self.fact_checker = dspy.ChainOfThought(JudgeLeakage)\n",
220 |     "\n",
221 |     "    def forward(self, user_query, og_resp, new_resp=None, updated_query=None, pii_str=None):\n",
222 |     "        judgment_1 = self.quality_judge(user_query=user_query, response_A=new_resp, response_B=og_resp).judgment\n",
223 |     "        judgment_2 = self.quality_judge(user_query=user_query, response_A=og_resp, response_B=new_resp).judgment\n",
224 |     "        judgment = judgment_1 or (judgment_1 == judgment_2)  # True if better or if judge is inconsistent\n",
225 |     "\n",
226 |     "        pii = list(set(pii_str.split(\"||\")))  # The pii_str field must be separated by `||`\n",
227 |     "        pii_score = self.fact_checker(pii=pii, prompt=updated_query).num_pii_leaked\n",
228 |     "        pii_score = pii_score / len(pii) if len(pii) > 0 else 0\n",
229 |     "\n",
230 |     "        return dspy.Prediction(quality=judgment, leakage=pii_score)\n",
231 |     "\n",
232 |     "\n",
233 |     "llm_judge = LLMJudge()\n",
234 |     "llm_judge.set_lm(large_lm)"
235 |    ]
236 |   },
237 |   {
238 |    "cell_type": "markdown",
239 |    "metadata": {},
240 |    "source": [
241 |     "With these judges, we can now define the metric for evaluation."
242 |    ]
243 |   },
244 |   {
245 |    "cell_type": "code",
246 |    "execution_count": 5,
247 |    "metadata": {},
248 |    "outputs": [],
249 |    "source": [
250 |     "def compute_metrics(gold, pred, trace=None):\n",
251 |     "    return llm_judge(\n",
252 |     "        user_query=gold.user_query,\n",
253 |     "        new_resp=pred.response,\n",
254 |     "        og_resp=gold.target_response,\n",
255 |     "        updated_query=pred.llm_request,\n",
256 |     "        pii_str=gold.pii_str,\n",
257 |     "    )\n",
258 |     "\n",
259 |     "def compute_overall_score(gold, pred, trace=None):\n",
260 |     "    metrics = compute_metrics(gold, pred, trace)\n",
261 |     "    overall_score = (metrics.quality + (1 - metrics.leakage)) / 2.0\n",
262 |     "    return overall_score"
263 |    ]
264 |   },
265 |   {
266 |    "cell_type": "markdown",
267 |    "metadata": {},
268 |    "source": [
269 |     "### Evaluate unoptimized PAPILLON\n",
270 |     "\n",
271 |     "Let's now use the PUPA data and the judges above to evaluate the unoptimized version of our PAPILLON pipeline!"
272 |    ]
273 |   },
274 |   {
275 |    "cell_type": "code",
276 |    "execution_count": 6,
277 |    "metadata": {},
278 |    "outputs": [
279 |     {
280 |      "name": "stdout",
281 |      "output_type": "stream",
282 |      "text": [
283 |       "  0%|                                                                                                                                                                                          | 0/214 [00:00<?, ?it/s]Average Metric: 107.87 / 139 (77.6%):  65%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                | 139/214 [00:13<00:05, 12.75it/s]"
284 |      ]
285 |     },
286 |     {
287 |      "name": "stderr",
288 |      "output_type": "stream",
289 |      "text": [
290 |       "2025/08/12 18:31:42 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.0)  if the reason for truncation is repetition.\n"
291 |      ]
292 |     },
293 |     {
294 |      "name": "stdout",
295 |      "output_type": "stream",
296 |      "text": [
297 |       "Average Metric: 163.71 / 214 (76.5%): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [00:22<00:00,  9.57it/s]"
298 |      ]
299 |     },
300 |     {
301 |      "name": "stderr",
302 |      "output_type": "stream",
303 |      "text": [
304 |       "2025/08/12 18:31:52 INFO dspy.evaluate.evaluate: Average Metric: 163.70979853479852 / 214 (76.5%)\n"
305 |      ]
306 |     },
307 |     {
308 |      "name": "stdout",
309 |      "output_type": "stream",
310 |      "text": [
311 |       "\n"
312 |      ]
313 |     },
314 |     {
315 |      "data": {
316 |       "text/html": [
317 |        "<div>\n",
318 |        "<style scoped>\n",
319 |        "    .dataframe tbody tr th:only-of-type {\n",
320 |        "        vertical-align: middle;\n",
321 |        "    }\n",
322 |        "\n",
323 |        "    .dataframe tbody tr th {\n",
324 |        "        vertical-align: top;\n",
325 |        "    }\n",
326 |        "\n",
327 |        "    .dataframe thead th {\n",
328 |        "        text-align: right;\n",
329 |        "    }\n",
330 |        "</style>\n",
331 |        "<table border=\"1\" class=\"dataframe\">\n",
332 |        "  <thead>\n",
333 |        "    <tr style=\"text-align: right;\">\n",
334 |        "      <th></th>\n",
335 |        "      <th>target_response</th>\n",
336 |        "      <th>user_query</th>\n",
337 |        "      <th>pii_str</th>\n",
338 |        "      <th>llm_request</th>\n",
339 |        "      <th>llm_response</th>\n",
340 |        "      <th>response</th>\n",
341 |        "      <th>compute_overall_score</th>\n",
342 |        "    </tr>\n",
343 |        "  </thead>\n",
344 |        "  <tbody>\n",
345 |        "    <tr>\n",
346 |        "      <th>0</th>\n",
347 |        "      <td>Subject: Follow-up on Endpoint Management Solution Discussion Dear...</td>\n",
348 |        "      <td>rewrite in professional \"as discusseed in last meeting with you an...</td>\n",
349 |        "      <td>effem||balaji||balaji</td>\n",
350 |        "      <td>Rewrite the following message in a professional tone, ensuring cla...</td>\n",
351 |        "      <td>Subject: Follow-up on E-FEM Account Management and Solution Finali...</td>\n",
352 |        "      <td>Subject: Clarification on E-FEM Account Management and Next Steps ...</td>\n",
353 |        "      <td></td>\n",
354 |        "    </tr>\n",
355 |        "    <tr>\n",
356 |        "      <th>1</th>\n",
357 |        "      <td>Кампания \"Share a Coke\" была запущена Coca-Cola в 2011 году в Авст...</td>\n",
358 |        "      <td>расскажи про PR-кампанию Coca-Cola “Share a Coke\"</td>\n",
359 |        "      <td>coca-cola</td>\n",
360 |        "      <td>Please provide a detailed summary of Coca-Cola's \"Share a Coke\" PR...</td>\n",
361 |        "      <td>Certainly! Here is a detailed summary of Coca-Cola's \"Share a Coke...</td>\n",
362 |        "      <td>Кампания Coca-Cola \"Share a Coke\" — это очень успешная маркетингов...</td>\n",
363 |        "      <td>✔️ [1.000]</td>\n",
364 |        "    </tr>\n",
365 |        "    <tr>\n",
366 |        "      <th>2</th>\n",
367 |        "      <td>Creating a Facebook ad campaign within a specific budget and targe...</td>\n",
368 |        "      <td>give me a facebook ads campaign for the next 7 days for canva pro ...</td>\n",
369 |        "      <td>jordan||canva</td>\n",
370 |        "      <td>Create a comprehensive Facebook ads campaign plan for selling Canv...</td>\n",
371 |        "      <td>Certainly! Below is a comprehensive 7-day Facebook Ads campaign pl...</td>\n",
372 |        "      <td>Certainly! Here's a 7-day Facebook ads campaign plan for selling C...</td>\n",
373 |        "      <td>✔️ [1.000]</td>\n",
374 |        "    </tr>\n",
375 |        "    <tr>\n",
376 |        "      <th>3</th>\n",
377 |        "      <td>The delivery was scheduled for today, but due to a payment delay t...</td>\n",
378 |        "      <td>The delivery supposed to be today. Seems our payment delay to Lvio...</td>\n",
379 |        "      <td>seagate||lviv||ms. chan</td>\n",
380 |        "      <td>Please rewrite the following message in a professional and clear m...</td>\n",
381 |        "      <td>The delivery was originally scheduled for today; however, due to a...</td>\n",
382 |        "      <td>The delivery was originally scheduled for today; however, due to a...</td>\n",
383 |        "      <td>✔️ [0.833]</td>\n",
384 |        "    </tr>\n",
385 |        "    <tr>\n",
386 |        "      <th>4</th>\n",
387 |        "      <td>Dear Ms. Amanda, I hope this email finds you well. I wanted to bri...</td>\n",
388 |        "      <td>write a email to Ms, Amanda ; tell her, we have a way to overcome ...</td>\n",
389 |        "      <td>india||amanda||hermann(germany)||china||vims(france)</td>\n",
390 |        "      <td>Write a professional email to Ms. Amanda explaining that we have a...</td>\n",
391 |        "      <td>Subject: Proposal to Overcome Certification Challenges and Product...</td>\n",
392 |        "      <td>Dear Ms. Amanda, I hope you are well. We have identified a way to ...</td>\n",
393 |        "      <td>✔️ [0.700]</td>\n",
394 |        "    </tr>\n",
395 |        "  </tbody>\n",
396 |        "</table>\n",
397 |        "</div>"
398 |       ],
399 |       "text/plain": [
400 |        "                                                         target_response  \\\n",
401 |        "0  Subject: Follow-up on Endpoint Management Solution Discussion Dear...   \n",
402 |        "1  Кампания \"Share a Coke\" была запущена Coca-Cola в 2011 году в Авст...   \n",
403 |        "2  Creating a Facebook ad campaign within a specific budget and targe...   \n",
404 |        "3  The delivery was scheduled for today, but due to a payment delay t...   \n",
405 |        "4  Dear Ms. Amanda, I hope this email finds you well. I wanted to bri...   \n",
406 |        "\n",
407 |        "                                                              user_query  \\\n",
408 |        "0  rewrite in professional \"as discusseed in last meeting with you an...   \n",
409 |        "1                      расскажи про PR-кампанию Coca-Cola “Share a Coke\"   \n",
410 |        "2  give me a facebook ads campaign for the next 7 days for canva pro ...   \n",
411 |        "3  The delivery supposed to be today. Seems our payment delay to Lvio...   \n",
412 |        "4  write a email to Ms, Amanda ; tell her, we have a way to overcome ...   \n",
413 |        "\n",
414 |        "                                                pii_str  \\\n",
415 |        "0                                 effem||balaji||balaji   \n",
416 |        "1                                             coca-cola   \n",
417 |        "2                                         jordan||canva   \n",
418 |        "3                               seagate||lviv||ms. chan   \n",
419 |        "4  india||amanda||hermann(germany)||china||vims(france)   \n",
420 |        "\n",
421 |        "                                                             llm_request  \\\n",
422 |        "0  Rewrite the following message in a professional tone, ensuring cla...   \n",
423 |        "1  Please provide a detailed summary of Coca-Cola's \"Share a Coke\" PR...   \n",
424 |        "2  Create a comprehensive Facebook ads campaign plan for selling Canv...   \n",
425 |        "3  Please rewrite the following message in a professional and clear m...   \n",
426 |        "4  Write a professional email to Ms. Amanda explaining that we have a...   \n",
427 |        "\n",
428 |        "                                                            llm_response  \\\n",
429 |        "0  Subject: Follow-up on E-FEM Account Management and Solution Finali...   \n",
430 |        "1  Certainly! Here is a detailed summary of Coca-Cola's \"Share a Coke...   \n",
431 |        "2  Certainly! Below is a comprehensive 7-day Facebook Ads campaign pl...   \n",
432 |        "3  The delivery was originally scheduled for today; however, due to a...   \n",
433 |        "4  Subject: Proposal to Overcome Certification Challenges and Product...   \n",
434 |        "\n",
435 |        "                                                                response  \\\n",
436 |        "0  Subject: Clarification on E-FEM Account Management and Next Steps ...   \n",
437 |        "1  Кампания Coca-Cola \"Share a Coke\" — это очень успешная маркетингов...   \n",
438 |        "2  Certainly! Here's a 7-day Facebook ads campaign plan for selling C...   \n",
439 |        "3  The delivery was originally scheduled for today; however, due to a...   \n",
440 |        "4  Dear Ms. Amanda, I hope you are well. We have identified a way to ...   \n",
441 |        "\n",
442 |        "  compute_overall_score  \n",
443 |        "0                        \n",
444 |        "1            ✔️ [1.000]  \n",
445 |        "2            ✔️ [1.000]  \n",
446 |        "3            ✔️ [0.833]  \n",
447 |        "4            ✔️ [0.700]  "
448 |       ]
449 |      },
450 |      "metadata": {},
451 |      "output_type": "display_data"
452 |     },
453 |     {
454 |      "data": {
455 |       "text/html": [
456 |        "\n",
457 |        "            <div style='\n",
458 |        "                text-align: center;\n",
459 |        "                font-size: 16px;\n",
460 |        "                font-weight: bold;\n",
461 |        "                color: #555;\n",
462 |        "                margin: 10px 0;'>\n",
463 |        "                ... 209 more rows not displayed ...\n",
464 |        "            </div>\n",
465 |        "            "
466 |       ],
467 |       "text/plain": [
468 |        "<IPython.core.display.HTML object>"
469 |       ]
470 |      },
471 |      "metadata": {},
472 |      "output_type": "display_data"
473 |     },
474 |     {
475 |      "data": {
476 |       "text/plain": [
477 |        "EvaluationResult(score=76.5, results=<list of 214 results>)"
478 |       ]
479 |      },
480 |      "execution_count": 6,
481 |      "metadata": {},
482 |      "output_type": "execute_result"
483 |     }
484 |    ],
485 |    "source": [
486 |     "zeroshot = PAPILLON(untrusted_model=large_lm)\n",
487 |     "\n",
488 |     "kwargs = dict(num_threads=16, display_progress=True, display_table=5, max_errors=100)\n",
489 |     "evaluate = dspy.Evaluate(metric=compute_overall_score, devset=testset, **kwargs)\n",
490 |     "evaluate(zeroshot)"
491 |    ]
492 |   },
493 |   {
494 |    "cell_type": "markdown",
495 |    "metadata": {},
496 |    "source": [
497 |     "### Optimize PAPILLON with `dspy.GEPA`\n",
498 |     "\n",
499 |     "GEPA is a _reflective_ prompt optimizer, and it's strength lies in being able to view textual feedback from the DSPy program's execution and evaluation pipelines, which provides GEPA more visibility into why the system got the score that it did, and then GEPA can introspect to identify how to improve the score. Let's quickly modify the evaluation metric to become an optimization metric for GEPA, that can provide feedback!\n",
500 |     "\n",
501 |     "In this case, since the evaluation metric is an aggregate of 2 distinct scores, \"quality\" score and \"leakage\" score, the feedback metric can be as simple as showing what the quality and leakage scores are, so GEPA can reflect on what needs to be improved!"
502 |    ]
503 |   },
504 |   {
505 |    "cell_type": "code",
506 |    "execution_count": 7,
507 |    "metadata": {},
508 |    "outputs": [],
509 |    "source": [
510 |     "def compute_overall_score_with_feedback(gold, pred, trace=None, pred_name=None, pred_trace=None):\n",
511 |     "    metrics = compute_metrics(gold, pred, trace)\n",
512 |     "    overall_score = (metrics.quality + (1 - metrics.leakage)) / 2.0\n",
513 |     "    feedback_text = f\"The overall score is {overall_score:.2f}, which is the arithmetic mean of the quality score ({metrics.quality:.2f}) and the leakage score ({1 - metrics.leakage:.2f}). Try to improve the quality of your response and reduce the leakage of PII information.\"\n",
514 |     "    return dspy.Prediction(\n",
515 |     "        score=overall_score,\n",
516 |     "        feedback=feedback_text,\n",
517 |     "    )"
518 |    ]
519 |   },
520 |   {
521 |    "cell_type": "markdown",
522 |    "metadata": {},
523 |    "source": [
524 |     "Notice how the metric function we had already defined provided all the components we need for this feedback function! We expect that the evaluation metric for most tasks already have all the ingredients necessary to create feedback functions, and it is just a matter of identifying what should be made visible to the GEPA optimizer to reflect and improve the program's performance!"
525 |    ]
526 |   },
527 |   {
528 |    "cell_type": "markdown",
529 |    "metadata": {},
530 |    "source": [
531 |     "Let's use GEPA on PAPILLON. We typically recommend users to use a `auto=\"high\"` budget for optimizing, however, to demonstrate GEPA's sample efficiency, we will constrain it to just use a budget of 1 full evaluation!"
532 |    ]
533 |   },
534 |   {
535 |    "cell_type": "code",
536 |    "execution_count": 8,
537 |    "metadata": {},
538 |    "outputs": [
539 |     {
540 |      "name": "stderr",
541 |      "output_type": "stream",
542 |      "text": [
543 |       "2025/08/12 18:38:42 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 450 metric calls of the program. This amounts to 1.00 full evals on the train+val set.\n",
544 |       "2025/08/12 18:38:42 INFO dspy.teleprompt.gepa.gepa: Using 225 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n",
545 |       "2025/08/12 18:38:58 INFO dspy.evaluate.evaluate: Average Metric: 179.6126262626263 / 225 (79.8%)\n",
546 |       "2025/08/12 18:38:58 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.7982783389450057\n",
547 |       "2025/08/12 18:38:58 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.7982783389450057\n",
548 |       "Average Metric: 2.00 / 3 (66.7%): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 13.20it/s]\n",
549 |       "2025/08/12 18:38:59 INFO dspy.evaluate.evaluate: Average Metric: 2.0 / 3 (66.7%)\n",
550 |       "\n",
551 |       "2025/08/12 18:39:13 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for craft_redacted_request.predict: You are given a private user query as input. Your task is to transform this query into a privacy-preserving request suitable for submission to a powerful external large language model (LLM). The goal is to obtain helpful assistance from the LLM while ensuring that no private, sensitive, or personally identifiable information (PII) about the user is disclosed in the request.\n",
552 |       "\n",
553 |       "Follow these steps:\n",
554 |       "\n",
555 |       "1. **Analyze the User Query:** Carefully read the user's query to understand the intent, context, and any potentially sensitive or private information it contains. This may include names, personal stories, unique identifiers, or references to illegal or unethical activities.\n",
556 |       "\n",
557 |       "2. **Reasoning Step:** Briefly explain your reasoning process. Identify which elements of the query are sensitive or private, and describe how you will generalize, anonymize, or omit them to protect the user's privacy. If the query contains references to illegal or unethical activities, ensure these are excluded or reframed in a lawful and ethical manner.\n",
558 |       "\n",
559 |       "3. **Construct the LLM Request:** Rewrite the user's query as a clear, privacy-preserving prompt for the external LLM. \n",
560 |       "    - Remove or generalize any PII or sensitive details (e.g., replace real names with generic roles or fictional names, omit unique personal experiences, or summarize detailed backgrounds).\n",
561 |       "    - If the query requests creative content (e.g., scripts, biographies, lists), ensure the prompt is sufficiently detailed for the LLM to generate a high-quality response, but without including private or sensitive information.\n",
562 |       "    - If the query includes illegal or unethical activities, exclude these from the LLM request and focus on lawful, ethical alternatives.\n",
563 |       "    - Maintain the original intent and utility of the user's request as much as possible.\n",
564 |       "\n",
565 |       "4. **Output Format:** Your response should include two sections:\n",
566 |       "    - **reasoning:** A concise explanation of your privacy-preserving strategy for this specific query.\n",
567 |       "    - **llm_request:** The rewritten, privacy-preserving prompt to be sent to the external LLM.\n",
568 |       "\n",
569 |       "**Additional Notes:**\n",
570 |       "- Strive for a balance between preserving user privacy and maintaining the quality and usefulness of the LLM request.\n",
571 |       "- Avoid over-generalizing to the point where the LLM cannot provide a meaningful response.\n",
572 |       "- Do not include any information in the LLM request that could be used to identify the user or any other real individual.\n",
573 |       "- If the user query is already privacy-preserving, you may use it as-is, but still provide a brief reasoning statement confirming this.\n",
574 |       "\n",
575 |       "This process ensures that user privacy is protected while leveraging the capabilities of external language models for a wide range of tasks, including creative writing, information synthesis, and idea generation.\n",
576 |       "2025/08/12 18:40:01 INFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\n",
577 |       "2025/08/12 18:45:52 INFO dspy.evaluate.evaluate: Average Metric: 189.65 / 225 (84.3%)\n",
578 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New program is on the linear pareto front\n",
579 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset score for new program: 0.8428888888888889\n",
580 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full train_val score for new program: 0.8428888888888889\n",
581 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Individual valset scores for new program: [1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.7, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5, 1.0, 0.875, 0.6666666666666667, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666667, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.75, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.8333333333333334]\n",
582 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New valset pareto front scores: [1.0, 0.5, 1.0, 1.0, 0.75, 1.0, 0.8333333333333334, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.7, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.75, 1.0, 1.0, 0.6666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.875, 1.0, 1.0, 1.0, 1.0, 0.5, 0.9, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5, 1.0, 0.875, 0.6666666666666667, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.75, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8333333333333334]\n",
583 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset pareto front score: 0.9004444444444444\n",
584 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Updated valset pareto front programs: [{1}, {0, 1}, {1}, {1}, {0}, {0, 1}, {0}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0}, {1}, {0}, {0}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0}, {0, 1}, {1}, {0, 1}, {1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {1}, {0, 1}, {1}, {1}, {0, 1}, {1}, {0, 1}, {1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {1}, {0, 1}, {0, 1}, {1}, {1}, {0, 1}, {0, 1}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {1}, {0, 1}, {0, 1}, {1}, {0, 1}, {1}, {0, 1}, {0, 1}, {1}, {1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0}, {1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {1}, {1}, {0, 1}, {0}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}, {0, 1}, {1}, {0}, {0, 1}, {0, 1}, {0}, {0, 1}, {0, 1}]\n",
585 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best valset aggregate score so far: 0.8428888888888889\n",
586 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on train_val: 1\n",
587 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on valset: 1\n",
588 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on valset: 0.8428888888888889\n",
589 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on train_val: 0.8428888888888889\n",
590 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Linear pareto front program index: 1\n",
591 |       "2025/08/12 18:45:52 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New program candidate index: 1\n"
592 |      ]
593 |     }
594 |    ],
595 |    "source": [
596 |     "from dspy import GEPA\n",
597 |     "\n",
598 |     "papillon = PAPILLON(untrusted_model=large_lm)\n",
599 |     "papillon.set_lm(local_lm)\n",
600 |     "\n",
601 |     "compiler = GEPA(\n",
602 |     "    metric=compute_overall_score_with_feedback,\n",
603 |     "    reflection_lm=dspy.LM(model=\"openai/gpt-4.1\", api_key=api_key),\n",
604 |     "    num_threads=16,\n",
605 |     "    track_stats=True,\n",
606 |     "    track_best_outputs=True,\n",
607 |     "\n",
608 |     "    # Set the budget. GEPA accepts any one of \"auto\" or \"max_full_evals\" arguments.\n",
609 |     "    # GEPA scales with higher budget. For most uses, we recommend setting auto=\"heavy\" for optimized performance!\n",
610 |     "    # auto=\"heavy\", \n",
611 |     "    max_full_evals=1 # <-- For this demonstration, we will allow GEPA to just perform just 1 full evaluation!\n",
612 |     ")\n",
613 |     "\n",
614 |     "optimized_papillon = compiler.compile(\n",
615 |     "    student=papillon,\n",
616 |     "    trainset=trainset,\n",
617 |     "    valset=devset,\n",
618 |     ")"
619 |    ]
620 |   },
621 |   {
622 |    "cell_type": "markdown",
623 |    "metadata": {},
624 |    "source": [
625 |     "### Display the GEPA generated prompt\n",
626 |     "\n",
627 |     "Note that since we allowed GEPA the budget to only generate 1 candidate, it has updated the prompt for only one of the predictors"
628 |    ]
629 |   },
630 |   {
631 |    "cell_type": "code",
632 |    "execution_count": 9,
633 |    "metadata": {},
634 |    "outputs": [
635 |     {
636 |      "name": "stdout",
637 |      "output_type": "stream",
638 |      "text": [
639 |       "Task Description:\n",
640 |       "You will be given a private user query that may contain sensitive, personal, or identifying information. Your task is to transform this user query into a privacy-preserving prompt or request suitable for a powerful external large language model (LLM). The generated request should enable the external LLM to assist with the user’s original intent effectively, while ensuring that no private, personally identifiable, or sensitive information about the user is exposed or leaked.\n",
641 |       "\n",
642 |       "Input Format:\n",
643 |       "- The user query is a free-form text input that may include explicit personal details, real names, descriptive traits, or specific events.\n",
644 |       "- The input may range from requests for creative writing, role descriptions, lists of hobbies, profiles, or any other user-generated content.\n",
645 |       "  \n",
646 |       "Detailed Task Requirements and Domain-Specific Considerations:\n",
647 |       "1. Privacy Preservation:\n",
648 |       "   - Do not include any directly identifying details, real names, or sensitive personal information in the generated request.\n",
649 |       "   - Replace or anonymize any personal data, including but not limited to names, dates, biographical details, or events tied to individuals.\n",
650 |       "   - Avoid leaking private user context or information that could be traced back to the user.\n",
651 |       "\n",
652 |       "2. Maintaining Task Integrity:\n",
653 |       "   - The LLM request must capture the user’s original intention and task goals as faithfully as possible, phrased in a generalized and context-neutral way.\n",
654 |       "   - For content requests involving real-world entities (e.g., institutions, events), consider making the scenario fictionalized or anonymized if it involves potentially identifying or sensitive context.\n",
655 |       "   - For character or profile descriptions, abstract/remove intimate personal details but preserve key personality traits or narrative elements in a generalized manner.\n",
656 |       "   \n",
657 |       "3. Quality of Generated LLM Request:\n",
658 |       "   - The description should be clear, coherent, and sufficiently detailed to enable a relevant and helpful response from the LLM without needing further clarifications.\n",
659 |       "   - Avoid introducing new biases or assumptions outside the user’s input.\n",
660 |       "   - Exclude illegal or ethically sensitive activities where applicable, or transform them into acceptable analogues to maintain ethical standards.\n",
661 |       "\n",
662 |       "4. Reasoning Component:\n",
663 |       "   - Alongside the final LLM request, provide a concise reasoning summary that explains how private data was identified and sanitized, the thought process behind generalization or anonymization, and how the final prompt preserves user intent while protecting privacy.\n",
664 |       "\n",
665 |       "Generalizable Strategy:\n",
666 |       "- Identify sensitive or private details in the user query.\n",
667 |       "- Abstract or anonymize such details without losing essential meaning.\n",
668 |       "- Frame the request in terms of generic or fictional scenarios.\n",
669 |       "- Explicitly instruct the external LLM to avoid real or private details and to focus on a neutral, generic, or fictional perspective when relevant.\n",
670 |       "- Validate that no PII or identifiable content remains in the final prompt.\n",
671 |       "\n",
672 |       "Outputs:\n",
673 |       "- A reasoning section summarizing privacy considerations and prompt construction approach.\n",
674 |       "- A final privacy-preserving LLM prompt/request ready to be sent to an external LLM.\n",
675 |       "\n",
676 |       "This approach ensures strong privacy protection aligned with best practices while maximizing task quality and relevance.\n"
677 |      ]
678 |     }
679 |    ],
680 |    "source": [
681 |     "print(optimized_papillon.craft_redacted_request.predict.signature.instructions)"
682 |    ]
683 |   },
684 |   {
685 |    "cell_type": "code",
686 |    "execution_count": 10,
687 |    "metadata": {},
688 |    "outputs": [
689 |     {
690 |      "name": "stdout",
691 |      "output_type": "stream",
692 |      "text": [
693 |       "Average Metric: 184.26 / 214 (86.1%): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [00:13<00:00, 15.90it/s]"
694 |      ]
695 |     },
696 |     {
697 |      "name": "stderr",
698 |      "output_type": "stream",
699 |      "text": [
700 |       "2025/08/12 18:27:34 INFO dspy.evaluate.evaluate: Average Metric: 184.2602564102564 / 214 (86.1%)\n"
701 |      ]
[TRUNCATED]
```

tutorials/games/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Fine-tuning Agents\n",
8 |     "\n",
9 |     "Let's walk through a quick example of optimizing the _language model weights_ (i.e., fine-tuning) inside a DSPy module that represents a ReAct agent playing a game with 50-step tasks.\n",
10 |     "\n",
11 |     "### Install dependencies and download data\n",
12 |     "\n",
13 |     "Install the latest DSPy via `pip install -U dspy` and follow along. This tutorial uses the AlfWorld dataset, which depends on DSPy 2.6.0.\n",
14 |     "\n",
15 |     "You will also need the following dependencies:\n",
16 |     "\n",
17 |     "```shell\n",
18 |     "> pip install -U alfworld==0.3.5 multiprocess\n",
19 |     "> alfworld-download\n",
20 |     "```"
21 |    ]
22 |   },
23 |   {
24 |    "cell_type": "markdown",
25 |    "metadata": {},
26 |    "source": [
27 |     "<details>\n",
28 |     "<summary>Recommended: Setup MLflow Tracing for learning what's happening under the hood</summary>\n",
29 |     "\n",
30 |     "### MLflow DSPy Integration\n",
31 |     "\n",
32 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
33 |     "\n",
34 |     "![MLflow Trace](./mlflow-tracing-agent.png)\n",
35 |     "\n",
36 |     "1. Install MLflow\n",
37 |     "\n",
38 |     "```bash\n",
39 |     "%pip install mlflow>=2.20\n",
40 |     "```\n",
41 |     "\n",
42 |     "2. Start MLflow UI in a separate terminal\n",
43 |     "```bash\n",
44 |     "mlflow ui --port 5000\n",
45 |     "```\n",
46 |     "\n",
47 |     "3. Connect the notebook to MLflow\n",
48 |     "```python\n",
49 |     "import mlflow\n",
50 |     "\n",
51 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
52 |     "mlflow.set_experiment(\"DSPy\")\n",
53 |     "```\n",
54 |     "\n",
55 |     "4. Enabling tracing.\n",
56 |     "```python\n",
57 |     "mlflow.dspy.autolog()\n",
58 |     "```\n",
59 |     "\n",
60 |     "\n",
61 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
62 |     "</details>"
63 |    ]
64 |   },
65 |   {
66 |    "cell_type": "markdown",
67 |    "metadata": {},
68 |    "source": [
69 |     "### Set up the language models\n",
70 |     "\n",
71 |     "Our goal is to allow `gpt-4o-mini` to play the AlfWorld household game proficiently, without tinkering with string prompts or example trajectories by hand.\n",
72 |     "\n",
73 |     "Though it's not strictly necessary, we'll make our job a little easier by using the larger `gpt-4o` for prompt optimization and fine-tuning, building our small `gpt-4o-mini` agent."
74 |    ]
75 |   },
76 |   {
77 |    "cell_type": "code",
78 |    "execution_count": 1,
79 |    "metadata": {},
80 |    "outputs": [],
81 |    "source": [
82 |     "import dspy\n",
83 |     "\n",
84 |     "gpt4o_mini = dspy.LM('gpt-4o-mini-2024-07-18')\n",
85 |     "gpt4o = dspy.LM('openai/gpt-4o')\n",
86 |     "dspy.configure(experimental=True)"
87 |    ]
88 |   },
89 |   {
90 |    "cell_type": "markdown",
91 |    "metadata": {},
92 |    "source": [
93 |     "Let's load 200 training and 200 development tasks from AlfWorld. The dataset is much larger, but a small number of examples will help keep this tutorial run in 1-2 hours, including fine-tuning.\n",
94 |     "\n",
95 |     "With just 100 training tasks, we'll teach 4o-mini to go from 19% (can barely play the game) to 72%. If you use 500 tasks and retain the demonstrations during fine-tuning, you can push that easily to 82%."
96 |    ]
97 |   },
98 |   {
99 |    "cell_type": "code",
100 |    "execution_count": 2,
101 |    "metadata": {},
102 |    "outputs": [
103 |     {
104 |      "data": {
105 |       "text/plain": [
106 |        "(200, 200)"
107 |       ]
108 |      },
109 |      "execution_count": 2,
110 |      "metadata": {},
111 |      "output_type": "execute_result"
112 |     }
113 |    ],
114 |    "source": [
115 |     "from dspy.datasets.alfworld import AlfWorld\n",
116 |     "\n",
117 |     "alfworld = AlfWorld()\n",
118 |     "trainset, devset = alfworld.trainset[:200], alfworld.devset[-200:]\n",
119 |     "len(trainset), len(devset)"
120 |    ]
121 |   },
122 |   {
123 |    "cell_type": "markdown",
124 |    "metadata": {},
125 |    "source": [
126 |     "Before we proceed, let's view an example of this task."
127 |    ]
128 |   },
129 |   {
130 |    "cell_type": "code",
131 |    "execution_count": 3,
132 |    "metadata": {},
133 |    "outputs": [
134 |     {
135 |      "name": "stdout",
136 |      "output_type": "stream",
137 |      "text": [
138 |       "-= Welcome to TextWorld, ALFRED! =-\n",
139 |       "\n",
140 |       "You are in the middle of a room. Looking quickly around you, you see a countertop 1, a drawer 8, a drawer 7, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a handtowelholder 1, a sinkbasin 2, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1.\n",
141 |       "\n",
142 |       "Your task is to: put a clean soapbar in garbagecan.\n"
143 |      ]
144 |     }
145 |    ],
146 |    "source": [
147 |     "example = trainset[0]\n",
148 |     "\n",
149 |     "with alfworld.POOL.session() as env:\n",
150 |     "    task, info = env.init(**example.inputs())\n",
151 |     "\n",
152 |     "print(task)"
153 |    ]
154 |   },
155 |   {
156 |    "cell_type": "markdown",
157 |    "metadata": {},
158 |    "source": [
159 |     "### Defining the Agent program\n",
160 |     "\n",
161 |     "The agent is a pretty simple `dspy.Module` with one sub-module called `self.react`.\n",
162 |     "\n",
163 |     "This sub-module consumes a definition of a specific `task`, sees its previous `trajectory`, and sees a list of\n",
164 |     "`possible_actions` it can take. It responds simply with the next action.\n",
165 |     "\n",
166 |     "In the `forward` method, we just initialize an environment for the given task `idx`. And we loop up to `self.max_iters`,\n",
167 |     "repeatedly invoking the `self.react` module to take the next action."
168 |    ]
169 |   },
170 |   {
171 |    "cell_type": "code",
172 |    "execution_count": 4,
173 |    "metadata": {},
174 |    "outputs": [],
175 |    "source": [
176 |     "class Agent(dspy.Module):\n",
177 |     "    def __init__(self, max_iters=50, verbose=False):\n",
178 |     "        self.max_iters = max_iters\n",
179 |     "        self.verbose = verbose\n",
180 |     "        self.react = dspy.Predict(\"task, trajectory, possible_actions: list[str] -> action\")\n",
181 |     "\n",
182 |     "    def forward(self, idx):\n",
183 |     "        with alfworld.POOL.session() as env:\n",
184 |     "            trajectory = []\n",
185 |     "            task, info = env.init(idx)\n",
186 |     "            if self.verbose:\n",
187 |     "                print(f\"Task: {task}\")\n",
188 |     "\n",
189 |     "            for _ in range(self.max_iters):\n",
190 |     "                trajectory_ = \"\\n\".join(trajectory)\n",
191 |     "                possible_actions = info[\"admissible_commands\"][0] + [\"think: ${...thoughts...}\"]\n",
192 |     "                prediction = self.react(task=task, trajectory=trajectory_, possible_actions=possible_actions)\n",
193 |     "                trajectory.append(f\"> {prediction.action}\")\n",
194 |     "\n",
195 |     "                if prediction.action.startswith(\"think:\"):\n",
196 |     "                    trajectory.append(\"OK.\")\n",
197 |     "                    continue\n",
198 |     "\n",
199 |     "                obs, reward, done, info = env.step(prediction.action)\n",
200 |     "                obs, reward, done = obs[0], reward[0], done[0]\n",
201 |     "                trajectory.append(obs)\n",
202 |     "\n",
203 |     "                if self.verbose:\n",
204 |     "                    print(\"\\n\".join(trajectory[-2:]))\n",
205 |     "\n",
206 |     "                if done:\n",
207 |     "                    break\n",
208 |     "\n",
209 |     "        assert reward == int(info[\"won\"][0]), (reward, info[\"won\"][0])\n",
210 |     "        return dspy.Prediction(trajectory=trajectory, success=reward)"
211 |    ]
212 |   },
213 |   {
214 |    "cell_type": "markdown",
215 |    "metadata": {},
216 |    "source": [
217 |     "#### Aside: If you wanted to include instructions for your agent...\n",
218 |     "\n",
219 |     "Above, we opted to keep the agent super simple, without even providing short instructions that describe the task.\n",
220 |     "\n",
221 |     "In principle, you can copy a short definition of the AlfWorld task (based on Yao et al., 2022) and use that as the\n",
222 |     "instruction for your agent. This is not inherently essential, but it helps illustrate the role that\n",
223 |     "instructions play in DSPy: they're not for coercing the model to exhibit a certain behavior, but they're there to\n",
224 |     "describe the fundamentals of the task in a straightforward, human-readable way.\n",
225 |     "\n",
226 |     "If you want to do that, you can simply replace this:\n",
227 |     "\n",
228 |     "```python\n",
229 |     "self.react = dspy.Predict(\"task, trajectory, possible_actions: list[str] -> action\")\n",
230 |     "```\n",
231 |     "\n",
232 |     "with this:\n",
233 |     "\n",
234 |     "```python\n",
235 |     "INSTRUCTIONS = \"\"\"\n",
236 |     "Interact with a simulated household to achieve a high-level goal. Make sure to plan, track subgoals,\n",
237 |     "determine likely locations for common household items (e.g. desklamps will likely be on desks, shelfs, or dressers),\n",
238 |     "and explore systematically (e.g. check all desks one by one for desklamp).\n",
239 |     "\"\"\".strip()\n",
240 |     "\n",
241 |     "self.react = dspy.Predict(dspy.Signature(\"task, trajectory, possible_actions: list[str] -> action\", INSTRUCTIONS))\n",
242 |     "```"
243 |    ]
244 |   },
245 |   {
246 |    "cell_type": "markdown",
247 |    "metadata": {},
248 |    "source": [
249 |     "### Zero-shot evaluation"
250 |    ]
251 |   },
252 |   {
253 |    "cell_type": "markdown",
254 |    "metadata": {},
255 |    "source": [
256 |     "Now, let's try this simple program, prior to any optimization work."
257 |    ]
258 |   },
259 |   {
260 |    "cell_type": "code",
261 |    "execution_count": 5,
262 |    "metadata": {},
263 |    "outputs": [
264 |     {
265 |      "name": "stdout",
266 |      "output_type": "stream",
267 |      "text": [
268 |       "Task: -= Welcome to TextWorld, ALFRED! =-\n",
269 |       "\n",
270 |       "You are in the middle of a room. Looking quickly around you, you see a countertop 1, a drawer 8, a drawer 7, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a handtowelholder 1, a sinkbasin 2, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1.\n",
271 |       "\n",
272 |       "Your task is to: put a clean soapbar in garbagecan.\n",
273 |       "> go to countertop 1\n",
274 |       "You arrive at countertop 1. On the countertop 1, you see a candle 1, a soapbar 1, a soapbottle 2, a soapbottle 1, and a spraybottle 1.\n",
275 |       "> take soapbar 1 from countertop 1\n",
276 |       "You pick up the soapbar 1 from the countertop 1.\n",
277 |       "> go to garbagecan 1\n",
278 |       "You arrive at garbagecan 1. On the garbagecan 1, you see nothing.\n",
279 |       "> move soapbar 1 to garbagecan 1\n",
280 |       "You move the soapbar 1 to the garbagecan 1.\n",
281 |       "> examine garbagecan 1\n",
282 |       "On the garbagecan 1, you see a soapbar 1.\n",
283 |       "> examine garbagecan 1\n",
284 |       "On the garbagecan 1, you see a soapbar 1.\n",
285 |       "> examine garbagecan 1\n",
286 |       "On the garbagecan 1, you see a soapbar 1.\n",
287 |       "> examine garbagecan 1\n",
288 |       "On the garbagecan 1, you see a soapbar 1.\n",
289 |       "> look\n",
290 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
291 |       "> look\n",
292 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
293 |       "> look\n",
294 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
295 |       "> take soapbar 1 from garbagecan 1\n",
296 |       "You pick up the soapbar 1 from the garbagecan 1.\n",
297 |       "> move soapbar 1 to garbagecan 1\n",
298 |       "You move the soapbar 1 to the garbagecan 1.\n",
299 |       "> examine garbagecan 1\n",
300 |       "On the garbagecan 1, you see a soapbar 1.\n",
301 |       "> examine garbagecan 1\n",
302 |       "On the garbagecan 1, you see a soapbar 1.\n",
303 |       "> examine garbagecan 1\n",
304 |       "On the garbagecan 1, you see a soapbar 1.\n",
305 |       "> examine garbagecan 1\n",
306 |       "On the garbagecan 1, you see a soapbar 1.\n",
307 |       "> examine garbagecan 1\n",
308 |       "On the garbagecan 1, you see a soapbar 1.\n",
309 |       "> look\n",
310 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
311 |       "> look\n",
312 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
313 |       "> look\n",
314 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
315 |       "> look\n",
316 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
317 |       "> examine garbagecan 1\n",
318 |       "On the garbagecan 1, you see a soapbar 1.\n",
319 |       "> look\n",
320 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
321 |       "> look\n",
322 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
323 |       "> look\n",
324 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
325 |       "> look\n",
326 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
327 |       "> look\n",
328 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
329 |       "> examine garbagecan 1\n",
330 |       "On the garbagecan 1, you see a soapbar 1.\n",
331 |       "> look\n",
332 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
333 |       "> look\n",
334 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
335 |       "> look\n",
336 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
337 |       "> look\n",
338 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
339 |       "> look\n",
340 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
341 |       "> look\n",
342 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
343 |       "> look\n",
344 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
345 |       "> look\n",
346 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
347 |       "> look\n",
348 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
349 |       "> look\n",
350 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
351 |       "> look\n",
352 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
353 |       "> look\n",
354 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
355 |       "> look\n",
356 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
357 |       "> look\n",
358 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
359 |       "> look\n",
360 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
361 |       "> look\n",
362 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
363 |       "> look\n",
364 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
365 |       "> look\n",
366 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
367 |       "> look\n",
368 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n",
369 |       "> examine garbagecan 1\n",
370 |       "On the garbagecan 1, you see a soapbar 1.\n",
371 |       "> look\n",
372 |       "You are facing the garbagecan 1. Next to it, you see nothing.\n"
373 |      ]
374 |     },
375 |     {
376 |      "data": {
377 |       "text/plain": [
378 |        "Prediction(\n",
379 |        "    trajectory=['> go to countertop 1', 'You arrive at countertop 1. On the countertop 1, you see a candle 1, a soapbar 1, a soapbottle 2, a soapbottle 1, and a spraybottle 1.', '> take soapbar 1 from countertop 1', 'You pick up the soapbar 1 from the countertop 1.', '> go to garbagecan 1', 'You arrive at garbagecan 1. On the garbagecan 1, you see nothing.', '> move soapbar 1 to garbagecan 1', 'You move the soapbar 1 to the garbagecan 1.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> take soapbar 1 from garbagecan 1', 'You pick up the soapbar 1 from the garbagecan 1.', '> move soapbar 1 to garbagecan 1', 'You move the soapbar 1 to the garbagecan 1.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.', '> examine garbagecan 1', 'On the garbagecan 1, you see a soapbar 1.', '> look', 'You are facing the garbagecan 1. Next to it, you see nothing.'],\n",
380 |        "    success=0\n",
381 |        ")"
382 |       ]
383 |      },
384 |      "execution_count": 5,
385 |      "metadata": {},
386 |      "output_type": "execute_result"
387 |     }
388 |    ],
389 |    "source": [
390 |     "agent_4o = Agent()\n",
391 |     "agent_4o.set_lm(gpt4o)\n",
392 |     "agent_4o.verbose = True\n",
393 |     "\n",
394 |     "agent_4o(**example.inputs())"
395 |    ]
396 |   },
397 |   {
398 |    "cell_type": "markdown",
399 |    "metadata": {},
400 |    "source": [
401 |     "Okay, in this case it couldn't solve this example! Now, let's check the average quality of 4o and 4o-mini."
402 |    ]
403 |   },
404 |   {
405 |    "cell_type": "code",
406 |    "execution_count": 6,
407 |    "metadata": {},
408 |    "outputs": [],
409 |    "source": [
410 |     "metric = lambda x, y, trace=None: y.success\n",
411 |     "evaluate = dspy.Evaluate(devset=devset, metric=metric, display_progress=True, num_threads=16)"
412 |    ]
413 |   },
414 |   {
415 |    "cell_type": "markdown",
416 |    "metadata": {},
417 |    "source": [
418 |     "<details>\n",
419 |     "<summary>Tracking Evaluation Results in MLflow Experiment</summary>\n",
420 |     "\n",
421 |     "<br/>\n",
422 |     "\n",
423 |     "To track and visualize the evaluation results over time, you can record the results in MLflow Experiment.\n",
424 |     "\n",
425 |     "\n",
426 |     "```python\n",
427 |     "import mlflow\n",
428 |     "\n",
429 |     "with mlflow.start_run(run_name=\"agent_evaluation\"):\n",
430 |     "    evaluate = dspy.Evaluate(\n",
431 |     "        devset=devset,\n",
432 |     "        metric=metric,\n",
433 |     "        num_threads=16,\n",
434 |     "        display_progress=True,\n",
435 |     "        # To record the outputs and detailed scores to MLflow\n",
436 |     "        return_all_scores=True,\n",
437 |     "        return_outputs=True,\n",
438 |     "    )\n",
439 |     "\n",
440 |     "    # Evaluate the program as usual\n",
441 |     "    aggregated_score, outputs, all_scores = evaluate(cot)\n",
442 |     "\n",
443 |     "    # Log the aggregated score\n",
444 |     "    mlflow.log_metric(\"success_rate\", aggregated_score)\n",
445 |     "    # Log the detailed evaluation results as a table\n",
446 |     "    mlflow.log_table(\n",
447 |     "        {\n",
448 |     "            \"Idx\": [example.idx for example in eval_set],\n",
449 |     "            \"Result\": outputs,\n",
450 |     "            \"Success\": all_scores,\n",
451 |     "        },\n",
452 |     "        artifact_file=\"eval_results.json\",\n",
453 |     "    )\n",
454 |     "```\n",
455 |     "\n",
456 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
457 |     "\n",
458 |     "</details>"
459 |    ]
460 |   },
461 |   {
462 |    "cell_type": "code",
463 |    "execution_count": 7,
464 |    "metadata": {},
465 |    "outputs": [
466 |     {
467 |      "name": "stdout",
468 |      "output_type": "stream",
469 |      "text": [
470 |       "Average Metric: 115.00 / 200 (57.5%): 100%|██████████| 200/200 [06:14<00:00,  1.87s/it]"
471 |      ]
472 |     },
473 |     {
474 |      "name": "stderr",
475 |      "output_type": "stream",
476 |      "text": [
477 |       "2024/12/28 11:10:25 INFO dspy.evaluate.evaluate: Average Metric: 115 / 200 (57.5%)\n"
478 |      ]
479 |     },
480 |     {
481 |      "name": "stdout",
482 |      "output_type": "stream",
483 |      "text": [
484 |       "\n"
485 |      ]
486 |     },
487 |     {
488 |      "data": {
489 |       "text/plain": [
490 |        "57.5"
491 |       ]
492 |      },
493 |      "execution_count": 7,
494 |      "metadata": {},
495 |      "output_type": "execute_result"
496 |     }
497 |    ],
498 |    "source": [
499 |     "agent_4o.verbose = False\n",
500 |     "evaluate(agent_4o)"
501 |    ]
502 |   },
503 |   {
504 |    "cell_type": "code",
505 |    "execution_count": 8,
506 |    "metadata": {},
507 |    "outputs": [
508 |     {
509 |      "name": "stdout",
510 |      "output_type": "stream",
511 |      "text": [
512 |       "Average Metric: 30.00 / 200 (15.0%): 100%|██████████| 200/200 [08:33<00:00,  2.57s/it]"
513 |      ]
514 |     },
515 |     {
516 |      "name": "stderr",
517 |      "output_type": "stream",
518 |      "text": [
519 |       "2024/12/28 11:18:59 INFO dspy.evaluate.evaluate: Average Metric: 30 / 200 (15.0%)\n"
520 |      ]
521 |     },
522 |     {
523 |      "name": "stdout",
524 |      "output_type": "stream",
525 |      "text": [
526 |       "\n"
527 |      ]
528 |     },
529 |     {
530 |      "data": {
531 |       "text/plain": [
532 |        "15.0"
533 |       ]
534 |      },
535 |      "execution_count": 8,
536 |      "metadata": {},
537 |      "output_type": "execute_result"
538 |     }
539 |    ],
540 |    "source": [
541 |     "agent_4o_mini = Agent()\n",
542 |     "agent_4o_mini.set_lm(gpt4o_mini)\n",
543 |     "\n",
544 |     "evaluate(agent_4o_mini)"
545 |    ]
546 |   },
547 |   {
548 |    "cell_type": "markdown",
549 |    "metadata": {},
550 |    "source": [
551 |     "Out of the box, on this task, 4o is decent (58% success rate) while 4o-mini struggles (15% success rate).\n",
552 |     "\n",
553 |     "Let's apply the following strategy:\n",
554 |     "\n",
555 |     "1. We'll optimize the _prompts_ for gpt-4o in a lightweight way.\n",
556 |     "2. We'll then use this prompt-optimized agent as a teacher to fine-tune gpt-4o-mini on the task. This will increase its quality from 19% to 72% (or 82% if you use 500 trainset examples)."
557 |    ]
558 |   },
559 |   {
560 |    "cell_type": "markdown",
561 |    "metadata": {},
562 |    "source": [
563 |     "### Prompt-optimizing GPT-4o"
564 |    ]
565 |   },
566 |   {
567 |    "cell_type": "code",
568 |    "execution_count": 9,
569 |    "metadata": {},
570 |    "outputs": [],
571 |    "source": [
572 |     "optimizer = dspy.MIPROv2(metric=metric, auto=\"light\", num_threads=16, prompt_model=gpt4o)\n",
573 |     "\n",
574 |     "config = dict(max_bootstrapped_demos=1, max_labeled_demos=0, minibatch_size=40)\n",
575 |     "optimized_4o = optimizer.compile(agent_4o, trainset=trainset, **config)"
576 |    ]
577 |   },
578 |   {
579 |    "cell_type": "markdown",
580 |    "metadata": {},
581 |    "source": [
582 |     "### Fine-tuning GPT-4o-mini\n",
583 |     "\n",
584 |     "For fine-tuning, we'll need a teacher program (`optimized_4o` above) and a student program derived from it (`student_4om` below)."
585 |    ]
586 |   },
587 |   {
588 |    "cell_type": "code",
589 |    "execution_count": 10,
590 |    "metadata": {},
591 |    "outputs": [],
592 |    "source": [
593 |     "student_4o_mini = optimized_4o.deepcopy()\n",
594 |     "student_4o_mini.set_lm(gpt4o_mini)\n",
595 |     "# student_4o_mini.react.demos = []  # you can optionally reset the demos"
596 |    ]
597 |   },
598 |   {
599 |    "cell_type": "code",
600 |    "execution_count": 11,
601 |    "metadata": {},
602 |    "outputs": [],
603 |    "source": [
604 |     "optimizer = dspy.BootstrapFinetune(metric=metric, num_threads=16)\n",
605 |     "finetuned_4o_mini = optimizer.compile(student_4o_mini, teacher=optimized_4o, trainset=trainset)"
606 |    ]
607 |   },
608 |   {
609 |    "cell_type": "markdown",
610 |    "metadata": {},
611 |    "source": [
612 |     "### Evaluate the finetuned GPT-4o-mini agent"
613 |    ]
614 |   },
615 |   {
616 |    "cell_type": "code",
617 |    "execution_count": 12,
618 |    "metadata": {},
619 |    "outputs": [
620 |     {
621 |      "name": "stdout",
622 |      "output_type": "stream",
623 |      "text": [
624 |       "Average Metric: 143.00 / 200 (71.5%): 100%|██████████| 200/200 [03:15<00:00,  1.05it/s]"
625 |      ]
626 |     }
627 |    ],
628 |    "source": [
629 |     "evaluate(finetuned_4o_mini)"
630 |    ]
631 |   },
632 |   {
633 |    "cell_type": "markdown",
634 |    "metadata": {},
635 |    "source": [
636 |     "Having done all this optimization, let's save our program so we can use it later! This will keep a reference to the fine-tuned model as well, as long as it continued to exist with the same identifier at the provider side."
637 |    ]
638 |   },
639 |   {
640 |    "cell_type": "code",
641 |    "execution_count": 13,
642 |    "metadata": {},
643 |    "outputs": [],
644 |    "source": [
645 |     "finetuned_4o_mini.save('finetuned_4o_mini_001.pkl')"
646 |    ]
647 |   },
648 |   {
649 |    "cell_type": "markdown",
650 |    "metadata": {},
651 |    "source": [
652 |     "<details>\n",
653 |     "<summary>Saving programs in MLflow Experiment</summary>\n",
654 |     "\n",
655 |     "<br/>\n",
656 |     "\n",
657 |     "Instead of saving the program to a local file, you can track it in MLflow for better reproducibility and collaboration.\n",
658 |     "\n",
659 |     "1. **Dependency Management**: MLflow automatically save the frozen environment metadata along with the program to ensure reproducibility.\n",
660 |     "2. **Experiment Tracking**: With MLflow, you can track the program's performance and cost along with the program itself.\n",
661 |     "3. **Collaboration**: You can share the program and results with your team members by sharing the MLflow experiment.\n",
662 |     "\n",
663 |     "To save the program in MLflow, run the following code:\n",
664 |     "\n",
665 |     "```python\n",
666 |     "import mlflow\n",
667 |     "\n",
668 |     "# Start an MLflow Run and save the program\n",
669 |     "with mlflow.start_run(run_name=\"optimized\"):\n",
670 |     "    model_info = mlflow.dspy.log_model(\n",
671 |     "        finetuned_4o_mini,\n",
672 |     "        artifact_path=\"model\", # Any name to save the program in MLflow\n",
673 |     "    )\n",
674 |     "\n",
675 |     "# Load the program back from MLflow\n",
676 |     "loaded = mlflow.dspy.load_model(model_info.model_uri)\n",
677 |     "```\n",
678 |     "\n",
679 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
680 |     "\n",
681 |     "</details>"
682 |    ]
683 |   },
684 |   {
685 |    "cell_type": "markdown",
686 |    "metadata": {},
687 |    "source": [
688 |     "Let's now check out one task using our fine-tuned agent program!"
689 |    ]
690 |   },
691 |   {
692 |    "cell_type": "code",
693 |    "execution_count": 14,
694 |    "metadata": {},
695 |    "outputs": [
696 |     {
697 |      "name": "stdout",
698 |      "output_type": "stream",
699 |      "text": [
700 |       "Task: -= Welcome to TextWorld, ALFRED! =-\n",
701 |       "\n",
702 |       "You are in the middle of a room. Looking quickly around you, you see a armchair 1, a cabinet 1, a drawer 21, a drawer 20, a drawer 19, a drawer 18, a drawer 17, a drawer 16, a drawer 15, a drawer 14, a drawer 13, a drawer 12, a drawer 11, a drawer 10, a drawer 9, a drawer 8, a drawer 7, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a dresser 1, a garbagecan 1, a sidetable 5, a sidetable 4, a sidetable 3, a sidetable 2, a sidetable 1, and a sofa 1.\n",
703 |       "\n",
704 |       "Your task is to: put some box on dresser.\n",
705 |       "> go to cabinet 1\n",
706 |       "You arrive at cabinet 1. On the cabinet 1, you see nothing.\n",
707 |       "> go to dresser 1\n",
708 |       "You arrive at dresser 1. On the dresser 1, you see a book 1, a newspaper 1, a remotecontrol 1, a statue 3, and a television 1.\n",
709 |       "> look\n",
710 |       "You are facing the dresser 1. Next to it, you see nothing.\n",
711 |       "> go to sidetable 1\n",
712 |       "You arrive at sidetable 1. On the sidetable 1, you see a cellphone 1, and a desklamp 1.\n",
713 |       "> go to sidetable 2\n",
714 |       "You arrive at sidetable 2. On the sidetable 2, you see a box 2.\n",
715 |       "> take box 2 from sidetable 2\n",
716 |       "You pick up the box 2 from the sidetable 2.\n",
717 |       "> go to dresser 1\n",
718 |       "You arrive at dresser 1. On the dresser 1, you see a book 1, a newspaper 1, a remotecontrol 1, a statue 3, and a television 1.\n",
719 |       "> move box 2 to dresser 1\n",
720 |       "You move the box 2 to the dresser 1.\n"
721 |      ]
722 |     },
723 |     {
724 |      "data": {
725 |       "text/plain": [
726 |        "Prediction(\n",
727 |        "    trajectory=['> go to cabinet 1', 'You arrive at cabinet 1. On the cabinet 1, you see nothing.', '> go to dresser 1', 'You arrive at dresser 1. On the dresser 1, you see a book 1, a newspaper 1, a remotecontrol 1, a statue 3, and a television 1.', '> look', 'You are facing the dresser 1. Next to it, you see nothing.', '> go to sidetable 1', 'You arrive at sidetable 1. On the sidetable 1, you see a cellphone 1, and a desklamp 1.', '> go to sidetable 2', 'You arrive at sidetable 2. On the sidetable 2, you see a box 2.', '> take box 2 from sidetable 2', 'You pick up the box 2 from the sidetable 2.', '> go to dresser 1', 'You arrive at dresser 1. On the dresser 1, you see a book 1, a newspaper 1, a remotecontrol 1, a statue 3, and a television 1.', '> move box 2 to dresser 1', 'You move the box 2 to the dresser 1.'],\n",
728 |        "    success=1\n",
729 |        ")"
730 |       ]
731 |      },
732 |      "execution_count": 15,
733 |      "metadata": {},
734 |      "output_type": "execute_result"
735 |     }
736 |    ],
737 |    "source": [
738 |     "finetuned_4o_mini.verbose = True\n",
739 |     "finetuned_4o_mini(**devset[0].inputs())"
740 |    ]
741 |   },
742 |   {
743 |    "cell_type": "markdown",
744 |    "metadata": {},
745 |    "source": [
746 |     "If you want to load and use the agent program, you can do that as follows."
747 |    ]
748 |   },
749 |   {
750 |    "cell_type": "code",
751 |    "execution_count": 16,
752 |    "metadata": {},
753 |    "outputs": [],
754 |    "source": [
755 |     "loaded = Agent()\n",
756 |     "loaded.load('finetuned_4o_mini_001.pkl')"
757 |    ]
758 |   }
759 |  ],
760 |  "metadata": {
761 |   "kernelspec": {
762 |    "display_name": "jun2024_py310",
763 |    "language": "python",
764 |    "name": "python3"
765 |   },
766 |   "language_info": {
767 |    "codemirror_mode": {
768 |     "name": "ipython",
769 |     "version": 3
770 |    },
771 |    "file_extension": ".py",
772 |    "mimetype": "text/x-python",
773 |    "name": "python",
774 |    "nbconvert_exporter": "python",
775 |    "pygments_lexer": "ipython3",
776 |    "version": "3.10.14"
777 |   }
778 |  },
779 |  "nbformat": 4,
780 |  "nbformat_minor": 2
781 | }
```

tutorials/gepa_facilitysupportanalyzer/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "id": "23b9ab7f",
6 |    "metadata": {},
7 |    "source": [
8 |     "# Tutorial: GEPA for Structured Information Extraction for Enterprise Tasks"
9 |    ]
10 |   },
11 |   {
12 |    "cell_type": "markdown",
13 |    "id": "1114c356",
14 |    "metadata": {},
15 |    "source": [
16 |     "In this tutorial, we'll explore a three-part task for structured information extraction and classification using the [Facility Support Analyzer](https://github.com/meta-llama/llama-prompt-ops/tree/main/use-cases/facility-support-analyzer) dataset released by Meta. Given an email or message sent in an enterprise setting related to facility maintenance or support requests, the goal is to extract its urgency, assess the sentiment, and identify all relevant service request categories.\n",
17 |     "\n",
18 |     "We will build a simple DSPy program, and then use the `dspy.GEPA` optimizer to optimize it for the task.\n",
19 |     "\n",
20 |     "<details>\n",
21 |     "<summary>Recommended: Set up MLflow Autologging to understand what's happening under the hood.</summary>\n",
22 |     "\n",
23 |     "### MLflow DSPy Integration\n",
24 |     "\n",
25 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. MLflow's autologging capability automatically tracks progress of GEPA optimization, as well as visualizes prompts and module executions as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
26 |     "\n",
27 |     "**Visualize module executions as traces**\n",
28 |     "\n",
29 |     "![MLflow Trace](./mlflow-tracing-gepa-support.png)\n",
30 |     "\n",
31 |     "**Automatically track optimization progress and results**\n",
32 |     "\n",
33 |     "![MLflow Tracking](./mlflow-tracking-gepa-support-optimization.png)\n",
34 |     "\n",
35 |     "\n",
36 |     "**Setup MLflow**\n",
37 |     "\n",
38 |     "1. Install MLflow\n",
39 |     "\n",
40 |     "```bash\n",
41 |     "%pip install mlflow>=3.0.0\n",
42 |     "```\n",
43 |     "\n",
44 |     "2. Start MLflow UI in a separate terminal\n",
45 |     "```bash\n",
46 |     "mlflow ui --port 5000 --backend-store-uri sqlite:///mlruns.db\n",
47 |     "```\n",
48 |     "\n",
49 |     "3. Connect the notebook to MLflow\n",
50 |     "```python\n",
51 |     "import mlflow\n",
52 |     "\n",
53 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
54 |     "mlflow.set_experiment(\"DSPy\")\n",
55 |     "```\n",
56 |     "\n",
57 |     "4. Enabling autologging.\n",
58 |     "\n",
59 |     "```python\n",
60 |     "mlflow.dspy.autolog(\n",
61 |     "    # Log the optimization progress\n",
62 |     "    log_compiles=True,\n",
63 |     "    # Log the evaluation results\n",
64 |     "    log_evals=True,\n",
65 |     "    # Log traces from module executions\n",
66 |     "    log_traces=True\n",
67 |     ")\n",
68 |     "```\n",
69 |     "\n",
70 |     "\n",
71 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
72 |     "</details>"
73 |    ]
74 |   },
75 |   {
76 |    "cell_type": "markdown",
77 |    "id": "990d074e",
78 |    "metadata": {},
79 |    "source": [
80 |     "### Setup the LM\n",
81 |     "\n",
82 |     "We use GPT-4.1 nano to demonstrate how a small model can be tuned with GEPA."
83 |    ]
84 |   },
85 |   {
86 |    "cell_type": "code",
87 |    "execution_count": 1,
88 |    "id": "283588ae",
89 |    "metadata": {},
90 |    "outputs": [],
91 |    "source": [
92 |     "api_key = input(\"Enter your OpenAI API key: \")\n",
93 |     "import dspy\n",
94 |     "lm = dspy.LM(\"openai/gpt-4.1-nano\", temperature=1, api_key=api_key)\n",
95 |     "dspy.configure(lm=lm)"
96 |    ]
97 |   },
98 |   {
99 |    "cell_type": "markdown",
100 |    "id": "014f3da6",
101 |    "metadata": {},
102 |    "source": [
103 |     "### Load the dataset"
104 |    ]
105 |   },
106 |   {
107 |    "cell_type": "code",
108 |    "execution_count": 2,
109 |    "id": "c9e78285",
110 |    "metadata": {},
111 |    "outputs": [],
112 |    "source": [
113 |     "import requests\n",
114 |     "import dspy\n",
115 |     "import json\n",
116 |     "import random\n",
117 |     "\n",
118 |     "def init_dataset():\n",
119 |     "    # Load from the url\n",
120 |     "    url = \"https://raw.githubusercontent.com/meta-llama/llama-prompt-ops/refs/heads/main/use-cases/facility-support-analyzer/dataset.json\"\n",
121 |     "    dataset = json.loads(requests.get(url).text)\n",
122 |     "    dspy_dataset = [\n",
123 |     "        dspy.Example({\n",
124 |     "            \"message\": d['fields']['input'],\n",
125 |     "            \"answer\": d['answer'],\n",
126 |     "        }).with_inputs(\"message\")\n",
127 |     "        for d in dataset\n",
128 |     "    ]\n",
129 |     "    random.Random(0).shuffle(dspy_dataset)\n",
130 |     "    train_set = dspy_dataset[:int(len(dspy_dataset) * 0.33)]\n",
131 |     "    val_set = dspy_dataset[int(len(dspy_dataset) * 0.33):int(len(dspy_dataset) * 0.66)]\n",
132 |     "    test_set = dspy_dataset[int(len(dspy_dataset) * 0.66):]\n",
133 |     "\n",
134 |     "    return train_set, val_set, test_set"
135 |    ]
136 |   },
137 |   {
138 |    "cell_type": "code",
139 |    "execution_count": 3,
140 |    "id": "51822ab6",
141 |    "metadata": {},
142 |    "outputs": [
143 |     {
144 |      "data": {
145 |       "text/plain": [
146 |        "(66, 66, 68)"
147 |       ]
148 |      },
149 |      "execution_count": 3,
150 |      "metadata": {},
151 |      "output_type": "execute_result"
152 |     }
153 |    ],
154 |    "source": [
155 |     "train_set, val_set, test_set = init_dataset()\n",
156 |     "\n",
157 |     "len(train_set), len(val_set), len(test_set)"
158 |    ]
159 |   },
160 |   {
161 |    "cell_type": "markdown",
162 |    "id": "d9f36cd4",
163 |    "metadata": {},
164 |    "source": [
165 |     "Let's view an example task input"
166 |    ]
167 |   },
168 |   {
169 |    "cell_type": "code",
170 |    "execution_count": 4,
171 |    "id": "96f1e064",
172 |    "metadata": {},
173 |    "outputs": [
174 |     {
175 |      "name": "stdout",
176 |      "output_type": "stream",
177 |      "text": [
178 |       "Input Message:\n",
179 |       "Subject: Adjusting Bi-Weekly Cleaning Schedule for My Office\n",
180 |       "\n",
181 |       "Dear ProCare Facility Solutions Support Team,\n",
182 |       "\n",
183 |       "I hope this message finds you well. My name is Dr. Alex Turner, and I have been utilizing your services for my office space for the past year. I must say, your team's dedication to maintaining a pristine environment has been commendable and greatly appreciated.\n",
184 |       "\n",
185 |       "I am reaching out to discuss the scheduling of our regular cleaning services. While I find the logistical challenges of coordinating these services intellectually stimulating, I believe we could optimize the current schedule to better suit the needs of my team and our workflow. Specifically, I would like to explore the possibility of adjusting our cleaning schedule to a bi-weekly arrangement, ideally on Tuesdays and Fridays, to ensure our workspace remains consistently clean without disrupting our research activities.\n",
186 |       "\n",
187 |       "Previously, I have attempted to adjust the schedule through the online portal, but I encountered some difficulties in finalizing the changes. I would appreciate your assistance in making these adjustments or guiding me through the process if there is a more efficient way to do so.\n",
188 |       "\n",
189 |       "Thank you for your attention to this matter. I look forward to your response and continued excellent service.\n",
190 |       "\n",
191 |       "Best regards,\n",
192 |       "\n",
193 |       "Dr. Alex Turner\n",
194 |       "Cryptography Researcher\n",
195 |       "\n",
196 |       "\n",
197 |       "Gold Answer:\n",
198 |       "categories: {'routine_maintenance_requests': False, 'customer_feedback_and_complaints': False, 'training_and_support_requests': False, 'quality_and_safety_concerns': False, 'sustainability_and_environmental_practices': False, 'cleaning_services_scheduling': True, 'specialized_cleaning_services': False, 'emergency_repair_services': False, 'facility_management_issues': False, 'general_inquiries': False}\n",
199 |       "sentiment: neutral\n",
200 |       "urgency: low\n"
201 |      ]
202 |     }
203 |    ],
204 |    "source": [
205 |     "print(\"Input Message:\")\n",
206 |     "print(train_set[0]['message'])\n",
207 |     "\n",
208 |     "print(\"\\n\\nGold Answer:\")\n",
209 |     "for k, v in json.loads(train_set[0]['answer']).items():\n",
210 |     "    print(f\"{k}: {v}\")"
211 |    ]
212 |   },
213 |   {
214 |    "cell_type": "markdown",
215 |    "id": "552769fd",
216 |    "metadata": {},
217 |    "source": [
218 |     "### Defining a DSPy program to solve the task\n",
219 |     "The program is a 3-module system, each of which handles the urgency, sentiment and categories classification respectively"
220 |    ]
221 |   },
222 |   {
223 |    "cell_type": "code",
224 |    "execution_count": 5,
225 |    "id": "71598add",
226 |    "metadata": {},
227 |    "outputs": [],
228 |    "source": [
229 |     "from typing import List, Literal\n",
230 |     "\n",
231 |     "\n",
232 |     "class FacilitySupportAnalyzerUrgency(dspy.Signature):\n",
233 |     "    \"\"\"\n",
234 |     "    Read the provided message and determine the urgency.\n",
235 |     "    \"\"\"\n",
236 |     "    message: str = dspy.InputField()\n",
237 |     "    urgency: Literal['low', 'medium', 'high'] = dspy.OutputField()\n",
238 |     "\n",
239 |     "class FacilitySupportAnalyzerSentiment(dspy.Signature):\n",
240 |     "    \"\"\"\n",
241 |     "    Read the provided message and determine the sentiment.\n",
242 |     "    \"\"\"\n",
243 |     "    message: str = dspy.InputField()\n",
244 |     "    sentiment: Literal['positive', 'neutral', 'negative'] = dspy.OutputField()\n",
245 |     "\n",
246 |     "class FacilitySupportAnalyzerCategories(dspy.Signature):\n",
247 |     "    \"\"\"\n",
248 |     "    Read the provided message and determine the set of categories applicable to the message.\n",
249 |     "    \"\"\"\n",
250 |     "    message: str = dspy.InputField()\n",
251 |     "    categories: List[Literal[\"emergency_repair_services\", \"routine_maintenance_requests\", \"quality_and_safety_concerns\", \"specialized_cleaning_services\", \"general_inquiries\", \"sustainability_and_environmental_practices\", \"training_and_support_requests\", \"cleaning_services_scheduling\", \"customer_feedback_and_complaints\", \"facility_management_issues\"]] = dspy.OutputField()\n",
252 |     "\n",
253 |     "class FacilitySupportAnalyzerMM(dspy.Module):\n",
254 |     "    def __init__(self):\n",
255 |     "        self.urgency_module = dspy.ChainOfThought(FacilitySupportAnalyzerUrgency)\n",
256 |     "        self.sentiment_module = dspy.ChainOfThought(FacilitySupportAnalyzerSentiment)\n",
257 |     "        self.categories_module = dspy.ChainOfThought(FacilitySupportAnalyzerCategories)\n",
258 |     "    \n",
259 |     "    def forward(self, message: str):\n",
260 |     "        urgency = self.urgency_module(message=message)\n",
261 |     "        sentiment = self.sentiment_module(message=message)\n",
262 |     "        categories = self.categories_module(message=message)\n",
263 |     "\n",
264 |     "        return dspy.Prediction(\n",
265 |     "            urgency=urgency.urgency,\n",
266 |     "            sentiment=sentiment.sentiment,\n",
267 |     "            categories=categories.categories\n",
268 |     "        )\n",
269 |     "\n",
270 |     "program = FacilitySupportAnalyzerMM()"
271 |    ]
272 |   },
273 |   {
274 |    "cell_type": "markdown",
275 |    "id": "6e57234e",
276 |    "metadata": {},
277 |    "source": [
278 |     "### Define the metric to evaluate the outputs\n",
279 |     "The metric evaluates the output of all the three tasks, and returns the aggregate score"
280 |    ]
281 |   },
282 |   {
283 |    "cell_type": "code",
284 |    "execution_count": 6,
285 |    "id": "1c0d2c5a",
286 |    "metadata": {},
287 |    "outputs": [],
288 |    "source": [
289 |     "def score_urgency(gold_urgency, pred_urgency):\n",
290 |     "    \"\"\"\n",
291 |     "    Compute score for the urgency module.\n",
292 |     "    \"\"\"\n",
293 |     "    score = 1.0 if gold_urgency == pred_urgency else 0.0\n",
294 |     "    return score\n",
295 |     "\n",
296 |     "def score_sentiment(gold_sentiment, pred_sentiment):\n",
297 |     "    \"\"\"\n",
298 |     "    Compute score for the sentiment module.\n",
299 |     "    \"\"\"\n",
300 |     "    score = 1.0 if gold_sentiment == pred_sentiment else 0.0\n",
301 |     "    return score\n",
302 |     "\n",
303 |     "def score_categories(gold_categories, pred_categories):\n",
304 |     "    \"\"\"\n",
305 |     "    Compute score for the categories module.\n",
306 |     "    Uses the same match/mismatch logic as category accuracy in the score.\n",
307 |     "    \"\"\"\n",
308 |     "    correct = 0\n",
309 |     "    for k, v in gold_categories.items():\n",
310 |     "        if v and k in pred_categories:\n",
311 |     "            correct += 1\n",
312 |     "        elif not v and k not in pred_categories:\n",
313 |     "            correct += 1\n",
314 |     "    score = correct / len(gold_categories)\n",
315 |     "    return score\n",
316 |     "\n",
317 |     "def metric(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
318 |     "    \"\"\"\n",
319 |     "    Computes a score based on agreement between prediction and gold standard for categories, sentiment, and urgency.\n",
320 |     "    Returns the score (float).\n",
321 |     "    \"\"\"\n",
322 |     "    # Parse gold standard from example\n",
323 |     "    gold = json.loads(example['answer'])\n",
324 |     "\n",
325 |     "    # Compute scores for all modules\n",
326 |     "    score_urgency_val = score_urgency(gold['urgency'], pred.urgency)\n",
327 |     "    score_sentiment_val = score_sentiment(gold['sentiment'], pred.sentiment)\n",
328 |     "    score_categories_val = score_categories(gold['categories'], pred.categories)\n",
329 |     "\n",
330 |     "    # Overall score: average of the three accuracies\n",
331 |     "    total = (score_urgency_val + score_sentiment_val + score_categories_val) / 3\n",
332 |     "\n",
333 |     "    return total"
334 |    ]
335 |   },
336 |   {
337 |    "cell_type": "markdown",
338 |    "id": "785e4e74",
339 |    "metadata": {},
340 |    "source": [
341 |     "### Evaluating the unoptimized program (running with GPT-4.1 nano)"
342 |    ]
343 |   },
344 |   {
345 |    "cell_type": "code",
346 |    "execution_count": 7,
347 |    "id": "e52d3e50",
348 |    "metadata": {},
349 |    "outputs": [
350 |     {
351 |      "name": "stdout",
352 |      "output_type": "stream",
353 |      "text": [
354 |       "Average Metric: 51.30 / 68 (75.4%): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68/68 [00:00<00:00, 322.00it/s]"
355 |      ]
356 |     },
357 |     {
358 |      "name": "stderr",
359 |      "output_type": "stream",
360 |      "text": [
361 |       "2025/08/12 18:09:18 INFO dspy.evaluate.evaluate: Average Metric: 51.3 / 68 (75.4%)\n"
362 |      ]
363 |     },
364 |     {
365 |      "name": "stdout",
366 |      "output_type": "stream",
367 |      "text": [
368 |       "\n"
369 |      ]
370 |     },
371 |     {
372 |      "data": {
373 |       "text/html": [
374 |        "<div>\n",
375 |        "<style scoped>\n",
376 |        "    .dataframe tbody tr th:only-of-type {\n",
377 |        "        vertical-align: middle;\n",
378 |        "    }\n",
379 |        "\n",
380 |        "    .dataframe tbody tr th {\n",
381 |        "        vertical-align: top;\n",
382 |        "    }\n",
383 |        "\n",
384 |        "    .dataframe thead th {\n",
385 |        "        text-align: right;\n",
386 |        "    }\n",
387 |        "</style>\n",
388 |        "<table border=\"1\" class=\"dataframe\">\n",
389 |        "  <thead>\n",
390 |        "    <tr style=\"text-align: right;\">\n",
391 |        "      <th></th>\n",
392 |        "      <th>message</th>\n",
393 |        "      <th>answer</th>\n",
394 |        "      <th>urgency</th>\n",
395 |        "      <th>sentiment</th>\n",
396 |        "      <th>categories</th>\n",
397 |        "      <th>metric</th>\n",
398 |        "    </tr>\n",
399 |        "  </thead>\n",
400 |        "  <tbody>\n",
401 |        "    <tr>\n",
402 |        "      <th>0</th>\n",
403 |        "      <td>Hey ProCare Support Team, Hope you all are doing great! My name is...</td>\n",
404 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...</td>\n",
405 |        "      <td>low</td>\n",
406 |        "      <td>positive</td>\n",
407 |        "      <td>[sustainability_and_environmental_practices]</td>\n",
408 |        "      <td>✔️ [1.000]</td>\n",
409 |        "    </tr>\n",
410 |        "    <tr>\n",
411 |        "      <th>1</th>\n",
412 |        "      <td>Hey ProCare Team, Hope you’re all doing well! My name’s Jake, and ...</td>\n",
413 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": true, \"customer_fe...</td>\n",
414 |        "      <td>medium</td>\n",
415 |        "      <td>positive</td>\n",
416 |        "      <td>[routine_maintenance_requests, customer_feedback_and_complaints]</td>\n",
417 |        "      <td>✔️ [0.967]</td>\n",
418 |        "    </tr>\n",
419 |        "    <tr>\n",
420 |        "      <th>2</th>\n",
421 |        "      <td>Subject: Assistance Needed for HVAC Maintenance Hi [Receiver], I h...</td>\n",
422 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": true, \"customer_fe...</td>\n",
423 |        "      <td>medium</td>\n",
424 |        "      <td>neutral</td>\n",
425 |        "      <td>[routine_maintenance_requests]</td>\n",
426 |        "      <td>✔️ [1.000]</td>\n",
427 |        "    </tr>\n",
428 |        "    <tr>\n",
429 |        "      <th>3</th>\n",
430 |        "      <td>Subject: A Green Inquiry from a Bill Maher Enthusiast Hey ProCare ...</td>\n",
431 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...</td>\n",
432 |        "      <td>low</td>\n",
433 |        "      <td>positive</td>\n",
434 |        "      <td>[sustainability_and_environmental_practices]</td>\n",
435 |        "      <td>✔️ [1.000]</td>\n",
436 |        "    </tr>\n",
437 |        "    <tr>\n",
438 |        "      <th>4</th>\n",
439 |        "      <td>Subject: Inquiry on Sustainability Practices Dear ProCare Facility...</td>\n",
440 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...</td>\n",
441 |        "      <td>medium</td>\n",
442 |        "      <td>neutral</td>\n",
443 |        "      <td>[sustainability_and_environmental_practices]</td>\n",
444 |        "      <td>✔️ [0.667]</td>\n",
445 |        "    </tr>\n",
446 |        "    <tr>\n",
447 |        "      <th>...</th>\n",
448 |        "      <td>...</td>\n",
449 |        "      <td>...</td>\n",
450 |        "      <td>...</td>\n",
451 |        "      <td>...</td>\n",
452 |        "      <td>...</td>\n",
453 |        "      <td>...</td>\n",
454 |        "    </tr>\n",
455 |        "    <tr>\n",
456 |        "      <th>63</th>\n",
457 |        "      <td>Subject: Inquiry About Your Eco-Friendly Practices Dear ProCare Fa...</td>\n",
458 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...</td>\n",
459 |        "      <td>medium</td>\n",
460 |        "      <td>neutral</td>\n",
461 |        "      <td>[sustainability_and_environmental_practices]</td>\n",
462 |        "      <td>✔️ [0.600]</td>\n",
463 |        "    </tr>\n",
464 |        "    <tr>\n",
465 |        "      <th>64</th>\n",
466 |        "      <td>Subject: Assistance Needed for Facility Management Issue Dear ProC...</td>\n",
467 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...</td>\n",
468 |        "      <td>high</td>\n",
469 |        "      <td>positive</td>\n",
470 |        "      <td>[facility_management_issues]</td>\n",
471 |        "      <td>✔️ [0.667]</td>\n",
472 |        "    </tr>\n",
473 |        "    <tr>\n",
474 |        "      <th>65</th>\n",
475 |        "      <td>Subject: Request for Training and Support Hi ProCare Support Team,...</td>\n",
476 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...</td>\n",
477 |        "      <td>low</td>\n",
478 |        "      <td>positive</td>\n",
479 |        "      <td>[training_and_support_requests]</td>\n",
480 |        "      <td>✔️ [1.000]</td>\n",
481 |        "    </tr>\n",
482 |        "    <tr>\n",
483 |        "      <th>66</th>\n",
484 |        "      <td>Subject: Concerns About Studio Maintenance and Rent Increase Dear ...</td>\n",
485 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": true, \"customer_fe...</td>\n",
486 |        "      <td>medium</td>\n",
487 |        "      <td>negative</td>\n",
488 |        "      <td>[routine_maintenance_requests, facility_management_issues]</td>\n",
489 |        "      <td>✔️ [0.600]</td>\n",
490 |        "    </tr>\n",
491 |        "    <tr>\n",
492 |        "      <th>67</th>\n",
493 |        "      <td>Subject: Feedback on Recent Maintenance Service Dear ProCare Suppo...</td>\n",
494 |        "      <td>{\"categories\": {\"routine_maintenance_requests\": true, \"customer_fe...</td>\n",
495 |        "      <td>medium</td>\n",
496 |        "      <td>neutral</td>\n",
497 |        "      <td>[routine_maintenance_requests, customer_feedback_and_complaints]</td>\n",
498 |        "      <td>✔️ [0.967]</td>\n",
499 |        "    </tr>\n",
500 |        "  </tbody>\n",
501 |        "</table>\n",
502 |        "<p>68 rows × 6 columns</p>\n",
503 |        "</div>"
504 |       ],
505 |       "text/plain": [
506 |        "                                                                  message  \\\n",
507 |        "0   Hey ProCare Support Team, Hope you all are doing great! My name is...   \n",
508 |        "1   Hey ProCare Team, Hope you’re all doing well! My name’s Jake, and ...   \n",
509 |        "2   Subject: Assistance Needed for HVAC Maintenance Hi [Receiver], I h...   \n",
510 |        "3   Subject: A Green Inquiry from a Bill Maher Enthusiast Hey ProCare ...   \n",
511 |        "4   Subject: Inquiry on Sustainability Practices Dear ProCare Facility...   \n",
512 |        "..                                                                    ...   \n",
513 |        "63  Subject: Inquiry About Your Eco-Friendly Practices Dear ProCare Fa...   \n",
514 |        "64  Subject: Assistance Needed for Facility Management Issue Dear ProC...   \n",
515 |        "65  Subject: Request for Training and Support Hi ProCare Support Team,...   \n",
516 |        "66  Subject: Concerns About Studio Maintenance and Rent Increase Dear ...   \n",
517 |        "67  Subject: Feedback on Recent Maintenance Service Dear ProCare Suppo...   \n",
518 |        "\n",
519 |        "                                                                   answer  \\\n",
520 |        "0   {\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...   \n",
521 |        "1   {\"categories\": {\"routine_maintenance_requests\": true, \"customer_fe...   \n",
522 |        "2   {\"categories\": {\"routine_maintenance_requests\": true, \"customer_fe...   \n",
523 |        "3   {\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...   \n",
524 |        "4   {\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...   \n",
525 |        "..                                                                    ...   \n",
526 |        "63  {\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...   \n",
527 |        "64  {\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...   \n",
528 |        "65  {\"categories\": {\"routine_maintenance_requests\": false, \"customer_f...   \n",
529 |        "66  {\"categories\": {\"routine_maintenance_requests\": true, \"customer_fe...   \n",
530 |        "67  {\"categories\": {\"routine_maintenance_requests\": true, \"customer_fe...   \n",
531 |        "\n",
532 |        "   urgency sentiment  \\\n",
533 |        "0      low  positive   \n",
534 |        "1   medium  positive   \n",
535 |        "2   medium   neutral   \n",
536 |        "3      low  positive   \n",
537 |        "4   medium   neutral   \n",
538 |        "..     ...       ...   \n",
539 |        "63  medium   neutral   \n",
540 |        "64    high  positive   \n",
541 |        "65     low  positive   \n",
542 |        "66  medium  negative   \n",
543 |        "67  medium   neutral   \n",
544 |        "\n",
545 |        "                                                          categories  \\\n",
546 |        "0                       [sustainability_and_environmental_practices]   \n",
547 |        "1   [routine_maintenance_requests, customer_feedback_and_complaints]   \n",
548 |        "2                                     [routine_maintenance_requests]   \n",
549 |        "3                       [sustainability_and_environmental_practices]   \n",
550 |        "4                       [sustainability_and_environmental_practices]   \n",
551 |        "..                                                               ...   \n",
552 |        "63                      [sustainability_and_environmental_practices]   \n",
553 |        "64                                      [facility_management_issues]   \n",
554 |        "65                                   [training_and_support_requests]   \n",
555 |        "66        [routine_maintenance_requests, facility_management_issues]   \n",
556 |        "67  [routine_maintenance_requests, customer_feedback_and_complaints]   \n",
557 |        "\n",
558 |        "        metric  \n",
559 |        "0   ✔️ [1.000]  \n",
560 |        "1   ✔️ [0.967]  \n",
561 |        "2   ✔️ [1.000]  \n",
562 |        "3   ✔️ [1.000]  \n",
563 |        "4   ✔️ [0.667]  \n",
564 |        "..         ...  \n",
565 |        "63  ✔️ [0.600]  \n",
566 |        "64  ✔️ [0.667]  \n",
567 |        "65  ✔️ [1.000]  \n",
568 |        "66  ✔️ [0.600]  \n",
569 |        "67  ✔️ [0.967]  \n",
570 |        "\n",
571 |        "[68 rows x 6 columns]"
572 |       ]
573 |      },
574 |      "metadata": {},
575 |      "output_type": "display_data"
576 |     },
577 |     {
578 |      "data": {
579 |       "text/plain": [
580 |        "EvaluationResult(score=75.44, results=<list of 68 results>)"
581 |       ]
582 |      },
583 |      "execution_count": 7,
584 |      "metadata": {},
585 |      "output_type": "execute_result"
586 |     }
587 |    ],
588 |    "source": [
589 |     "import dspy\n",
590 |     "evaluate = dspy.Evaluate(\n",
591 |     "    devset=test_set,\n",
592 |     "    metric=metric,\n",
593 |     "    num_threads=32,\n",
594 |     "    display_table=True,\n",
595 |     "    display_progress=True\n",
596 |     ")\n",
597 |     "\n",
598 |     "evaluate(program)"
599 |    ]
600 |   },
601 |   {
602 |    "cell_type": "markdown",
603 |    "id": "0aeb9bae",
604 |    "metadata": {},
605 |    "source": [
606 |     "### Optimizing with GEPA"
607 |    ]
608 |   },
609 |   {
610 |    "cell_type": "markdown",
611 |    "id": "872fe00c",
612 |    "metadata": {},
613 |    "source": [
614 |     "GEPA is a _reflective_ prompt optimizer. Its strength lies in its ability to examine textual feedback from the DSPy program's execution and evaluation pipelines. This gives GEPA greater insight into why the system achieved a particular score, enabling it to introspect and determine ways to enhance performance.\n",
615 |     "\n",
616 |     "In this scenario, the final score is based on performance across three distinct tasks. It's straightforward to see that each predictor handles a specific part of the overall score.\n",
617 |     "\n",
618 |     "GEPA supports providing feedback at the individual predictor level (though this isn't required—see the GEPA PAPILLON tutorial for an example without it). Let's make a quick adjustment to our evaluation metric, to make it an optimization metric, that also provides text feedback!"
619 |    ]
620 |   },
621 |   {
622 |    "cell_type": "code",
623 |    "execution_count": 8,
624 |    "id": "e21e86df",
625 |    "metadata": {},
626 |    "outputs": [],
627 |    "source": [
628 |     "import json\n",
629 |     "import dspy\n",
630 |     "\n",
631 |     "def feedback_urgency(gold_urgency, pred_urgency):\n",
632 |     "    \"\"\"\n",
633 |     "    Generate feedback for the urgency module.\n",
634 |     "    \"\"\"\n",
635 |     "    score = 1.0 if gold_urgency == pred_urgency else 0.0\n",
636 |     "    if gold_urgency == pred_urgency:\n",
637 |     "        feedback = f\"You correctly classified the urgency of the message as `{gold_urgency}`. This message is indeed of `{gold_urgency}` urgency.\"\n",
638 |     "    else:\n",
639 |     "        feedback = f\"You incorrectly classified the urgency of the message as `{pred_urgency}`. The correct urgency is `{gold_urgency}`. Think about how you could have reasoned to get the correct urgency label.\"\n",
640 |     "    return feedback, score\n",
641 |     "\n",
642 |     "def feedback_sentiment(gold_sentiment, pred_sentiment):\n",
643 |     "    \"\"\"\n",
644 |     "    Generate feedback for the sentiment module.\n",
645 |     "    \"\"\"\n",
646 |     "    score = 1.0 if gold_sentiment == pred_sentiment else 0.0\n",
647 |     "    if gold_sentiment == pred_sentiment:\n",
648 |     "        feedback = f\"You correctly classified the sentiment of the message as `{gold_sentiment}`. This message is indeed `{gold_sentiment}`.\"\n",
649 |     "    else:\n",
650 |     "        feedback = f\"You incorrectly classified the sentiment of the message as `{pred_sentiment}`. The correct sentiment is `{gold_sentiment}`. Think about how you could have reasoned to get the correct sentiment label.\"\n",
651 |     "    return feedback, score\n",
652 |     "\n",
653 |     "def feedback_categories(gold_categories, pred_categories):\n",
654 |     "    \"\"\"\n",
655 |     "    Generate feedback for the categories module.\n",
656 |     "    Uses the same match/mismatch logic as category accuracy in the score.\n",
657 |     "    \"\"\"\n",
658 |     "    correctly_included = [k for k, v in gold_categories.items() if v and k in pred_categories]\n",
659 |     "    incorrectly_included = [k for k, v in gold_categories.items() if not v and k in pred_categories]\n",
660 |     "    incorrectly_excluded = [k for k, v in gold_categories.items() if v and k not in pred_categories]\n",
661 |     "    correctly_excluded = [k for k, v in gold_categories.items() if not v and k not in pred_categories]  # For completeness in accuracy check\n",
662 |     "\n",
663 |     "    # Recompute category accuracy (matches score logic)\n",
664 |     "    score = (len(correctly_included) + len(correctly_excluded)) / len(gold_categories)\n",
665 |     "\n",
666 |     "    if score == 1.0:\n",
667 |     "        fb_text = f\"The category classification is perfect. You correctly identified that the message falls under the following categories: `{repr(correctly_included)}`.\"\n",
668 |     "    else:\n",
669 |     "        fb_text = f\"The category classification is not perfect. You correctly identified that the message falls under the following categories: `{repr(correctly_included)}`.\\n\"\n",
670 |     "        if incorrectly_included:\n",
671 |     "            fb_text += f\"However, you incorrectly identified that the message falls under the following categories: `{repr(incorrectly_included)}`. The message DOES NOT fall under these categories.\\n\"\n",
672 |     "        if incorrectly_excluded:\n",
673 |     "            prefix = \"Additionally, \" if incorrectly_included else \"However, \"\n",
674 |     "            fb_text += f\"{prefix}you didn't identify the following categories that the message actually falls under: `{repr(incorrectly_excluded)}`.\\n\"\n",
675 |     "        fb_text += \"Think about how you could have reasoned to get the correct category labels.\"\n",
676 |     "    return fb_text, score\n",
677 |     "\n",
678 |     "def metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
679 |     "    \"\"\"\n",
680 |     "    Computes a score based on agreement between prediction and gold standard for categories, sentiment, and urgency.\n",
681 |     "    Optionally provides feedback text for a specific predictor module, using the same comparison logic as the score.\n",
682 |     "    Returns a dspy.Prediction with score (float) and feedback (str).\n",
683 |     "    \"\"\"\n",
684 |     "    # Parse gold standard from example\n",
685 |     "    gold = json.loads(example['answer'])\n",
686 |     "\n",
687 |     "    # Compute feedback and scores for all modules\n",
688 |     "    fb_urgency, score_urgency = feedback_urgency(gold['urgency'], pred.urgency)\n",
689 |     "    fb_sentiment, score_sentiment = feedback_sentiment(gold['sentiment'], pred.sentiment)\n",
690 |     "    fb_categories, score_categories = feedback_categories(gold['categories'], pred.categories)\n",
691 |     "\n",
692 |     "    # Overall score: average of the three accuracies\n",
693 |     "    total = (score_urgency + score_sentiment + score_categories) / 3\n",
694 |     "\n",
695 |     "    if pred_name is None:\n",
696 |     "        return total\n",
697 |     "\n",
698 |     "    elif pred_name == 'urgency_module.predict':\n",
699 |     "        feedback = fb_urgency\n",
700 |     "    elif pred_name == 'sentiment_module.predict':\n",
701 |     "        feedback = fb_sentiment\n",
702 |     "    elif pred_name == 'categories_module.predict':\n",
703 |     "        feedback = fb_categories\n",
704 |     "\n",
705 |     "    return dspy.Prediction(score=total, feedback=feedback)"
706 |    ]
707 |   },
708 |   {
709 |    "cell_type": "markdown",
710 |    "id": "dec4e785",
711 |    "metadata": {},
712 |    "source": [
713 |     "Notice that the evaluation metric already contained all the information needed to generate the text feedback—we simply modified it to explicitly state what was being compared. In general, the metric functions for most tasks provide the essential components for creating such feedback; it often just requires identifying which elements to expose to the GEPA optimizer, enabling it to reflect on and enhance the program's performance."
714 |    ]
715 |   },
716 |   {
717 |    "cell_type": "markdown",
718 |    "id": "77b00899",
719 |    "metadata": {},
720 |    "source": [
721 |     "Let's run GEPA"
722 |    ]
723 |   },
724 |   {
725 |    "cell_type": "code",
726 |    "execution_count": 9,
727 |    "id": "1de921bc",
728 |    "metadata": {},
729 |    "outputs": [],
730 |    "source": [
731 |     "from dspy import GEPA\n",
732 |     "\n",
733 |     "optimizer = GEPA(\n",
734 |     "    metric=metric_with_feedback,\n",
735 |     "    auto=\"light\", # <-- We will use a light budget for this tutorial. However, we typically recommend using auto=\"heavy\" for optimized performance!\n",
736 |     "    num_threads=32,\n",
737 |     "    track_stats=True,\n",
738 |     "    use_merge=False,\n",
739 |     "    reflection_lm=dspy.LM(model=\"gpt-5\", temperature=1.0, max_tokens=32000, api_key=api_key)\n",
740 |     ")"
741 |    ]
742 |   },
743 |   {
744 |    "cell_type": "code",
745 |    "execution_count": 10,
746 |    "id": "b1404077",
747 |    "metadata": {},
748 |    "outputs": [
749 |     {
750 |      "name": "stderr",
751 |      "output_type": "stream",
752 |      "text": [
753 |       "2025/08/12 20:01:55 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 1643 metric calls of the program. This amounts to 12.45 full evals on the train+val set.\n",
754 |       "2025/08/12 20:01:55 INFO dspy.teleprompt.gepa.gepa: Using 66 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n",
755 |       "7\n",
756 |       "2025/08/12 20:01:56 INFO dspy.evaluate.evaluate: Average Metric: 47.56666666666666 / 66 (72.1%)\n",
757 |       "2025/08/12 20:01:56 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.7207070707070706\n",
758 |       "2025/08/12 20:01:56 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.7207070707070706\n",
759 |       "Average Metric: 2.27 / 3 (75.6%): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 83.95it/s]\n",
760 |       "2025/08/12 20:01:56 INFO dspy.evaluate.evaluate: Average Metric: 2.2666666666666666 / 3 (75.6%)\n",
761 |       "2025/08/12 20:01:56 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for urgency_module.predict: Task: Determine the urgency of a customer message to ProCare Facility Solutions.\n",
762 |       "\n",
763 |       "Context and domain:\n",
764 |       "- Messages are typically sent to ProCare Facility Solutions’ support team about facilities services (e.g., office/residential maintenance, cleaning, HVAC).\n",
765 |       "- Common topics include cleaning quality (especially in high-traffic areas), HVAC performance/safety, routine maintenance scheduling, and general inquiries (e.g., sustainability practices).\n",
766 |       "\n",
767 |       "How to assess urgency:\n",
768 |       "Use these primary factors:\n",
769 |       "1) Safety and risk:\n",
770 |       "   - High/urgent if there’s an immediate safety hazard or potential harm (e.g., electrical sparks, gas smell, active water leak/flood, critical HVAC failure in extreme conditions, security breach).\n",
771 |       "   - Medium if safety is mentioned but described as minor or without signs of imminent danger (e.g., “minor safety concerns” about HVAC, no indication of immediate risk).\n",
772 |       "2) Operational impact:\n",
773 |       "   - High if a critical system outage or issue prevents normal operations or poses serious disruption now.\n",
774 |       "   - Medium for service degradation or quality inconsistencies that need timely attention but are not emergencies (e.g., inconsistent cleaning in lobby/conference rooms, HVAC underperforming).\n",
775 |       "   - Low if no operational impact is described and the message is informational only.\n",
776 |       "3) Time sensitivity and deadlines:\n",
777 |       "   - High if action is needed immediately, today, or within 24–48 hours; explicit urgent language like “ASAP,” “emergency,” “immediately,” or a near-term hard deadline.\n",
778 |       "   - Medium if the sender requests scheduling within about 1–2 weeks or mentions an upcoming event within that window.\n",
779 |       "   - Low if no timeframe is specified and the request is routine or informational.\n",
780 |       "4) Tone and intent:\n",
781 |       "   - Phrases like “prompt response” alone do not imply high urgency (polite closing).\n",
782 |       "   - Curiosity/information requests without issues (e.g., eco-friendly practices) are typically low.\n",
783 |       "\n",
784 |       "Classification guide:\n",
785 |       "- High: Imminent safety hazard, critical failure/outage, or explicit urgent/near-term deadline (today/tomorrow).\n",
786 |       "- Medium: Non-critical but important issues requiring timely attention (quality/safety concerns without immediate danger; routine maintenance requested within ~1–2 weeks or tied to an upcoming event).\n",
787 |       "- Low: General inquiries, non-pressing interest or information requests, or routine items with no stated timeline/impact.\n",
788 |       "\n",
789 |       "Examples to mirror:\n",
790 |       "- Cleaning inconsistencies in high-traffic areas + minor HVAC safety concerns, no emergency described → medium.\n",
791 |       "- Routine HVAC maintenance with decreased performance and an event in 1–2 weeks → medium.\n",
792 |       "- Inquiry about eco-friendly/sustainability practices, no issue or deadline → low.\n",
793 |       "\n",
794 |       "Output format:\n",
795 |       "- Return exactly two fields:\n",
796 |       "  reasoning: 1–3 concise sentences explaining the classification based on the factors above, referencing details from the message.\n",
797 |       "  urgency: one of: low | medium | high\n",
798 |       "\n",
799 |       "Do not ask questions or add extra sections. Keep the reasoning succinct and specific.\n",
800 |       "2025/08/12 20:01:56 INFO dspy.evaluate.evaluate: Average Metric: 2.2666666666666666 / 3 (75.6%)\n",
801 |       "2025/08/12 20:01:56 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New subsample score is not better, skipping\n",
802 |       "2025/08/12 20:01:56 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 0 score: 0.7207070707070706\n",
803 |       "\n",
804 |       "Average Metric: 2.93 / 3 (97.8%): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 57.84it/s]\n",
805 |       "2025/08/12 20:01:56 INFO dspy.evaluate.evaluate: Average Metric: 2.9333333333333336 / 3 (97.8%)\n",
806 |       "2025/08/12 20:01:56 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for sentiment_module.predict: You are given a single input:\n",
807 |       "- message: A professional email-style message, often addressed to ProCare Facility Solutions (facility management/maintenance services). Messages may discuss maintenance quality, safety, cleaning products, HVAC performance, minor household issues (e.g., a leaking faucet), exhibit/artifact preservation needs, or requests for follow-up service.\n",
808 |       "\n",
809 |       "Your task:\n",
810 |       "- Determine the overall sentiment conveyed by the message and briefly explain your reasoning.\n",
811 |       "\n",
812 |       "Key guidance for this domain:\n",
813 |       "- Many messages will be polite, professional, and solution-seeking, even when describing problems (e.g., concerns about cleaning residues affecting artifacts, inconsistent HVAC performance, or minor leaks). Such messages are typically neutral if they lack strong emotional language.\n",
814 |       "- Standard courtesies (greetings, “thank you,” “best regards”) do not make a message positive by themselves.\n",
815 |       "- The mere presence of an issue (e.g., reporting a problem or oversight) is not inherently negative; assess the tone and emotional intensity.\n",
816 |       "- Treat references to ProCare Facility Solutions, maintenance protocols, HVAC, exhibit/artifacts, and similar domain terms as context, not sentiment indicators.\n",
817 |       "\n",
818 |       "Label definitions:\n",
819 |       "- positive: Clear praise, satisfaction, or strong gratitude about services or outcomes.\n",
820 |       "- neutral: Polite/professional tone; factual reporting; requests for help; constructive feedback without strong emotion; balanced notes of appreciation with mild concerns.\n",
821 |       "- negative: Clear dissatisfaction, frustration, blame, anger, or strong negative emotion; threats/urgency framed with discontent.\n",
822 |       "- mixed: Meaningful presence of both positive and negative emotions of similar weight.\n",
823 |       "- unclear: Insufficient content to infer sentiment.\n",
824 |       "\n",
825 |       "Process:\n",
826 |       "1) Read the message fully.\n",
827 |       "2) Identify explicit sentiment cues (praise, complaints, frustration, satisfaction) and evaluate overall tone and intent.\n",
828 |       "3) Choose the single best label from [positive, neutral, negative, mixed, unclear].\n",
829 |       "4) Justify briefly.\n",
830 |       "\n",
831 |       "Output format (plain text, no extra fields):\n",
832 |       "- reasoning: 1–3 concise sentences explaining the tone and why it maps to the chosen label. Do not summarize the entire message; focus on sentiment cues.\n",
833 |       "- sentiment: one of positive, neutral, negative, mixed, unclear (lowercase).\n",
834 |       "\n",
835 |       "Do not include any additional sections or formatting beyond these two fields.\n",
836 |       "\n",
837 |       "2025/08/12 20:01:56 INFO dspy.evaluate.evaluate: Average Metric: 2.9333333333333336 / 3 (97.8%)\n",
838 |       "2025/08/12 20:01:56 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New subsample score is not better, skipping\n",
839 |       "2025/08/12 20:01:56 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Selected program 0 score: 0.7207070707070706\n",
840 |       "Average Metric: 2.90 / 3 (96.7%): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 66.36it/s]\n",
841 |       "2025/08/12 20:01:56 INFO dspy.evaluate.evaluate: Average Metric: 2.9 / 3 (96.7%)\n",
842 |       "2025/08/12 20:01:56 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Proposed new text for categories_module.predict: You are classifying customer messages sent to ProCare Facility Solutions (a facilities/cleaning services provider). Your goal is to read a single message and assign all applicable categories from a fixed list. Use evidence from the message only; select all that apply; do not add categories that are not supported by the text.\n",
843 |       "\n",
844 |       "Allowed categories and definitions:\n",
845 |       "- cleaning_services_scheduling\n",
846 |       "  - Use when the primary intent is to schedule, reschedule, adjust, or inquire about dates/times for cleaning services.\n",
847 |       "  - Includes: requests to change cleaning times, book a service, check availability, or align schedules.\n",
848 |       "  - Exclude when rescheduling is requested only as part of resolving a complaint about poor service (see rule below).\n",
849 |       "\n",
850 |       "- specialized_cleaning_services\n",
851 |       "  - Use when the message mentions specific/specialized cleaning types or tasks, such as deep cleaning, carpet maintenance, window washing, or other specialized treatments beyond generic cleaning.\n",
852 |       "\n",
853 |       "- customer_feedback_and_complaints\n",
854 |       "  - Use when the message expresses dissatisfaction, reports subpar service, communication issues, requests remedies (e.g., redo, refund), or otherwise complains about prior service.\n",
855 |       "\n",
856 |       "- quality_and_safety_concerns\n",
857 |       "  - Use when the message raises concerns about service quality not meeting standards (e.g., still stained, not clean enough) or safety issues.\n",
858 |       "  - Can co-occur with customer_feedback_and_complaints when the sender is unhappy with outcomes.\n",
859 |       "\n",
860 |       "- general_inquiries\n",
861 |       "  - Use when the sender asks for information (e.g., availability, requirements, what’s included) before committing, or seeks clarification about services or process.\n",
862 |       "\n",
863 |       "Key decision rules:\n",
864 |       "- Multi-label: Assign every category that is clearly supported by the message.\n",
865 |       "- Scheduling vs. Complaint distinction:\n",
866 |       "  - If the message’s main purpose is logistics around timing/availability (initial booking, standalone reschedule/adjustment), include cleaning_services_scheduling.\n",
867 |       "  - If rescheduling is requested only as a remedy within a complaint about poor service, do NOT include cleaning_services_scheduling. In that case, include customer_feedback_and_complaints and quality_and_safety_concerns (and specialized_cleaning_services if specific tasks are referenced).\n",
868 |       "- Specialized services: If the message mentions deep cleaning, carpet maintenance, window washing, or similar, include specialized_cleaning_services in addition to any other applicable categories.\n",
869 |       "- General inquiries often co-occur: If the sender is asking for availability or requirements before booking, include general_inquiries alongside cleaning_services_scheduling and any service-type categories.\n",
870 |       "- Do not infer categories not supported by the text (e.g., do not add scheduling if the only scheduling mention is within a complaint resolution).\n",
871 |       "\n",
872 |       "Domain notes:\n",
873 |       "- Messages pertain to cleaning services for residential or commercial properties handled by ProCare Facility Solutions.\n",
874 |       "- Examples of specialized tasks: deep cleaning, carpet maintenance, window washing.\n",
875 |       "- Contexts may include high-profile clients, confidentiality/privacy needs, and urgency; these do not create categories by themselves unless tied to the definitions above.\n",
876 |       "\n",
877 |       "Output format:\n",
878 |       "- Provide two top-level keys in plain text:\n",
879 |       "  - reasoning: A brief justification naming each selected category and noting any notable exclusions (especially the scheduling-vs-complaint distinction, when relevant).\n",
[TRUNCATED]
```

tutorials/image_generation_prompting/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Image Generation Prompt iteration\n",
8 |     "\n",
9 |     "This is based off of a tweet from [@ThorondorLLC](https://x.com/ThorondorLLC)\n",
10 |     "\n",
11 |     "Tweet is [here](https://x.com/ThorondorLLC/status/1880048546382221313)\n",
12 |     "\n",
13 |     "This will take an initial desired prompt, and iteratively refine it until the image generated matches the desired prompt.\n",
14 |     "\n",
15 |     "This is not DSPy prompt optimization as it is normally used, but it is a good example of how to use multimodal DSPy.\n",
16 |     "\n",
17 |     "A future upgrade would be to create a dataset of initial, final prompts to optimize the prompt generation."
18 |    ]
19 |   },
20 |   {
21 |    "cell_type": "markdown",
22 |    "metadata": {},
23 |    "source": [
24 |     "You can install DSPy via:\n",
25 |     "```bash\n",
26 |     "pip install -U dspy\n",
27 |     "```\n",
28 |     "\n",
29 |     "For this example, we'll use Flux Pro from FAL. You can get an API key [here](https://fal.com/flux-pro)\n",
30 |     "\n",
31 |     "We will also need to install Pillow and dotenv.\n",
32 |     "```bash\n",
33 |     "pip install fal-client pillow dotenv\n",
34 |     "```\n",
35 |     "\n",
36 |     "\n",
37 |     "Now, let's import the necessary libraries and set up the environment:"
38 |    ]
39 |   },
40 |   {
41 |    "cell_type": "code",
42 |    "execution_count": null,
43 |    "metadata": {},
44 |    "outputs": [],
45 |    "source": [
46 |     "# Optional\n",
47 |     "#os.environ[\"FAL_API_KEY\"] = \"your_fal_api_key\"\n",
48 |     "#os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\""
49 |    ]
50 |   },
51 |   {
52 |    "cell_type": "code",
53 |    "execution_count": 1,
54 |    "metadata": {},
55 |    "outputs": [
56 |     {
57 |      "name": "stderr",
58 |      "output_type": "stream",
59 |      "text": [
60 |       "/Users/isaac/sd_optimizer/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
61 |       "* 'fields' has been removed\n",
62 |       "  warnings.warn(message, UserWarning)\n",
63 |       "/Users/isaac/sd_optimizer/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
64 |       "  from .autonotebook import tqdm as notebook_tqdm\n"
65 |      ]
66 |     }
67 |    ],
68 |    "source": [
69 |     "import dspy\n",
70 |     "\n",
71 |     "from PIL import Image\n",
72 |     "from io import BytesIO\n",
73 |     "import requests\n",
74 |     "import fal_client\n",
75 |     "\n",
76 |     "from dotenv import load_dotenv\n",
77 |     "load_dotenv()\n",
78 |     "\n",
79 |     "# import display\n",
80 |     "from IPython.display import display\n",
81 |     "\n",
82 |     "lm = dspy.LM(model=\"gpt-4o-mini\", temperature=0.5)\n",
83 |     "dspy.settings.configure(lm=lm)"
84 |    ]
85 |   },
86 |   {
87 |    "cell_type": "code",
88 |    "execution_count": 9,
89 |    "metadata": {},
90 |    "outputs": [],
91 |    "source": [
92 |     "def generate_image(prompt):\n",
93 |     "\n",
94 |     "    request_id = fal_client.submit(\n",
95 |     "        \"fal-ai/flux-pro/v1.1-ultra\",\n",
96 |     "        arguments={\n",
97 |     "            \"prompt\": prompt\n",
98 |     "        },\n",
99 |     "    ).request_id\n",
100 |     "\n",
101 |     "    result = fal_client.result(\"fal-ai/flux-pro/v1.1-ultra\", request_id)\n",
102 |     "    url = result[\"images\"][0][\"url\"]\n",
103 |     "\n",
104 |     "    return dspy.Image.from_url(url)\n",
105 |     "\n",
106 |     "def display_image(image):\n",
107 |     "    url = image.url\n",
108 |     "    # download the image\n",
109 |     "    response = requests.get(url)\n",
110 |     "    image = Image.open(BytesIO(response.content))\n",
111 |     "\n",
112 |     "    # display at 25% of original size\n",
113 |     "    display(image.resize((image.width // 4, image.height // 4)))\n"
114 |    ]
115 |   },
116 |   {
117 |    "cell_type": "code",
118 |    "execution_count": 18,
119 |    "metadata": {},
120 |    "outputs": [
121 |     {
122 |      "name": "stdout",
123 |      "output_type": "stream",
124 |      "text": [
125 |       "Iteration 1 of 5\n"
126 |      ]
127 |     },
128 |     {
129 |      "data": {
130 |       "image/jpeg": "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",
[TRUNCATED]
```

tutorials/mcp/index.md
```
1 | # Tutorial: Use MCP tools in DSPy
2 | 
3 | MCP, standing for Model Context Protocol, is an open protocol that standardizes how applications
4 | provide context to LLMs. Despite some development overhead, MCP offers a valuable opportunity to
5 | share tools, resources, and prompts with other developers regardless of the technical stack you are
6 | using. Likewise, you can use the tools built by other developers without rewriting code.
7 | 
8 | In this guide, we will walk you through how to use MCP tools in DSPy. For demonstration purposes,
9 | we will build an airline service agent that can help users book flights and modify or cancel
10 | existing bookings. This will rely on an MCP server with custom tools, but it should be easy to generalize
11 | to [MCP servers built by the community](https://modelcontextprotocol.io/examples).
12 | 
13 | ??? "How to run this tutorial"
14 |     This tutorial cannot be run in hosted IPython notebooks like Google Colab or Databricks notebooks.
15 |     To run the code, you will need to follow the guide to write code on your local device. The code
16 |     is tested on macOS and should work the same way in Linux environments.
17 | 
18 | ## Install Dependencies
19 | 
20 | Before starting, let's install the required dependencies:
21 | 
22 | ```shell
23 | pip install -U "dspy[mcp]"
24 | ```
25 | 
26 | ## MCP Server Setup
27 | 
28 | Let's first set up the MCP server for the airline agent, which contains:
29 | 
30 | - A set of databases
31 |   - User database, storing user information.
32 |   - Flight database, storing flight information.
33 |   - Ticket database, storing customer tickets.
34 | - A set of tools
35 |   - fetch_flight_info: get flight information for specific dates.
36 |   - fetch_itinerary: get information about booked itineraries.
37 |   - book_itinerary: book a flight on behalf of the user.
38 |   - modify_itinerary: modify an itinerary, either through flight changes or cancellation.
39 |   - get_user_info: get user information.
40 |   - file_ticket: file a backlog ticket for human assistance.
41 | 
42 | In your working directory, create a file `mcp_server.py`, and paste the following content into
43 | it:
44 | 
45 | ```python
46 | import random
47 | import string
48 | 
49 | from mcp.server.fastmcp import FastMCP
50 | from pydantic import BaseModel
51 | 
52 | # Create an MCP server
53 | mcp = FastMCP("Airline Agent")
54 | 
55 | 
56 | class Date(BaseModel):
57 |     # Somehow LLM is bad at specifying `datetime.datetime`
58 |     year: int
59 |     month: int
60 |     day: int
61 |     hour: int
62 | 
63 | 
64 | class UserProfile(BaseModel):
65 |     user_id: str
66 |     name: str
67 |     email: str
68 | 
69 | 
70 | class Flight(BaseModel):
71 |     flight_id: str
72 |     date_time: Date
73 |     origin: str
74 |     destination: str
75 |     duration: float
76 |     price: float
77 | 
78 | 
79 | class Itinerary(BaseModel):
80 |     confirmation_number: str
81 |     user_profile: UserProfile
82 |     flight: Flight
83 | 
84 | 
85 | class Ticket(BaseModel):
86 |     user_request: str
87 |     user_profile: UserProfile
88 | 
89 | 
90 | user_database = {
91 |     "Adam": UserProfile(user_id="1", name="Adam", email="adam@gmail.com"),
92 |     "Bob": UserProfile(user_id="2", name="Bob", email="bob@gmail.com"),
93 |     "Chelsie": UserProfile(user_id="3", name="Chelsie", email="chelsie@gmail.com"),
94 |     "David": UserProfile(user_id="4", name="David", email="david@gmail.com"),
95 | }
96 | 
97 | flight_database = {
98 |     "DA123": Flight(
99 |         flight_id="DA123",
100 |         origin="SFO",
101 |         destination="JFK",
102 |         date_time=Date(year=2025, month=9, day=1, hour=1),
103 |         duration=3,
104 |         price=200,
105 |     ),
106 |     "DA125": Flight(
107 |         flight_id="DA125",
108 |         origin="SFO",
109 |         destination="JFK",
110 |         date_time=Date(year=2025, month=9, day=1, hour=7),
111 |         duration=9,
112 |         price=500,
113 |     ),
114 |     "DA456": Flight(
115 |         flight_id="DA456",
116 |         origin="SFO",
117 |         destination="SNA",
118 |         date_time=Date(year=2025, month=10, day=1, hour=1),
119 |         duration=2,
120 |         price=100,
121 |     ),
122 |     "DA460": Flight(
123 |         flight_id="DA460",
124 |         origin="SFO",
125 |         destination="SNA",
126 |         date_time=Date(year=2025, month=10, day=1, hour=9),
127 |         duration=2,
128 |         price=120,
129 |     ),
130 | }
131 | 
132 | itinery_database = {}
133 | ticket_database = {}
134 | 
135 | 
136 | @mcp.tool()
137 | def fetch_flight_info(date: Date, origin: str, destination: str):
138 |     """Fetch flight information from origin to destination on the given date"""
139 |     flights = []
140 | 
141 |     for flight_id, flight in flight_database.items():
142 |         if (
143 |             flight.date_time.year == date.year
144 |             and flight.date_time.month == date.month
145 |             and flight.date_time.day == date.day
146 |             and flight.origin == origin
147 |             and flight.destination == destination
148 |         ):
149 |             flights.append(flight)
150 |     return flights
151 | 
152 | 
153 | @mcp.tool()
154 | def fetch_itinerary(confirmation_number: str):
155 |     """Fetch a booked itinerary information from database"""
156 |     return itinery_database.get(confirmation_number)
157 | 
158 | 
159 | @mcp.tool()
160 | def pick_flight(flights: list[Flight]):
161 |     """Pick up the best flight that matches users' request."""
162 |     sorted_flights = sorted(
163 |         flights,
164 |         key=lambda x: (
165 |             x.get("duration") if isinstance(x, dict) else x.duration,
166 |             x.get("price") if isinstance(x, dict) else x.price,
167 |         ),
168 |     )
169 |     return sorted_flights[0]
170 | 
171 | 
172 | def generate_id(length=8):
173 |     chars = string.ascii_lowercase + string.digits
174 |     return "".join(random.choices(chars, k=length))
175 | 
176 | 
177 | @mcp.tool()
178 | def book_itinerary(flight: Flight, user_profile: UserProfile):
179 |     """Book a flight on behalf of the user."""
180 |     confirmation_number = generate_id()
181 |     while confirmation_number in itinery_database:
182 |         confirmation_number = generate_id()
183 |     itinery_database[confirmation_number] = Itinerary(
184 |         confirmation_number=confirmation_number,
185 |         user_profile=user_profile,
186 |         flight=flight,
187 |     )
188 |     return confirmation_number, itinery_database[confirmation_number]
189 | 
190 | 
191 | @mcp.tool()
192 | def cancel_itinerary(confirmation_number: str, user_profile: UserProfile):
193 |     """Cancel an itinerary on behalf of the user."""
194 |     if confirmation_number in itinery_database:
195 |         del itinery_database[confirmation_number]
196 |         return
197 |     raise ValueError("Cannot find the itinerary, please check your confirmation number.")
198 | 
199 | 
200 | @mcp.tool()
201 | def get_user_info(name: str):
202 |     """Fetch the user profile from database with given name."""
203 |     return user_database.get(name)
204 | 
205 | 
206 | @mcp.tool()
207 | def file_ticket(user_request: str, user_profile: UserProfile):
208 |     """File a customer support ticket if this is something the agent cannot handle."""
209 |     ticket_id = generate_id(length=6)
210 |     ticket_database[ticket_id] = Ticket(
211 |         user_request=user_request,
212 |         user_profile=user_profile,
213 |     )
214 |     return ticket_id
215 | 
216 | 
217 | if __name__ == "__main__":
218 |     mcp.run()
219 | ```
220 | 
221 | Before we start the server, let's take a look at the code.
222 | 
223 | We first create a `FastMCP` instance, which is a utility that helps quickly build an MCP server:
224 | 
225 | ```python
226 | mcp = FastMCP("Airline Agent")
227 | ```
228 | 
229 | Then we define our data structures, which in a real-world application would be the database schema, e.g.:
230 | 
231 | ```python
232 | class Flight(BaseModel):
233 |     flight_id: str
234 |     date_time: Date
235 |     origin: str
236 |     destination: str
237 |     duration: float
238 |     price: float
239 | ```
240 | 
241 | Following that, we initialize our database instances. In a real-world application, these would be connectors to
242 | actual databases, but for simplicity, we just use dictionaries:
243 | 
244 | ```python
245 | user_database = {
246 |     "Adam": UserProfile(user_id="1", name="Adam", email="adam@gmail.com"),
247 |     "Bob": UserProfile(user_id="2", name="Bob", email="bob@gmail.com"),
248 |     "Chelsie": UserProfile(user_id="3", name="Chelsie", email="chelsie@gmail.com"),
249 |     "David": UserProfile(user_id="4", name="David", email="david@gmail.com"),
250 | }
251 | ```
252 | 
253 | The next step is to define the tools and mark them with `@mcp.tool()` so that they are discoverable by
254 | MCP clients as MCP tools:
255 | 
256 | ```python
257 | @mcp.tool()
258 | def fetch_flight_info(date: Date, origin: str, destination: str):
259 |     """Fetch flight information from origin to destination on the given date"""
260 |     flights = []
261 | 
262 |     for flight_id, flight in flight_database.items():
263 |         if (
264 |             flight.date_time.year == date.year
265 |             and flight.date_time.month == date.month
266 |             and flight.date_time.day == date.day
267 |             and flight.origin == origin
268 |             and flight.destination == destination
269 |         ):
270 |             flights.append(flight)
271 |     return flights
272 | ```
273 | 
274 | The last step is spinning up the server:
275 | 
276 | ```python
277 | if __name__ == "__main__":
278 |     mcp.run()
279 | ```
280 | 
281 | Now we have finished writing the server! Let's launch it:
282 | 
283 | ```shell
284 | python path_to_your_working_directory/mcp_server.py
285 | ```
286 | 
287 | ## Write a DSPy Program That Utilizes Tools in MCP Server
288 | 
289 | Now that the server is running, let's build the actual airline service agent which
290 | utilizes the MCP tools in our server to assist users. In your working directory,
291 | create a file named `dspy_mcp_agent.py`, and follow the guide to add code to it.
292 | 
293 | ### Gather Tools from MCP Servers
294 | 
295 | We first need to gather all available tools from the MCP server and make them
296 | usable by DSPy. DSPy provides an API [`dspy.Tool`](https://dspy.ai/api/primitives/Tool/)
297 | as the standard tool interface. Let's convert all the MCP tools to `dspy.Tool`.
298 | 
299 | We need to create an MCP client instance to communicate with the MCP server, fetch all available
300 | tools, and convert them to `dspy.Tool` using the static method `from_mcp_tool`:
301 | 
302 | ```python
303 | from mcp import ClientSession, StdioServerParameters
304 | from mcp.client.stdio import stdio_client
305 | 
306 | # Create server parameters for stdio connection
307 | server_params = StdioServerParameters(
308 |     command="python",  # Executable
309 |     args=["path_to_your_working_directory/mcp_server.py"],
310 |     env=None,
311 | )
312 | 
313 | async def run():
314 |     async with stdio_client(server_params) as (read, write):
315 |         async with ClientSession(read, write) as session:
316 |             # Initialize the connection
317 |             await session.initialize()
318 |             # List available tools
319 |             tools = await session.list_tools()
320 | 
321 |             # Convert MCP tools to DSPy tools
322 |             dspy_tools = []
323 |             for tool in tools.tools:
324 |                 dspy_tools.append(dspy.Tool.from_mcp_tool(session, tool))
325 | 
326 |             print(len(dspy_tools))
327 |             print(dspy_tools[0].args)
328 | 
329 | if __name__ == "__main__":
330 |     import asyncio
331 | 
332 |     asyncio.run(run())
333 | ```
334 | 
335 | With the code above, we have successfully collected all available MCP tools and converted
336 | them to DSPy tools.
337 | 
338 | 
339 | ### Build a DSPy Agent to Handle Customer Requests
340 | 
341 | Now we will use `dspy.ReAct` to build the agent for handling customer requests. `ReAct` stands
342 | for "reasoning and acting," which asks the LLM to decide whether to call a tool or wrap up the process.
343 | If a tool is required, the LLM takes responsibility for deciding which tool to call and providing
344 | the appropriate arguments.
345 | 
346 | As usual, we need to create a `dspy.Signature` to define the input and output of our agent:
347 | 
348 | ```python
349 | import dspy
350 | 
351 | class DSPyAirlineCustomerService(dspy.Signature):
352 |     """You are an airline customer service agent. You are given a list of tools to handle user requests. You should decide the right tool to use in order to fulfill users' requests."""
353 | 
354 |     user_request: str = dspy.InputField()
355 |     process_result: str = dspy.OutputField(
356 |         desc=(
357 |             "Message that summarizes the process result, and the information users need, "
358 |             "e.g., the confirmation_number if it's a flight booking request."
359 |         )
360 |     )
361 | ```
362 | 
363 | And choose an LM for our agent:
364 | 
365 | ```python
366 | dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
367 | ```
368 | 
369 | Then we create the ReAct agent by passing the tools and signature into the `dspy.ReAct` API. We can now
370 | put together the complete code script:
371 | 
372 | ```python
373 | from mcp import ClientSession, StdioServerParameters
374 | from mcp.client.stdio import stdio_client
375 | 
376 | import dspy
377 | 
378 | # Create server parameters for stdio connection
379 | server_params = StdioServerParameters(
380 |     command="python",  # Executable
381 |     args=["script_tmp/mcp_server.py"],  # Optional command line arguments
382 |     env=None,  # Optional environment variables
383 | )
384 | 
385 | 
386 | class DSPyAirlineCustomerService(dspy.Signature):
387 |     """You are an airline customer service agent. You are given a list of tools to handle user requests.
388 |     You should decide the right tool to use in order to fulfill users' requests."""
389 | 
390 |     user_request: str = dspy.InputField()
391 |     process_result: str = dspy.OutputField(
392 |         desc=(
393 |             "Message that summarizes the process result, and the information users need, "
394 |             "e.g., the confirmation_number if it's a flight booking request."
395 |         )
396 |     )
397 | 
398 | 
399 | dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
400 | 
401 | 
402 | async def run(user_request):
403 |     async with stdio_client(server_params) as (read, write):
404 |         async with ClientSession(read, write) as session:
405 |             # Initialize the connection
406 |             await session.initialize()
407 |             # List available tools
408 |             tools = await session.list_tools()
409 | 
410 |             # Convert MCP tools to DSPy tools
411 |             dspy_tools = []
412 |             for tool in tools.tools:
413 |                 dspy_tools.append(dspy.Tool.from_mcp_tool(session, tool))
414 | 
415 |             # Create the agent
416 |             react = dspy.ReAct(DSPyAirlineCustomerService, tools=dspy_tools)
417 | 
418 |             result = await react.acall(user_request=user_request)
419 |             print(result)
420 | 
421 | 
422 | if __name__ == "__main__":
423 |     import asyncio
424 | 
425 |     asyncio.run(run("please help me book a flight from SFO to JFK on 09/01/2025, my name is Adam"))
426 | ```
427 | 
428 | Note that we must call `react.acall` because MCP tools are async by default. Let's execute the script:
429 | 
430 | ```shell
431 | python path_to_your_working_directory/dspy_mcp_agent.py
432 | ```
433 | 
434 | You should see output similar to this:
435 | 
436 | ```
437 | Prediction(
438 |     trajectory={'thought_0': 'I need to fetch flight information for Adam from SFO to JFK on 09/01/2025 to find available flights for booking.', 'tool_name_0': 'fetch_flight_info', 'tool_args_0': {'date': {'year': 2025, 'month': 9, 'day': 1, 'hour': 0}, 'origin': 'SFO', 'destination': 'JFK'}, 'observation_0': ['{"flight_id": "DA123", "date_time": {"year": 2025, "month": 9, "day": 1, "hour": 1}, "origin": "SFO", "destination": "JFK", "duration": 3.0, "price": 200.0}', '{"flight_id": "DA125", "date_time": {"year": 2025, "month": 9, "day": 1, "hour": 7}, "origin": "SFO", "destination": "JFK", "duration": 9.0, "price": 500.0}'], ..., 'tool_name_4': 'finish', 'tool_args_4': {}, 'observation_4': 'Completed.'},
439 |     reasoning="I successfully booked a flight for Adam from SFO to JFK on 09/01/2025. I found two available flights, selected the more economical option (flight DA123 at 1 AM for $200), retrieved Adam's user profile, and completed the booking process. The confirmation number for the flight is 8h7clk3q.",
440 |     process_result='Your flight from SFO to JFK on 09/01/2025 has been successfully booked. Your confirmation number is 8h7clk3q.'
441 | )
442 | ```
443 | 
444 | The `trajectory` field contains the entire thinking and acting process. If you're curious about what's happening
445 | under the hood, check out the [Observability Guide](https://dspy.ai/tutorials/observability/) to set up MLflow,
446 | which visualizes every step happening inside `dspy.ReAct`!
447 | 
448 | 
449 | ## Conclusion
450 | 
451 | In this guide, we built an airline service agent that utilizes a custom MCP server and the `dspy.ReAct` module. In the context
452 | of MCP support, DSPy provides a simple interface for interacting with MCP tools, giving you the flexibility to implement
453 | any functionality you need.
```

tutorials/llms_txt_generation/index.md
```
1 | # Generating llms.txt for Code Documentation with DSPy
2 | 
3 | This tutorial demonstrates how to use DSPy to automatically generate an `llms.txt` file for the DSPy repository itself. The `llms.txt` standard provides LLM-friendly documentation that helps AI systems better understand codebases.
4 | 
5 | ## What is llms.txt?
6 | 
7 | `llms.txt` is a proposed standard for providing structured, LLM-friendly documentation about a project. It typically includes:
8 | 
9 | - Project overview and purpose
10 | - Key concepts and terminology
11 | - Architecture and structure
12 | - Usage examples
13 | - Important files and directories
14 | 
15 | ## Building a DSPy Program for llms.txt Generation
16 | 
17 | Let's create a DSPy program that analyzes a repository and generates comprehensive `llms.txt` documentation.
18 | 
19 | ### Step 1: Define Our Signatures
20 | 
21 | First, we'll define signatures for different aspects of documentation generation:
22 | 
23 | ```python
24 | import dspy
25 | from typing import List
26 | 
27 | class AnalyzeRepository(dspy.Signature):
28 |     """Analyze a repository structure and identify key components."""
29 |     repo_url: str = dspy.InputField(desc="GitHub repository URL")
30 |     file_tree: str = dspy.InputField(desc="Repository file structure")
31 |     readme_content: str = dspy.InputField(desc="README.md content")
32 |     
33 |     project_purpose: str = dspy.OutputField(desc="Main purpose and goals of the project")
34 |     key_concepts: list[str] = dspy.OutputField(desc="List of important concepts and terminology")
35 |     architecture_overview: str = dspy.OutputField(desc="High-level architecture description")
36 | 
37 | class AnalyzeCodeStructure(dspy.Signature):
38 |     """Analyze code structure to identify important directories and files."""
39 |     file_tree: str = dspy.InputField(desc="Repository file structure")
40 |     package_files: str = dspy.InputField(desc="Key package and configuration files")
41 |     
42 |     important_directories: list[str] = dspy.OutputField(desc="Key directories and their purposes")
43 |     entry_points: list[str] = dspy.OutputField(desc="Main entry points and important files")
44 |     development_info: str = dspy.OutputField(desc="Development setup and workflow information")
45 | 
46 | class GenerateLLMsTxt(dspy.Signature):
47 |     """Generate a comprehensive llms.txt file from analyzed repository information."""
48 |     project_purpose: str = dspy.InputField()
49 |     key_concepts: list[str] = dspy.InputField()
50 |     architecture_overview: str = dspy.InputField()
51 |     important_directories: list[str] = dspy.InputField()
52 |     entry_points: list[str] = dspy.InputField()
53 |     development_info: str = dspy.InputField()
54 |     usage_examples: str = dspy.InputField(desc="Common usage patterns and examples")
55 |     
56 |     llms_txt_content: str = dspy.OutputField(desc="Complete llms.txt file content following the standard format")
57 | ```
58 | 
59 | ### Step 2: Create the Repository Analyzer Module
60 | 
61 | ```python
62 | class RepositoryAnalyzer(dspy.Module):
63 |     def __init__(self):
64 |         super().__init__()
65 |         self.analyze_repo = dspy.ChainOfThought(AnalyzeRepository)
66 |         self.analyze_structure = dspy.ChainOfThought(AnalyzeCodeStructure)
67 |         self.generate_examples = dspy.ChainOfThought("repo_info -> usage_examples")
68 |         self.generate_llms_txt = dspy.ChainOfThought(GenerateLLMsTxt)
69 |     
70 |     def forward(self, repo_url, file_tree, readme_content, package_files):
71 |         # Analyze repository purpose and concepts
72 |         repo_analysis = self.analyze_repo(
73 |             repo_url=repo_url,
74 |             file_tree=file_tree,
75 |             readme_content=readme_content
76 |         )
77 |         
78 |         # Analyze code structure
79 |         structure_analysis = self.analyze_structure(
80 |             file_tree=file_tree,
81 |             package_files=package_files
82 |         )
83 |         
84 |         # Generate usage examples
85 |         usage_examples = self.generate_examples(
86 |             repo_info=f"Purpose: {repo_analysis.project_purpose}\nConcepts: {repo_analysis.key_concepts}"
87 |         )
88 |         
89 |         # Generate final llms.txt
90 |         llms_txt = self.generate_llms_txt(
91 |             project_purpose=repo_analysis.project_purpose,
92 |             key_concepts=repo_analysis.key_concepts,
93 |             architecture_overview=repo_analysis.architecture_overview,
94 |             important_directories=structure_analysis.important_directories,
95 |             entry_points=structure_analysis.entry_points,
96 |             development_info=structure_analysis.development_info,
97 |             usage_examples=usage_examples.usage_examples
98 |         )
99 |         
100 |         return dspy.Prediction(
101 |             llms_txt_content=llms_txt.llms_txt_content,
102 |             analysis=repo_analysis,
103 |             structure=structure_analysis
104 |         )
105 | ```
106 | 
107 | ### Step 3: Gather Repository Information
108 | 
109 | Let's create helper functions to extract repository information:
110 | 
111 | ```python
112 | import requests
113 | import os
114 | from pathlib import Path
115 | 
116 | os.environ["GITHUB_ACCESS_TOKEN"] = "<your_access_token>"
117 | 
118 | def get_github_file_tree(repo_url):
119 |     """Get repository file structure from GitHub API."""
120 |     # Extract owner/repo from URL
121 |     parts = repo_url.rstrip('/').split('/')
122 |     owner, repo = parts[-2], parts[-1]
123 |     
124 |     api_url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/main?recursive=1"
125 |     response = requests.get(api_url, headers={
126 |         "Authorization": f"Bearer {os.environ.get('GITHUB_ACCESS_TOKEN')}"
127 |     })
128 |     
129 |     if response.status_code == 200:
130 |         tree_data = response.json()
131 |         file_paths = [item['path'] for item in tree_data['tree'] if item['type'] == 'blob']
132 |         return '\n'.join(sorted(file_paths))
133 |     else:
134 |         raise Exception(f"Failed to fetch repository tree: {response.status_code}")
135 | 
136 | def get_github_file_content(repo_url, file_path):
137 |     """Get specific file content from GitHub."""
138 |     parts = repo_url.rstrip('/').split('/')
139 |     owner, repo = parts[-2], parts[-1]
140 |     
141 |     api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}"
142 |     response = requests.get(api_url, headers={
143 |         "Authorization": f"Bearer {os.environ.get('GITHUB_ACCESS_TOKEN')}"
144 |     })
145 |     
146 |     if response.status_code == 200:
147 |         import base64
148 |         content = base64.b64decode(response.json()['content']).decode('utf-8')
149 |         return content
150 |     else:
151 |         return f"Could not fetch {file_path}"
152 | 
153 | def gather_repository_info(repo_url):
154 |     """Gather all necessary repository information."""
155 |     file_tree = get_github_file_tree(repo_url)
156 |     readme_content = get_github_file_content(repo_url, "README.md")
157 |     
158 |     # Get key package files
159 |     package_files = []
160 |     for file_path in ["pyproject.toml", "setup.py", "requirements.txt", "package.json"]:
161 |         try:
162 |             content = get_github_file_content(repo_url, file_path)
163 |             if "Could not fetch" not in content:
164 |                 package_files.append(f"=== {file_path} ===\n{content}")
165 |         except:
166 |             continue
167 |     
168 |     package_files_content = "\n\n".join(package_files)
169 |     
170 |     return file_tree, readme_content, package_files_content
171 | ```
172 | 
173 | ### Step 4: Configure DSPy and Generate llms.txt
174 | 
175 | ```python
176 | def generate_llms_txt_for_dspy():
177 |     # Configure DSPy (use your preferred LM)
178 |     lm = dspy.LM(model="gpt-4o-mini")
179 |     dspy.configure(lm=lm)
180 |     os.environ["OPENAI_API_KEY"] = "<YOUR OPENAI KEY>"
181 |     
182 |     # Initialize our analyzer
183 |     analyzer = RepositoryAnalyzer()
184 |     
185 |     # Gather DSPy repository information
186 |     repo_url = "https://github.com/stanfordnlp/dspy"
187 |     file_tree, readme_content, package_files = gather_repository_info(repo_url)
188 |     
189 |     # Generate llms.txt
190 |     result = analyzer(
191 |         repo_url=repo_url,
192 |         file_tree=file_tree,
193 |         readme_content=readme_content,
194 |         package_files=package_files
195 |     )
196 |     
197 |     return result
198 | 
199 | # Run the generation
200 | if __name__ == "__main__":
201 |     result = generate_llms_txt_for_dspy()
202 |     
203 |     # Save the generated llms.txt
204 |     with open("llms.txt", "w") as f:
205 |         f.write(result.llms_txt_content)
206 |     
207 |     print("Generated llms.txt file!")
208 |     print("\nPreview:")
209 |     print(result.llms_txt_content[:500] + "...")
210 | ```
211 | 
212 | ## Expected Output Structure
213 | 
214 | The generated `llms.txt` for DSPy would follow this structure:
215 | 
216 | ```
217 | # DSPy: Programming Language Models
218 | 
219 | ## Project Overview
220 | DSPy is a framework for programming—rather than prompting—language models...
221 | 
222 | ## Key Concepts
223 | - **Modules**: Building blocks for LM programs
224 | - **Signatures**: Input/output specifications  
225 | - **Teleprompters**: Optimization algorithms
226 | - **Predictors**: Core reasoning components
227 | 
228 | ## Architecture
229 | - `/dspy/`: Main package directory
230 |   - `/adapters/`: Input/output format handlers
231 |   - `/clients/`: LM client interfaces
232 |   - `/predict/`: Core prediction modules
233 |   - `/teleprompt/`: Optimization algorithms
234 | 
235 | ## Usage Examples
236 | 1. **Building a Classifier**: Using DSPy, a user can define a modular classifier that takes in text data and categorizes it into predefined classes. The user can specify the classification logic declaratively, allowing for easy adjustments and optimizations.
237 | 2. **Creating a RAG Pipeline**: A developer can implement a retrieval-augmented generation pipeline that first retrieves relevant documents based on a query and then generates a coherent response using those documents. DSPy facilitates the integration of retrieval and generation components seamlessly.
238 | 3. **Optimizing Prompts**: Users can leverage DSPy to create a system that automatically optimizes prompts for language models based on performance metrics, improving the quality of responses over time without manual intervention.
239 | 4. **Implementing Agent Loops**: A user can design an agent loop that continuously interacts with users, learns from feedback, and refines its responses, showcasing the self-improving capabilities of the DSPy framework.
240 | 5. **Compositional Code**: Developers can write compositional code that allows different modules of the AI system to interact with each other, enabling complex workflows that can be easily modified and extended.
241 | ```
242 | 
243 | The resulting `llms.txt` file provides a comprehensive, LLM-friendly overview of the DSPy repository that can help other AI systems better understand and work with the codebase.
244 | 
245 | ## Next Steps
246 | 
247 | - Extend the program to analyze multiple repositories
248 | - Add support for different documentation formats
249 | - Create metrics for documentation quality assessment
250 | - Build a web interface for interactive repository analysis
```

tutorials/mem0_react_agent/index.md
```
1 | # Building Memory-Enabled Agents with DSPy ReAct and Mem0
2 | 
3 | This tutorial demonstrates how to build intelligent conversational agents that can remember information across interactions using DSPy's ReAct framework combined with [Mem0](https://docs.mem0.ai/)'s memory capabilities. You'll learn to create agents that can store, retrieve, and use contextual information to provide personalized and coherent responses.
4 | 
5 | ## What You'll Build
6 | 
7 | By the end of this tutorial, you'll have a memory-enabled agent that can:
8 | 
9 | - **Remember user preferences** and past conversations
10 | - **Store and retrieve factual information** about users and topics
11 | - **Use memory to inform decisions** and provide personalized responses
12 | - **Handle complex multi-turn conversations** with context awareness
13 | - **Manage different types of memories** (facts, preferences, experiences)
14 | 
15 | ## Prerequisites
16 | 
17 | - Basic understanding of DSPy and ReAct agents
18 | - Python 3.9+ installed
19 | - API keys for your preferred LLM provider
20 | 
21 | ## Installation and Setup
22 | 
23 | ```bash
24 | pip install dspy mem0ai
25 | ```
26 | 
27 | ## Step 1: Understanding Mem0 Integration
28 | 
29 | Mem0 provides a memory layer that can store, search, and retrieve memories for AI agents. Let's start by understanding how to integrate it with DSPy:
30 | 
31 | ```python
32 | import dspy
33 | from mem0 import Memory
34 | import os
35 | from typing import List, Dict, Any, Optional
36 | from datetime import datetime
37 | 
38 | # Configure environment
39 | os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
40 | 
41 | # Initialize Mem0 memory system
42 | config = {
43 |     "llm": {
44 |         "provider": "openai",
45 |         "config": {
46 |             "model": "gpt-4o-mini",
47 |             "temperature": 0.1
48 |         }
49 |     },
50 |     "embedder": {
51 |         "provider": "openai",
52 |         "config": {
53 |             "model": "text-embedding-3-small"
54 |         }
55 |     }
56 | }
57 | ```
58 | 
59 | ## Step 2: Create Memory-Aware Tools
60 | 
61 | Let's create tools that can interact with the memory system:
62 | 
63 | ```python
64 | import datetime
65 | 
66 | class MemoryTools:
67 |     """Tools for interacting with the Mem0 memory system."""
68 | 
69 |     def __init__(self, memory: Memory):
70 |         self.memory = memory
71 | 
72 |     def store_memory(self, content: str, user_id: str = "default_user") -> str:
73 |         """Store information in memory."""
74 |         try:
75 |             self.memory.add(content, user_id=user_id)
76 |             return f"Stored memory: {content}"
77 |         except Exception as e:
78 |             return f"Error storing memory: {str(e)}"
79 | 
80 |     def search_memories(self, query: str, user_id: str = "default_user", limit: int = 5) -> str:
81 |         """Search for relevant memories."""
82 |         try:
83 |             results = self.memory.search(query, user_id=user_id, limit=limit)
84 |             if not results:
85 |                 return "No relevant memories found."
86 | 
87 |             memory_text = "Relevant memories found:\n"
88 |             for i, result in enumerate(results["results"]):
89 |                 memory_text += f"{i}. {result['memory']}\n"
90 |             return memory_text
91 |         except Exception as e:
92 |             return f"Error searching memories: {str(e)}"
93 | 
94 |     def get_all_memories(self, user_id: str = "default_user") -> str:
95 |         """Get all memories for a user."""
96 |         try:
97 |             results = self.memory.get_all(user_id=user_id)
98 |             if not results:
99 |                 return "No memories found for this user."
100 | 
101 |             memory_text = "All memories for user:\n"
102 |             for i, result in enumerate(results["results"]):
103 |                 memory_text += f"{i}. {result['memory']}\n"
104 |             return memory_text
105 |         except Exception as e:
106 |             return f"Error retrieving memories: {str(e)}"
107 | 
108 |     def update_memory(self, memory_id: str, new_content: str) -> str:
109 |         """Update an existing memory."""
110 |         try:
111 |             self.memory.update(memory_id, new_content)
112 |             return f"Updated memory with new content: {new_content}"
113 |         except Exception as e:
114 |             return f"Error updating memory: {str(e)}"
115 | 
116 |     def delete_memory(self, memory_id: str) -> str:
117 |         """Delete a specific memory."""
118 |         try:
119 |             self.memory.delete(memory_id)
120 |             return "Memory deleted successfully."
121 |         except Exception as e:
122 |             return f"Error deleting memory: {str(e)}"
123 | 
124 | def get_current_time() -> str:
125 |     """Get the current date and time."""
126 |     return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
127 | ```
128 | 
129 | ## Step 3: Build the Memory-Enhanced ReAct Agent
130 | 
131 | Now let's create our main ReAct agent that can use memory:
132 | 
133 | ```python
134 | class MemoryQA(dspy.Signature):
135 |     """
136 |     You're a helpful assistant and have access to memory method.
137 |     Whenever you answer a user's input, remember to store the information in memory
138 |     so that you can use it later.
139 |     """
140 |     user_input: str = dspy.InputField()
141 |     response: str = dspy.OutputField()
142 | 
143 | class MemoryReActAgent(dspy.Module):
144 |     """A ReAct agent enhanced with Mem0 memory capabilities."""
145 | 
146 |     def __init__(self, memory: Memory):
147 |         super().__init__()
148 |         self.memory_tools = MemoryTools(memory)
149 | 
150 |         # Create tools list for ReAct
151 |         self.tools = [
152 |             self.memory_tools.store_memory,
153 |             self.memory_tools.search_memories,
154 |             self.memory_tools.get_all_memories,
155 |             get_current_time,
156 |             self.set_reminder,
157 |             self.get_preferences,
158 |             self.update_preferences,
159 |         ]
160 | 
161 |         # Initialize ReAct with our tools
162 |         self.react = dspy.ReAct(
163 |             signature=MemoryQA,
164 |             tools=self.tools,
165 |             max_iters=6
166 |         )
167 | 
168 |     def forward(self, user_input: str):
169 |         """Process user input with memory-aware reasoning."""
170 |         
171 |         return self.react(user_input=user_input)
172 | 
173 |     def set_reminder(self, reminder_text: str, date_time: str = None, user_id: str = "default_user") -> str:
174 |         """Set a reminder for the user."""
175 |         reminder = f"Reminder set for {date_time}: {reminder_text}"
176 |         return self.memory_tools.store_memory(
177 |             f"REMINDER: {reminder}", 
178 |             user_id=user_id
179 |         )
180 | 
181 |     def get_preferences(self, category: str = "general", user_id: str = "default_user") -> str:
182 |         """Get user preferences for a specific category."""
183 |         query = f"user preferences {category}"
184 |         return self.memory_tools.search_memories(
185 |             query=query,
186 |             user_id=user_id
187 |         )
188 | 
189 |     def update_preferences(self, category: str, preference: str, user_id: str = "default_user") -> str:
190 |         """Update user preferences."""
191 |         preference_text = f"User preference for {category}: {preference}"
192 |         return self.memory_tools.store_memory(
193 |             preference_text,
194 |             user_id=user_id
195 |         )
196 | ```
197 | 
198 | ## Step 4: Running the Memory-Enhanced Agent
199 | 
200 | Let's create a simple interface to interact with our memory-enabled agent:
201 | 
202 | ```python
203 | import time
204 | def run_memory_agent_demo():
205 |     """Demonstration of memory-enhanced ReAct agent."""
206 | 
207 |     # Configure DSPy
208 |     lm = dspy.LM(model='openai/gpt-4o-mini')
209 |     dspy.configure(lm=lm)
210 | 
211 |     # Initialize memory system
212 |     memory = Memory.from_config(config)
213 | 
214 |     # Create our agent
215 |     agent = MemoryReActAgent(memory)
216 | 
217 |     # Sample conversation demonstrating memory capabilities
218 |     print("🧠 Memory-Enhanced ReAct Agent Demo")
219 |     print("=" * 50)
220 | 
221 |     conversations = [
222 |         "Hi, I'm Alice and I love Italian food, especially pasta carbonara.",
223 |         "I'm Alice. I prefer to exercise in the morning around 7 AM.",
224 |         "I'm Alice. What do you remember about my food preferences?",
225 |         "I'm Alice. Set a reminder for me to go grocery shopping tomorrow.",
226 |         "I'm Alice. What are my exercise preferences?",
227 |         "I'm Alice. I also enjoy hiking on weekends.",
228 |         "I'm Alice. What do you know about me so far?"
229 |     ]
230 | 
231 |     for i, user_input in enumerate(conversations, 1):
232 |         print(f"\n📝 User: {user_input}")
233 | 
234 |         try:
235 |             response = agent(user_input=user_input)
236 |             print(f"🤖 Agent: {response.response}")
237 |             time.sleep(1)
238 | 
239 |         except Exception as e:
240 |             print(f"❌ Error: {e}")
241 | 
242 | # Run the demonstration
243 | if __name__ == "__main__":
244 |     run_memory_agent_demo()
245 | ```
246 | 
247 | ## Expected Output
248 | ```
249 | 🧠 Memory-Enhanced ReAct Agent Demo
250 | ==================================================
251 | 
252 | 📝 User: Hi, I'm Alice and I love Italian food, especially pasta carbonara.
253 | 
254 | 🤖 Agent: Hi Alice! It's great to meet you! Pasta carbonara is a delicious choice. Do you have a favorite place to get it, or do you enjoy making it at home?
255 | 
256 | 📝 User: I'm Alice. I prefer to exercise in the morning around 7 AM.
257 | 
258 | 🤖 Agent: Nice to meet you, Alice! I've noted that you prefer to exercise in the morning around 7 AM. If you have any questions or need tips related to your morning workouts, feel free to ask!
259 | 
260 | 📝 User: I'm Alice. What do you remember about my food preferences?
261 | 
262 | 🤖 Agent: I remember that you love Italian food, and your favorite dish is pasta carbonara!
263 | 
264 | 📝 User: I'm Alice. Set a reminder for me to go grocery shopping tomorrow.
265 | 
266 | 🤖 Agent: I have set a reminder for you, Alice, to go grocery shopping tomorrow.
267 | 
268 | 📝 User: I'm Alice. What are my exercise preferences?
269 | 
270 | 🤖 Agent: Your exercise preference is to work out in the morning around 7 AM.
271 | 
272 | 📝 User: I'm Alice. I also enjoy hiking on weekends.
273 | 
274 | 🤖 Agent: Nice to meet you, Alice! It's great to hear that you enjoy hiking on weekends. If you need any recommendations for trails or tips for your hikes, feel free to ask!
275 | 
276 | 📝 User: I'm Alice. What do you know about me so far?
277 | 
278 | 🤖 Agent: So far, I know that you enjoy hiking on weekends, prefer to exercise in the morning around 7 AM, and love Italian food, especially pasta carbonara. Additionally, you have a reminder set for grocery shopping on October 7, 2023.
279 | ```
280 | 
281 | ## Next Steps
282 | 
283 | - **Implement memory persistence** with databases (PostgreSQL, MongoDB)
284 | - **Add memory categorization** and tagging for better organization
285 | - **Create memory expiration policies** for data management
286 | - **Build multi-user memory isolation** for production applications
287 | - **Add memory analytics** and insights
288 | - **Integrate with vector databases** for enhanced semantic search
289 | - **Implement memory compression** for long-term storage efficiency
290 | 
291 | This tutorial demonstrates how DSPy's ReAct framework can be enhanced with Mem0's memory capabilities to create intelligent, context-aware agents that can learn and remember information across interactions, making them more useful for real-world applications.
```

tutorials/multihop_search/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Multi-Hop Retrieval\n",
8 |     "\n",
9 |     "Let's walk through a quick example of building a `dspy.Module` with multiple sub-modules. We'll do this for the task for multi-hop search.\n",
10 |     "\n",
11 |     "Install the latest DSPy via `pip install -U dspy` and follow along. You also need to run `pip install datasets`."
12 |    ]
13 |   },
14 |   {
15 |    "cell_type": "markdown",
16 |    "metadata": {},
17 |    "source": [
18 |     "<details>\n",
19 |     "<summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>\n",
20 |     "\n",
21 |     "### MLflow DSPy Integration\n",
22 |     "\n",
23 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
24 |     "\n",
25 |     "1. Install MLflow\n",
26 |     "\n",
27 |     "```bash\n",
28 |     "%pip install mlflow>=2.20\n",
29 |     "```\n",
30 |     "\n",
31 |     "2. Start MLflow UI in a separate terminal\n",
32 |     "```bash\n",
33 |     "mlflow ui --port 5000\n",
34 |     "```\n",
35 |     "\n",
36 |     "3. Connect the notebook to MLflow\n",
37 |     "```python\n",
38 |     "import mlflow\n",
39 |     "\n",
40 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
41 |     "mlflow.set_experiment(\"DSPy\")\n",
42 |     "```\n",
43 |     "\n",
44 |     "4. Enabling tracing.\n",
45 |     "```python\n",
46 |     "mlflow.dspy.autolog()\n",
47 |     "```\n",
48 |     "\n",
49 |     "![MLflow Trace](./mlflow-tracing-multi-hop.png)\n",
50 |     "\n",
51 |     "\n",
52 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
53 |     "</details>"
54 |    ]
55 |   },
56 |   {
57 |    "cell_type": "markdown",
58 |    "metadata": {},
59 |    "source": [
60 |     "In this tutorial, we'll use a small local LM, Meta's `Llama-3.1-8B-Instruct` which has 8 billion parameters.\n",
61 |     "\n",
62 |     "You might be able to host the 8B model on your laptop with Ollama, on your GPU server with SGLang, or via a provider that hosts it for you like Databricks or Together.\n",
63 |     "\n",
64 |     "In the snippet below, we'll configure this small model as our main LM. We'll also set up a larger LM, i.e. `GPT-4o`, as a teacher that we'll invoke a very small number of times to help teach the small LM. This is technically not necessary; the small model can typically teach itself tasks like this in DSPy. But using a larger teacher will give us some peace of mind, where the initial system or optimizer configuration doesn't matter as much."
65 |    ]
66 |   },
67 |   {
68 |    "cell_type": "code",
69 |    "execution_count": 1,
70 |    "metadata": {},
71 |    "outputs": [],
72 |    "source": [
73 |     "import dspy\n",
74 |     "\n",
75 |     "lm = dspy.LM('<your_provider>/Llama-3.1-8B-Instruct', max_tokens=3000)\n",
76 |     "gpt4o = dspy.LM('openai/gpt-4o', max_tokens=3000)\n",
77 |     "\n",
78 |     "dspy.configure(lm=lm)"
79 |    ]
80 |   },
81 |   {
82 |    "cell_type": "markdown",
83 |    "metadata": {},
84 |    "source": [
85 |     "### Install dependencies and download data\n",
86 |     "\n",
87 |     "To do the retrieval, we'll use the cool BM25S library, as it's pretty lightweight. You can replace this components with whatever you like.\n",
88 |     "\n",
89 |     "```shell\n",
90 |     "> pip install -U bm25s PyStemmer \"jax[cpu]\"\n",
91 |     "```\n",
92 |     "\n",
93 |     "Next, we'll download a snapshot abstracts (i.e., first paragraphs) of all 5,000,000 Wikipedia pages as of 2017. We'll use this as our retrieval corpus.\n",
94 |     "\n",
95 |     "This is 500MB compressed, so the download and decompression may take 2-3 minutes.\n",
96 |     "\n",
97 |     "```python\n",
98 |     "from dspy.utils import download\n",
99 |     "\n",
100 |     "download(\"https://huggingface.co/dspy/cache/resolve/main/wiki.abstracts.2017.tar.gz\")\n",
101 |     "!tar -xzvf wiki.abstracts.2017.tar.gz\n",
102 |     "```"
103 |    ]
104 |   },
105 |   {
106 |    "cell_type": "markdown",
107 |    "metadata": {},
108 |    "source": [
109 |     "Let's now load the corpus."
110 |    ]
111 |   },
112 |   {
113 |    "cell_type": "code",
114 |    "execution_count": 2,
115 |    "metadata": {},
116 |    "outputs": [
117 |     {
118 |      "data": {
119 |       "text/plain": [
120 |        "5233330"
121 |       ]
122 |      },
123 |      "execution_count": 2,
124 |      "metadata": {},
125 |      "output_type": "execute_result"
126 |     }
127 |    ],
128 |    "source": [
129 |     "import ujson\n",
130 |     "corpus = []\n",
131 |     "\n",
132 |     "with open(\"wiki.abstracts.2017.jsonl\") as f:\n",
133 |     "    for line in f:\n",
134 |     "        line = ujson.loads(line)\n",
135 |     "        corpus.append(f\"{line['title']} | {' '.join(line['text'])}\")\n",
136 |     "\n",
137 |     "len(corpus)"
138 |    ]
139 |   },
140 |   {
141 |    "cell_type": "markdown",
142 |    "metadata": {},
143 |    "source": [
144 |     "And then let's index it for BM25 retrieval! This will take 2-3 minutes."
145 |    ]
146 |   },
147 |   {
148 |    "cell_type": "code",
149 |    "execution_count": null,
150 |    "metadata": {},
151 |    "outputs": [],
152 |    "source": [
153 |     "import bm25s\n",
154 |     "import Stemmer\n",
155 |     "\n",
156 |     "stemmer = Stemmer.Stemmer(\"english\")\n",
157 |     "corpus_tokens = bm25s.tokenize(corpus, stopwords=\"en\", stemmer=stemmer)\n",
158 |     "\n",
159 |     "retriever = bm25s.BM25(k1=0.9, b=0.4)\n",
160 |     "retriever.index(corpus_tokens)"
161 |    ]
162 |   },
163 |   {
164 |    "cell_type": "markdown",
165 |    "metadata": {},
166 |    "source": [
167 |     "### Load the HoVer dataset.\n",
168 |     "\n",
169 |     "Let's load a dataset for our task. We'll load examples from the HoVer multi-hop task, where the input is a (really!) complex claim and the output we're seeking is the set of Wikipedia pages that are required to fact-check that claim."
170 |    ]
171 |   },
172 |   {
173 |    "cell_type": "code",
174 |    "execution_count": 4,
175 |    "metadata": {},
176 |    "outputs": [],
177 |    "source": [
178 |     "import random\n",
179 |     "from dspy.datasets import DataLoader\n",
180 |     "\n",
181 |     "kwargs = dict(fields=(\"claim\", \"supporting_facts\", \"hpqa_id\", \"num_hops\"), input_keys=(\"claim\",))\n",
182 |     "hover = DataLoader().from_huggingface(dataset_name=\"hover-nlp/hover\", split=\"train\", trust_remote_code=True, **kwargs)\n",
183 |     "\n",
184 |     "hpqa_ids = set()\n",
185 |     "hover = [\n",
186 |     "    dspy.Example(claim=x.claim, titles=list(set([y[\"key\"] for y in x.supporting_facts]))).with_inputs(\"claim\")\n",
187 |     "    for x in hover\n",
188 |     "    if x[\"num_hops\"] == 3 and x[\"hpqa_id\"] not in hpqa_ids and not hpqa_ids.add(x[\"hpqa_id\"])\n",
189 |     "]\n",
190 |     "\n",
191 |     "random.Random(0).shuffle(hover)\n",
192 |     "trainset, devset, testset = hover[:200], hover[200:500], hover[650:]"
193 |    ]
194 |   },
195 |   {
196 |    "cell_type": "markdown",
197 |    "metadata": {},
198 |    "source": [
199 |     "Let's view an example of this task:"
200 |    ]
201 |   },
202 |   {
203 |    "cell_type": "code",
204 |    "execution_count": 5,
205 |    "metadata": {},
206 |    "outputs": [
207 |     {
208 |      "name": "stdout",
209 |      "output_type": "stream",
210 |      "text": [
211 |       "Claim: This director is known for his work on Miss Potter. The Academy of Motion Picture Arts and Sciences presents the award in which he was nominated for his work in \"Babe\".\n",
212 |       "Pages that must be retrieved: ['Miss Potter', 'Chris Noonan', 'Academy Award for Best Director']\n"
213 |      ]
214 |     }
215 |    ],
216 |    "source": [
217 |     "example = trainset[0]\n",
218 |     "\n",
219 |     "print(\"Claim:\", example.claim)\n",
220 |     "print(\"Pages that must be retrieved:\", example.titles)"
221 |    ]
222 |   },
223 |   {
224 |    "cell_type": "markdown",
225 |    "metadata": {},
226 |    "source": [
227 |     "Now, let's define a function to do the search in Wikipedia. This will use our BM25 index."
228 |    ]
229 |   },
230 |   {
231 |    "cell_type": "code",
232 |    "execution_count": 6,
233 |    "metadata": {},
234 |    "outputs": [],
235 |    "source": [
236 |     "def search(query: str, k: int) -> list[str]:\n",
237 |     "    tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
238 |     "    results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
239 |     "    run = {corpus[doc]: float(score) for doc, score in zip(results[0], scores[0])}\n",
240 |     "    return run"
241 |    ]
242 |   },
243 |   {
244 |    "cell_type": "markdown",
245 |    "metadata": {},
246 |    "source": [
247 |     "Now, let's define the multi-hop program in DSPy. It's going to be super simple: it'll take a `claim` and produce a list `titles: list[str]`.\n",
248 |     "\n",
249 |     "It will do this via two sub-modules: `generate_query` and `append_notes`."
250 |    ]
251 |   },
252 |   {
253 |    "cell_type": "code",
254 |    "execution_count": 7,
255 |    "metadata": {},
256 |    "outputs": [],
257 |    "source": [
258 |     "class Hop(dspy.Module):\n",
259 |     "    def __init__(self, num_docs=10, num_hops=4):\n",
260 |     "        self.num_docs, self.num_hops = num_docs, num_hops\n",
261 |     "        self.generate_query = dspy.ChainOfThought('claim, notes -> query')\n",
262 |     "        self.append_notes = dspy.ChainOfThought('claim, notes, context -> new_notes: list[str], titles: list[str]')\n",
263 |     "\n",
264 |     "    def forward(self, claim: str) -> list[str]:\n",
265 |     "        notes = []\n",
266 |     "        titles = []\n",
267 |     "\n",
268 |     "        for _ in range(self.num_hops):\n",
269 |     "            query = self.generate_query(claim=claim, notes=notes).query\n",
270 |     "            context = search(query, k=self.num_docs)\n",
271 |     "            prediction = self.append_notes(claim=claim, notes=notes, context=context)\n",
272 |     "            notes.extend(prediction.new_notes)\n",
273 |     "            titles.extend(prediction.titles)\n",
274 |     "        \n",
275 |     "        return dspy.Prediction(notes=notes, titles=list(set(titles)))"
276 |    ]
277 |   },
278 |   {
279 |    "cell_type": "markdown",
280 |    "metadata": {},
281 |    "source": [
282 |     "Great. Now let's set up an evaluation metric, `top5_recall`.\n",
283 |     "\n",
284 |     "It will return the fraction of the gold pages (which are always 3) that are retrieved in the top-5 titles returned by the program."
285 |    ]
286 |   },
287 |   {
288 |    "cell_type": "code",
289 |    "execution_count": 8,
290 |    "metadata": {},
291 |    "outputs": [],
292 |    "source": [
293 |     "def top5_recall(example, pred, trace=None):\n",
294 |     "    gold_titles = example.titles\n",
295 |     "    recall = sum(x in pred.titles[:5] for x in gold_titles) / len(gold_titles)\n",
296 |     "\n",
297 |     "    # If we're \"bootstrapping\" for optimization, return True if and only if the recall is perfect.\n",
298 |     "    if trace is not None:\n",
299 |     "        return recall >= 1.0\n",
300 |     "    \n",
301 |     "    # If we're just doing inference, just measure the recall.\n",
302 |     "    return recall\n",
303 |     "\n",
304 |     "evaluate = dspy.Evaluate(devset=devset, metric=top5_recall, num_threads=16, display_progress=True, display_table=5)"
305 |    ]
306 |   },
307 |   {
308 |    "cell_type": "markdown",
309 |    "metadata": {},
310 |    "source": [
311 |     "Let's evaluate our off-the-shelf program!"
312 |    ]
313 |   },
314 |   {
315 |    "cell_type": "code",
316 |    "execution_count": 9,
317 |    "metadata": {},
318 |    "outputs": [
319 |     {
320 |      "name": "stdout",
321 |      "output_type": "stream",
322 |      "text": [
323 |       "Average Metric: 27.67 / 98 (28.2%):  32%|███▏      | 97/300 [00:02<00:04, 49.34it/s]"
324 |      ]
325 |     },
326 |     {
327 |      "name": "stderr",
328 |      "output_type": "stream",
329 |      "text": [
330 |       "2024/12/25 12:18:00 ERROR dspy.utils.parallelizer: Error processing item Example({'claim': \"All That is the show that the co-creator with the host of Vibe and Wild 'N Out had a debut on.\", 'titles': ['Chris Spencer (actor)', 'Nick Cannon', 'Vibe (talk show)']}) (input_keys={'claim'}): Expected dict_keys(['reasoning', 'new_notes', 'titles']) but got dict_keys(['reasoning', 'new_notes']). Set `provide_traceback=True` to see the stack trace.\n"
331 |      ]
332 |     },
333 |     {
334 |      "name": "stdout",
335 |      "output_type": "stream",
336 |      "text": [
337 |       "Average Metric: 59.33 / 186 (31.9%):  62%|██████▏   | 186/300 [00:03<00:02, 51.84it/s]"
338 |      ]
339 |     },
340 |     {
341 |      "name": "stderr",
342 |      "output_type": "stream",
343 |      "text": [
344 |       "2024/12/25 12:18:02 ERROR dspy.utils.parallelizer: Error processing item Example({'claim': 'The song, which Billie Anthony is best known for her Top 10 hit version, topped the UK chart in 1981 in a recording by a platinum-selling British rock and roll singer whose recording and performing career began in the late 1960s.', 'titles': [\"Shakin' Stevens\", 'This Ole House', 'Billie Anthony']}) (input_keys={'claim'}): Expected dict_keys(['reasoning', 'new_notes', 'titles']) but got dict_keys(['reasoning']). Set `provide_traceback=True` to see the stack trace.\n"
345 |      ]
346 |     },
347 |     {
348 |      "name": "stdout",
349 |      "output_type": "stream",
350 |      "text": [
351 |       "Average Metric: 94.00 / 298 (31.5%): 100%|██████████| 300/300 [00:06<00:00, 48.56it/s]\n"
352 |      ]
353 |     },
354 |     {
355 |      "name": "stderr",
356 |      "output_type": "stream",
357 |      "text": [
358 |       "2024/12/25 12:18:04 INFO dspy.evaluate.evaluate: Average Metric: 93.99999999999993 / 300 (31.3%)\n"
359 |      ]
360 |     },
361 |     {
362 |      "data": {
363 |       "text/html": [
364 |        "<div>\n",
365 |        "<style scoped>\n",
366 |        "    .dataframe tbody tr th:only-of-type {\n",
367 |        "        vertical-align: middle;\n",
368 |        "    }\n",
369 |        "\n",
370 |        "    .dataframe tbody tr th {\n",
371 |        "        vertical-align: top;\n",
372 |        "    }\n",
373 |        "\n",
374 |        "    .dataframe thead th {\n",
375 |        "        text-align: right;\n",
376 |        "    }\n",
377 |        "</style>\n",
378 |        "<table border=\"1\" class=\"dataframe\">\n",
379 |        "  <thead>\n",
380 |        "    <tr style=\"text-align: right;\">\n",
381 |        "      <th></th>\n",
382 |        "      <th>claim</th>\n",
383 |        "      <th>example_titles</th>\n",
384 |        "      <th>notes</th>\n",
385 |        "      <th>pred_titles</th>\n",
386 |        "      <th>top5_recall</th>\n",
387 |        "      <th>titles</th>\n",
388 |        "    </tr>\n",
389 |        "  </thead>\n",
390 |        "  <tbody>\n",
391 |        "    <tr>\n",
392 |        "      <th>0</th>\n",
393 |        "      <td>Nike football team has had a player endorse the football boot Nike...</td>\n",
394 |        "      <td>[Nike Hypervenom, Nike Total 90, Marcus Rashford]</td>\n",
395 |        "      <td>['The Nike Total 90 has been replaced by the Nike Hypervenom.', 'T...</td>\n",
396 |        "      <td>['Nike Mercurial Vapor | The Mercurial Vapor is a football boot ma...</td>\n",
397 |        "      <td>✔️ [0.333]</td>\n",
398 |        "      <td>NaN</td>\n",
399 |        "    </tr>\n",
400 |        "    <tr>\n",
401 |        "      <th>1</th>\n",
402 |        "      <td>Bill Boyd is the chairman of the appliance company that operates t...</td>\n",
403 |        "      <td>[Suncoast Hotel and Casino, Boyd Gaming, Thomas Eje]</td>\n",
404 |        "      <td>['Bill Boyd is not mentioned as the chairman of an appliance compa...</td>\n",
405 |        "      <td>[Suncoast Casino, Thomas Eje, Boyd Gaming Corporation, Bill Boyd, ...</td>\n",
406 |        "      <td>✔️ [0.333]</td>\n",
407 |        "      <td>NaN</td>\n",
408 |        "    </tr>\n",
409 |        "    <tr>\n",
410 |        "      <th>2</th>\n",
411 |        "      <td>The president of South Korea was born 24 January 1953. The group t...</td>\n",
412 |        "      <td>[Presidential Council on Nation Branding, Korea, Moon Jae-in, Euh ...</td>\n",
413 |        "      <td>['The president of South Korea was likely born before 1945', 'Euh ...</td>\n",
414 |        "      <td>['Yi Cheol-seung', 'List of Presidents of South Korea', 'Lifespan ...</td>\n",
415 |        "      <td></td>\n",
416 |        "      <td>NaN</td>\n",
417 |        "    </tr>\n",
418 |        "    <tr>\n",
419 |        "      <th>3</th>\n",
420 |        "      <td>The movie Khan Kluay was released 2 months before the 2009 movie t...</td>\n",
421 |        "      <td>[Fantastic Mr. Fox (film), Jason Schwartzman, Khan Kluay]</td>\n",
422 |        "      <td>['The movie Khan Kluay was released in 2006.', 'The 2009 movie tha...</td>\n",
423 |        "      <td>[Khan Kluay, The Darjeeling Limited]</td>\n",
424 |        "      <td>✔️ [0.333]</td>\n",
425 |        "      <td>NaN</td>\n",
426 |        "    </tr>\n",
427 |        "    <tr>\n",
428 |        "      <th>4</th>\n",
429 |        "      <td>The director of Finding Dory co-directed the film A Bug's Life.</td>\n",
430 |        "      <td>[Andrew Stanton, Finding Dory, A Bug's Life]</td>\n",
431 |        "      <td>['The director of Finding Dory is Andrew Stanton and Angus MacLane...</td>\n",
432 |        "      <td>[Finding Dory, A Bug's Life]</td>\n",
433 |        "      <td>✔️ [0.667]</td>\n",
434 |        "      <td>NaN</td>\n",
435 |        "    </tr>\n",
436 |        "  </tbody>\n",
437 |        "</table>\n",
438 |        "</div>"
439 |       ],
440 |       "text/plain": [
441 |        "                                                                   claim  \\\n",
442 |        "0  Nike football team has had a player endorse the football boot Nike...   \n",
443 |        "1  Bill Boyd is the chairman of the appliance company that operates t...   \n",
444 |        "2  The president of South Korea was born 24 January 1953. The group t...   \n",
445 |        "3  The movie Khan Kluay was released 2 months before the 2009 movie t...   \n",
446 |        "4        The director of Finding Dory co-directed the film A Bug's Life.   \n",
447 |        "\n",
448 |        "                                                          example_titles  \\\n",
449 |        "0                      [Nike Hypervenom, Nike Total 90, Marcus Rashford]   \n",
450 |        "1                   [Suncoast Hotel and Casino, Boyd Gaming, Thomas Eje]   \n",
451 |        "2  [Presidential Council on Nation Branding, Korea, Moon Jae-in, Euh ...   \n",
452 |        "3              [Fantastic Mr. Fox (film), Jason Schwartzman, Khan Kluay]   \n",
453 |        "4                           [Andrew Stanton, Finding Dory, A Bug's Life]   \n",
454 |        "\n",
455 |        "                                                                   notes  \\\n",
456 |        "0  ['The Nike Total 90 has been replaced by the Nike Hypervenom.', 'T...   \n",
457 |        "1  ['Bill Boyd is not mentioned as the chairman of an appliance compa...   \n",
458 |        "2  ['The president of South Korea was likely born before 1945', 'Euh ...   \n",
459 |        "3  ['The movie Khan Kluay was released in 2006.', 'The 2009 movie tha...   \n",
460 |        "4  ['The director of Finding Dory is Andrew Stanton and Angus MacLane...   \n",
461 |        "\n",
462 |        "                                                             pred_titles  \\\n",
463 |        "0  ['Nike Mercurial Vapor | The Mercurial Vapor is a football boot ma...   \n",
464 |        "1  [Suncoast Casino, Thomas Eje, Boyd Gaming Corporation, Bill Boyd, ...   \n",
465 |        "2  ['Yi Cheol-seung', 'List of Presidents of South Korea', 'Lifespan ...   \n",
466 |        "3                                   [Khan Kluay, The Darjeeling Limited]   \n",
467 |        "4                                           [Finding Dory, A Bug's Life]   \n",
468 |        "\n",
469 |        "  top5_recall titles  \n",
470 |        "0  ✔️ [0.333]    NaN  \n",
471 |        "1  ✔️ [0.333]    NaN  \n",
472 |        "2                NaN  \n",
473 |        "3  ✔️ [0.333]    NaN  \n",
474 |        "4  ✔️ [0.667]    NaN  "
475 |       ]
476 |      },
477 |      "metadata": {},
478 |      "output_type": "display_data"
479 |     },
480 |     {
481 |      "data": {
482 |       "text/html": [
483 |        "\n",
484 |        "                <div style='\n",
485 |        "                    text-align: center;\n",
486 |        "                    font-size: 16px;\n",
487 |        "                    font-weight: bold;\n",
488 |        "                    color: #555;\n",
489 |        "                    margin: 10px 0;'>\n",
490 |        "                    ... 295 more rows not displayed ...\n",
491 |        "                </div>\n",
492 |        "                "
493 |       ],
494 |       "text/plain": [
495 |        "<IPython.core.display.HTML object>"
496 |       ]
497 |      },
498 |      "metadata": {},
499 |      "output_type": "display_data"
500 |     },
501 |     {
502 |      "data": {
503 |       "text/plain": [
504 |        "31.33"
505 |       ]
506 |      },
507 |      "execution_count": 10,
508 |      "metadata": {},
509 |      "output_type": "execute_result"
510 |     }
511 |    ],
512 |    "source": [
513 |     "evaluate(Hop())"
514 |    ]
515 |   },
516 |   {
517 |    "cell_type": "markdown",
518 |    "metadata": {},
519 |    "source": [
520 |     "<details>\n",
521 |     "<summary>Tracking Evaluation Results in MLflow Experiment</summary>\n",
522 |     "\n",
523 |     "<br/>\n",
524 |     "\n",
525 |     "To track and visualize the evaluation results over time, you can record the results in MLflow Experiment.\n",
526 |     "\n",
527 |     "\n",
528 |     "```python\n",
529 |     "import mlflow\n",
530 |     "\n",
531 |     "with mlflow.start_run(run_name=\"hop_evaluation\"):\n",
532 |     "    evaluate = dspy.Evaluate(\n",
533 |     "        devset=devset,\n",
534 |     "        metric=top5_recall,\n",
535 |     "        num_threads=16,\n",
536 |     "        display_progress=True,\n",
537 |     "    )\n",
538 |     "\n",
539 |     "    # Evaluate the program as usual\n",
540 |     "    result = evaluate(Hop())\n",
541 |     "\n",
542 |     "    # Log the aggregated score\n",
543 |     "    mlflow.log_metric(\"top5_recall\", result.score)\n",
544 |     "    # Log the detailed evaluation results as a table\n",
545 |     "    mlflow.log_table(\n",
546 |     "        {\n",
547 |     "            \"Claim\": [example.claim for example in eval_set],\n",
548 |     "            \"Expected Titles\": [example.titles for example in eval_set],\n",
549 |     "            \"Predicted Titles\": [output[1] for output in result.results],\n",
550 |     "            \"Top 5 Recall\": [output[2] for output in result.results],\n",
551 |     "        },\n",
552 |     "        artifact_file=\"eval_results.json\",\n",
553 |     "    )\n",
554 |     "```\n",
555 |     "\n",
556 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
557 |     "\n",
558 |     "</details>"
559 |    ]
560 |   },
561 |   {
562 |    "cell_type": "markdown",
563 |    "metadata": {},
564 |    "source": [
565 |     "Let's now optimize the two prompts inside the `Hop()` program jointly to maximize the recall of our program. This may take around 35 minutes and make some $5 worth of calls to GPT-4o to optimize Llama-3.1-8B."
566 |    ]
567 |   },
568 |   {
569 |    "cell_type": "code",
570 |    "execution_count": 10,
571 |    "metadata": {},
572 |    "outputs": [],
573 |    "source": [
574 |     "models = dict(prompt_model=gpt4o, teacher_settings=dict(lm=gpt4o))\n",
575 |     "tp = dspy.MIPROv2(metric=top5_recall, auto=\"medium\", num_threads=16, **models)\n",
576 |     "\n",
577 |     "kwargs = dict(minibatch_size=40, minibatch_full_eval_steps=4)\n",
578 |     "optimized = tp.compile(Hop(), trainset=trainset, max_bootstrapped_demos=4, max_labeled_demos=4, **kwargs)"
579 |    ]
580 |   },
581 |   {
582 |    "cell_type": "markdown",
583 |    "metadata": {},
584 |    "source": [
585 |     "Let's now evaluate again, after optimization."
586 |    ]
587 |   },
588 |   {
589 |    "cell_type": "code",
590 |    "execution_count": 11,
591 |    "metadata": {},
592 |    "outputs": [
593 |     {
594 |      "name": "stdout",
595 |      "output_type": "stream",
596 |      "text": [
597 |       "Average Metric: 38.67 / 64 (60.4%):  21%|██        | 63/300 [00:01<00:06, 38.13it/s]"
598 |      ]
599 |     },
600 |     {
601 |      "name": "stderr",
602 |      "output_type": "stream",
603 |      "text": [
604 |       "2024/12/25 12:18:09 ERROR dspy.utils.parallelizer: Error processing item Example({'claim': 'Eliot Hyman co-founded  Seven Arts Productions in 1957. His co-founder produced the American-American black comedy-drama film directed by Stanley Kubrick.', 'titles': ['Ray Stark', 'Seven Arts Productions', 'Lolita (1962 film)']}) (input_keys={'claim'}): Expected dict_keys(['reasoning', 'query']) but got dict_keys(['reasoning']). Set `provide_traceback=True` to see the stack trace.\n"
605 |      ]
606 |     },
607 |     {
608 |      "name": "stdout",
609 |      "output_type": "stream",
610 |      "text": [
611 |       "Average Metric: 177.33 / 299 (59.3%): 100%|██████████| 300/300 [00:08<00:00, 36.01it/s]"
612 |      ]
613 |     },
614 |     {
615 |      "name": "stderr",
616 |      "output_type": "stream",
617 |      "text": [
618 |       "2024/12/25 12:18:16 INFO dspy.evaluate.evaluate: Average Metric: 177.33333333333334 / 300 (59.1%)\n"
619 |      ]
620 |     },
621 |     {
622 |      "name": "stdout",
623 |      "output_type": "stream",
624 |      "text": [
625 |       "\n"
626 |      ]
627 |     },
628 |     {
629 |      "data": {
630 |       "text/html": [
631 |        "<div>\n",
632 |        "<style scoped>\n",
633 |        "    .dataframe tbody tr th:only-of-type {\n",
634 |        "        vertical-align: middle;\n",
635 |        "    }\n",
636 |        "\n",
637 |        "    .dataframe tbody tr th {\n",
638 |        "        vertical-align: top;\n",
639 |        "    }\n",
640 |        "\n",
641 |        "    .dataframe thead th {\n",
642 |        "        text-align: right;\n",
643 |        "    }\n",
644 |        "</style>\n",
645 |        "<table border=\"1\" class=\"dataframe\">\n",
646 |        "  <thead>\n",
647 |        "    <tr style=\"text-align: right;\">\n",
648 |        "      <th></th>\n",
649 |        "      <th>claim</th>\n",
650 |        "      <th>example_titles</th>\n",
651 |        "      <th>notes</th>\n",
652 |        "      <th>pred_titles</th>\n",
653 |        "      <th>top5_recall</th>\n",
654 |        "      <th>titles</th>\n",
655 |        "    </tr>\n",
656 |        "  </thead>\n",
657 |        "  <tbody>\n",
658 |        "    <tr>\n",
659 |        "      <th>0</th>\n",
660 |        "      <td>Nike football team has had a player endorse the football boot Nike...</td>\n",
661 |        "      <td>[Nike Hypervenom, Nike Total 90, Marcus Rashford]</td>\n",
662 |        "      <td>[]</td>\n",
663 |        "      <td>[Nike Hypervenom, Nike Total 90, Kylian Mbappé, Marcus Rashford]</td>\n",
664 |        "      <td>✔️ [1.000]</td>\n",
665 |        "      <td>NaN</td>\n",
666 |        "    </tr>\n",
667 |        "    <tr>\n",
668 |        "      <th>1</th>\n",
669 |        "      <td>Bill Boyd is the chairman of the appliance company that operates t...</td>\n",
670 |        "      <td>[Suncoast Hotel and Casino, Boyd Gaming, Thomas Eje]</td>\n",
671 |        "      <td>[]</td>\n",
672 |        "      <td>[Bill Boyd, Suncoast Casino, Las Vegas, Thomas Eje]</td>\n",
673 |        "      <td>✔️ [0.333]</td>\n",
674 |        "      <td>NaN</td>\n",
675 |        "    </tr>\n",
676 |        "    <tr>\n",
677 |        "      <th>2</th>\n",
678 |        "      <td>The president of South Korea was born 24 January 1953. The group t...</td>\n",
679 |        "      <td>[Presidential Council on Nation Branding, Korea, Moon Jae-in, Euh ...</td>\n",
680 |        "      <td>['Euh Yoon-Dae is a South Korean professor, financier, and advisor...</td>\n",
681 |        "      <td>[Euh Yoon-Dae, KB Financial Group, Chang Dae-hwan, Maeil Business ...</td>\n",
682 |        "      <td></td>\n",
683 |        "      <td>NaN</td>\n",
684 |        "    </tr>\n",
685 |        "    <tr>\n",
686 |        "      <th>3</th>\n",
687 |        "      <td>The movie Khan Kluay was released 2 months before the 2009 movie t...</td>\n",
688 |        "      <td>[Fantastic Mr. Fox (film), Jason Schwartzman, Khan Kluay]</td>\n",
689 |        "      <td>[\"Jason Schwartzman collaborated with Wes Anderson on the 2009 mov...</td>\n",
690 |        "      <td>[Wes Anderson, Fantastic Mr. Fox, Khan Kluay 2, Jason Schwartzman,...</td>\n",
691 |        "      <td>✔️ [0.667]</td>\n",
692 |        "      <td>NaN</td>\n",
693 |        "    </tr>\n",
694 |        "    <tr>\n",
695 |        "      <th>4</th>\n",
696 |        "      <td>The director of Finding Dory co-directed the film A Bug's Life.</td>\n",
697 |        "      <td>[Andrew Stanton, Finding Dory, A Bug's Life]</td>\n",
698 |        "      <td>[\"Andrew Stanton co-directed A Bug's Life\", \"John Lasseter directe...</td>\n",
699 |        "      <td>[John Lasseter, Andrew Stanton, Finding Dory, A Bug's Life]</td>\n",
700 |        "      <td>✔️ [1.000]</td>\n",
701 |        "      <td>NaN</td>\n",
702 |        "    </tr>\n",
703 |        "  </tbody>\n",
704 |        "</table>\n",
705 |        "</div>"
706 |       ],
707 |       "text/plain": [
708 |        "                                                                   claim  \\\n",
709 |        "0  Nike football team has had a player endorse the football boot Nike...   \n",
710 |        "1  Bill Boyd is the chairman of the appliance company that operates t...   \n",
711 |        "2  The president of South Korea was born 24 January 1953. The group t...   \n",
712 |        "3  The movie Khan Kluay was released 2 months before the 2009 movie t...   \n",
713 |        "4        The director of Finding Dory co-directed the film A Bug's Life.   \n",
714 |        "\n",
715 |        "                                                          example_titles  \\\n",
716 |        "0                      [Nike Hypervenom, Nike Total 90, Marcus Rashford]   \n",
717 |        "1                   [Suncoast Hotel and Casino, Boyd Gaming, Thomas Eje]   \n",
718 |        "2  [Presidential Council on Nation Branding, Korea, Moon Jae-in, Euh ...   \n",
719 |        "3              [Fantastic Mr. Fox (film), Jason Schwartzman, Khan Kluay]   \n",
720 |        "4                           [Andrew Stanton, Finding Dory, A Bug's Life]   \n",
721 |        "\n",
722 |        "                                                                   notes  \\\n",
723 |        "0                                                                     []   \n",
724 |        "1                                                                     []   \n",
725 |        "2  ['Euh Yoon-Dae is a South Korean professor, financier, and advisor...   \n",
726 |        "3  [\"Jason Schwartzman collaborated with Wes Anderson on the 2009 mov...   \n",
727 |        "4  [\"Andrew Stanton co-directed A Bug's Life\", \"John Lasseter directe...   \n",
728 |        "\n",
729 |        "                                                             pred_titles  \\\n",
730 |        "0       [Nike Hypervenom, Nike Total 90, Kylian Mbappé, Marcus Rashford]   \n",
731 |        "1                    [Bill Boyd, Suncoast Casino, Las Vegas, Thomas Eje]   \n",
732 |        "2  [Euh Yoon-Dae, KB Financial Group, Chang Dae-hwan, Maeil Business ...   \n",
733 |        "3  [Wes Anderson, Fantastic Mr. Fox, Khan Kluay 2, Jason Schwartzman,...   \n",
734 |        "4            [John Lasseter, Andrew Stanton, Finding Dory, A Bug's Life]   \n",
735 |        "\n",
736 |        "  top5_recall titles  \n",
737 |        "0  ✔️ [1.000]    NaN  \n",
738 |        "1  ✔️ [0.333]    NaN  \n",
739 |        "2                NaN  \n",
740 |        "3  ✔️ [0.667]    NaN  \n",
741 |        "4  ✔️ [1.000]    NaN  "
742 |       ]
743 |      },
744 |      "metadata": {},
745 |      "output_type": "display_data"
746 |     },
747 |     {
748 |      "data": {
749 |       "text/html": [
750 |        "\n",
751 |        "                <div style='\n",
752 |        "                    text-align: center;\n",
753 |        "                    font-size: 16px;\n",
754 |        "                    font-weight: bold;\n",
755 |        "                    color: #555;\n",
756 |        "                    margin: 10px 0;'>\n",
757 |        "                    ... 295 more rows not displayed ...\n",
758 |        "                </div>\n",
759 |        "                "
760 |       ],
761 |       "text/plain": [
762 |        "<IPython.core.display.HTML object>"
763 |       ]
764 |      },
765 |      "metadata": {},
766 |      "output_type": "display_data"
767 |     },
768 |     {
769 |      "data": {
770 |       "text/plain": [
771 |        "59.11"
772 |       ]
773 |      },
774 |      "execution_count": 12,
775 |      "metadata": {},
776 |      "output_type": "execute_result"
777 |     }
778 |    ],
779 |    "source": [
780 |     "evaluate(optimized)"
781 |    ]
782 |   },
783 |   {
784 |    "cell_type": "markdown",
785 |    "metadata": {},
786 |    "source": [
787 |     "Awesome. It looks like the system improved drastically from around 30% recall to a little below 60% recall. That was a pretty straightforward approach, but DSPy gives you many tools to continue iterating on this from here.\n",
788 |     "\n",
789 |     "Next, let's inspect the optimized prompts to understand what it has learned. We'll run one query and then inspect the last two prompts, which will show us the prompts used for both sub-modules, in the later iteration inside the `Hop()` program. (Alternatively, if you enabled MLflow Tracing following the instructions above, you can see all steps done by the agent including LLM calls, prompts, tool execution, in a rich tree-view.)"
790 |    ]
791 |   },
792 |   {
793 |    "cell_type": "code",
794 |    "execution_count": 13,
795 |    "metadata": {},
796 |    "outputs": [
797 |     {
798 |      "data": {
799 |       "text/plain": [
800 |        "['Up Against It', 'Bernard-Marie Koltès', 'The Beatles', 'Joe Orton']"
801 |       ]
802 |      },
803 |      "execution_count": 14,
804 |      "metadata": {},
805 |      "output_type": "execute_result"
806 |     }
807 |    ],
808 |    "source": [
809 |     "optimized(claim=\"The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Koltès are both playwrights.\").titles"
810 |    ]
811 |   },
812 |   {
813 |    "cell_type": "code",
814 |    "execution_count": 15,
815 |    "metadata": {},
816 |    "outputs": [
817 |     {
818 |      "name": "stdout",
819 |      "output_type": "stream",
820 |      "text": [
821 |       "\n",
822 |       "\n",
823 |       "\n",
824 |       "\n",
825 |       "\u001b[34m[2024-12-25T12:18:16.177899]\u001b[0m\n",
826 |       "\n",
827 |       "\u001b[31mSystem message:\u001b[0m\n",
828 |       "\n",
829 |       "Your input fields are:\n",
830 |       "1. `claim` (str)\n",
831 |       "2. `notes` (str)\n",
832 |       "\n",
833 |       "Your output fields are:\n",
834 |       "1. `reasoning` (str)\n",
835 |       "2. `query` (str)\n",
836 |       "\n",
837 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
838 |       "\n",
839 |       "[[ ## claim ## ]]\n",
840 |       "{claim}\n",
841 |       "\n",
842 |       "[[ ## notes ## ]]\n",
843 |       "{notes}\n",
844 |       "\n",
845 |       "[[ ## reasoning ## ]]\n",
846 |       "{reasoning}\n",
847 |       "\n",
848 |       "[[ ## query ## ]]\n",
849 |       "{query}\n",
850 |       "\n",
851 |       "[[ ## completed ## ]]\n",
852 |       "\n",
853 |       "In adhering to this structure, your objective is: \n",
854 |       "        Given a claim and a set of notes, generate a query that can be used to gather additional evidence or context to support or refute the claim. Think step by step to ensure the query is specific and relevant to the information provided in the notes.\n",
855 |       "\n",
856 |       "\n",
857 |       "\u001b[31mUser message:\u001b[0m\n",
858 |       "\n",
859 |       "[[ ## claim ## ]]\n",
860 |       "Danyang, Jiangusu and this city are both cities in China. This city was the birthplace of Chen Xiuke.\n",
861 |       "\n",
862 |       "[[ ## notes ## ]]\n",
863 |       "[1] «Chen Xiuke was born in Dongfang, Hainan.»\n",
864 |       "[2] «Danyang is a city in Jiangsu province, China.»\n",
865 |       "[3] «Chen Xiuke was born in Dongfang, Hainan.»\n",
866 |       "[4] «Danyang is a city in Jiangsu province, China.»\n",
867 |       "[5] «Dongfang is a city in Hainan province, China.»\n",
868 |       "[6] «Chen Xiuke was born in Dongfang, Hainan.»\n",
869 |       "[7] «Danyang is a city in Jiangsu province, China.»\n",
870 |       "[8] «Dongfang is a city in Hainan province, China.»\n",
871 |       "[9] «Chen Xiuke was born in Dongfang, Hainan.»\n",
872 |       "[10] «Danyang is a city in Jiangsu province, China.»\n",
873 |       "[11] «Dongfang is a city in Hainan province, China.»\n",
874 |       "\n",
875 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## query ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
876 |       "\n",
877 |       "\n",
878 |       "\u001b[31mAssistant message:\u001b[0m\n",
879 |       "\n",
880 |       "[[ ## reasoning ## ]]\n",
881 |       "The claim states that Danyang, Jiangsu, and another city are both cities in China, and that this other city was the birthplace of Chen Xiuke. According to the notes, Chen Xiuke was born in Dongfang, Hainan. Therefore, the city referred to in the claim as \"this city\" is Dongfang, which is indeed a city in Hainan province, China. The claim is consistent with the information provided in the notes.\n",
882 |       "\n",
883 |       "[[ ## query ## ]]\n",
884 |       "What is the birthplace of Chen Xiuke?\n",
885 |       "\n",
886 |       "[[ ## completed ## ]]\n",
887 |       "\n",
888 |       "\n",
889 |       "\u001b[31mUser message:\u001b[0m\n",
890 |       "\n",
891 |       "[[ ## claim ## ]]\n",
892 |       "The star of Nothing to Report and Gary Barlow have a profession in common.\n",
893 |       "\n",
894 |       "[[ ## notes ## ]]\n",
895 |       "[1] «Chris Jericho is a musician and professional wrestler.»\n",
896 |       "[2] «Gary Barlow is a singer and songwriter, known for being a member of Take That.»\n",
897 |       "[3] «Chris Jericho is a musician and professional wrestler.»\n",
898 |       "[4] «Gary Barlow is a singer and songwriter, known for being a member of Take That.»\n",
899 |       "[5] «Chris Jericho stars in 'Nothing to Report'.»\n",
900 |       "[6] «Chris Jericho is a musician and professional wrestler.»\n",
901 |       "[7] «Gary Barlow is a singer and songwriter, known for being a member of Take That.»\n",
902 |       "[8] «Chris Jericho stars in 'Nothing to Report'.»\n",
903 |       "[9] «Chris Jericho is a musician and professional wrestler.»\n",
904 |       "[10] «Gary Barlow is a singer and songwriter, known for being a member of Take That.»\n",
905 |       "[11] «Chris Jericho stars in 'Nothing to Report'.»\n",
906 |       "\n",
907 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## query ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
908 |       "\n",
909 |       "\n",
910 |       "\u001b[31mAssistant message:\u001b[0m\n",
911 |       "\n",
912 |       "[[ ## reasoning ## ]]\n",
913 |       "The claim suggests that the star of \"Nothing to Report\" and Gary Barlow share a profession. From the notes, we know that Chris Jericho stars in \"Nothing to Report\" and is a musician and professional wrestler. Gary Barlow is a singer and songwriter. The common profession between Chris Jericho and Gary Barlow is that they are both musicians.\n",
914 |       "\n",
915 |       "[[ ## query ## ]]\n",
916 |       "What is the profession that Chris Jericho and Gary Barlow have in common?\n",
917 |       "\n",
918 |       "[[ ## completed ## ]]\n",
919 |       "\n",
920 |       "\n",
921 |       "\u001b[31mUser message:\u001b[0m\n",
922 |       "\n",
923 |       "[[ ## claim ## ]]\n",
924 |       "The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Koltès are both playwrights.\n",
925 |       "\n",
926 |       "[[ ## notes ## ]]\n",
927 |       "N/A\n",
928 |       "\n",
929 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## query ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
930 |       "\n",
931 |       "\n",
932 |       "\u001b[31mResponse:\u001b[0m\n",
933 |       "\n",
934 |       "\u001b[32m[[ ## reasoning ## ]]\n",
935 |       "The claim states that the author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Koltès are both playwrights. However, there is no information in the notes to support or refute this claim. The notes are empty, which means we cannot verify the author of the script or Bernard-Marie Koltès' profession.\n",
936 |       "\n",
937 |       "[[ ## query ## ]]\n",
938 |       "Who is the author of the 1960s unproduced script written for The Beatles, Up Against It?\n",
939 |       "\n",
940 |       "[[ ## completed ## ]]\u001b[0m\n",
941 |       "\n",
942 |       "\n",
943 |       "\n",
944 |       "\n",
945 |       "\n",
946 |       "\u001b[34m[2024-12-25T12:18:16.191896]\u001b[0m\n",
947 |       "\n",
948 |       "\u001b[31mSystem message:\u001b[0m\n",
949 |       "\n",
950 |       "Your input fields are:\n",
951 |       "1. `claim` (str)\n",
952 |       "2. `notes` (str)\n",
953 |       "3. `context` (str)\n",
954 |       "\n",
955 |       "Your output fields are:\n",
956 |       "1. `reasoning` (str)\n",
957 |       "2. `new_notes` (list[str])\n",
958 |       "3. `titles` (list[str])\n",
959 |       "\n",
960 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
961 |       "\n",
962 |       "[[ ## claim ## ]]\n",
963 |       "{claim}\n",
964 |       "\n",
965 |       "[[ ## notes ## ]]\n",
966 |       "{notes}\n",
967 |       "\n",
968 |       "[[ ## context ## ]]\n",
969 |       "{context}\n",
970 |       "\n",
971 |       "[[ ## reasoning ## ]]\n",
972 |       "{reasoning}\n",
973 |       "\n",
974 |       "[[ ## new_notes ## ]]\n",
975 |       "{new_notes}        # note: the value you produce must be pareseable according to the following JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
976 |       "\n",
977 |       "[[ ## titles ## ]]\n",
978 |       "{titles}        # note: the value you produce must be pareseable according to the following JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
979 |       "\n",
980 |       "[[ ## completed ## ]]\n",
981 |       "\n",
982 |       "In adhering to this structure, your objective is: \n",
983 |       "        Analyze the provided `claim`, `notes`, and `context` to generate `new_notes` that further support or refute the claim, and extract relevant `titles` that represent key topics or entities from the context.\n",
984 |       "\n",
985 |       "\n",
986 |       "\u001b[31mUser message:\u001b[0m\n",
987 |       "\n",
988 |       "This is an example of the task, though some input or output fields are not supplied.\n",
989 |       "\n",
990 |       "[[ ## claim ## ]]\n",
991 |       "The figure who currently holds the role Michael Noonan held from 2001 to 2002, was the Minister for Social Protection from 2016 until 2017.\n",
992 |       "\n",
993 |       "[[ ## notes ## ]]\n",
994 |       "Not supplied for this particular example.\n",
995 |       "\n",
996 |       "[[ ## context ## ]]\n",
997 |       "Not supplied for this particular example.\n",
998 |       "\n",
999 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## new_notes ## ]]` (must be formatted as a valid Python list[str]), then `[[ ## titles ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
1000 |       "\n",
1001 |       "\n",
1002 |       "\u001b[31mAssistant message:\u001b[0m\n",
1003 |       "\n",
1004 |       "[[ ## reasoning ## ]]\n",
1005 |       "Not supplied for this particular example.\n",
1006 |       "\n",
1007 |       "[[ ## new_notes ## ]]\n",
1008 |       "Not supplied for this particular example.\n",
1009 |       "\n",
1010 |       "[[ ## titles ## ]]\n",
1011 |       "[\"Michael Noonan\", \"Leader of Fine Gael\", \"Leo Varadkar\"]\n",
1012 |       "\n",
1013 |       "[[ ## completed ## ]]\n",
1014 |       "\n",
1015 |       "\n",
1016 |       "\u001b[31mUser message:\u001b[0m\n",
1017 |       "\n",
1018 |       "This is an example of the task, though some input or output fields are not supplied.\n",
1019 |       "\n",
1020 |       "[[ ## claim ## ]]\n",
1021 |       "The type of area Wiliwili are typically found in have a dominant tree species of Acacia koa. They are of the pea family.\n",
1022 |       "\n",
1023 |       "[[ ## notes ## ]]\n",
1024 |       "Not supplied for this particular example.\n",
1025 |       "\n",
1026 |       "[[ ## context ## ]]\n",
1027 |       "Not supplied for this particular example.\n",
1028 |       "\n",
[TRUNCATED]
```

tutorials/math/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Math Reasoning\n",
8 |     "\n",
9 |     "Let's walk through a quick example of setting up a `dspy.ChainOfThought` module and optimizing it for answering algebra questions.\n",
10 |     "\n",
11 |     "Install the latest DSPy via `pip install -U dspy` and follow along. You also need to run `pip install datasets`.\n",
12 |     "\n",
13 |     "<details>\n",
14 |     "<summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>\n",
15 |     "\n",
16 |     "### MLflow DSPy Integration\n",
17 |     "\n",
18 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
19 |     "\n",
20 |     "1. Install MLflow\n",
21 |     "\n",
22 |     "```bash\n",
23 |     "%pip install mlflow>=2.20\n",
24 |     "```\n",
25 |     "\n",
26 |     "2. Start MLflow UI in a separate terminal\n",
27 |     "```bash\n",
28 |     "mlflow ui --port 5000\n",
29 |     "```\n",
30 |     "\n",
31 |     "3. Connect the notebook to MLflow\n",
32 |     "```python\n",
33 |     "import mlflow\n",
34 |     "\n",
35 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
36 |     "mlflow.set_experiment(\"DSPy\")\n",
37 |     "```\n",
38 |     "\n",
39 |     "4. Enabling tracing.\n",
40 |     "```python\n",
41 |     "mlflow.dspy.autolog()\n",
42 |     "```\n",
43 |     "\n",
44 |     "Once you have completed the steps above, you can see traces for each program execution on the notebook. They provide great visibility into the model's behavior and helps you understand the DSPy's concepts better throughout the tutorial.\n",
45 |     "\n",
46 |     "![MLflow Trace](./mlflow-tracing-math.png)\n",
47 |     "\n",
48 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
49 |     "\n",
50 |     "</details>"
51 |    ]
52 |   },
53 |   {
54 |    "cell_type": "markdown",
55 |    "metadata": {},
56 |    "source": [
57 |     "Let's tell DSPy that we will use OpenAI's `gpt-4o-mini` in our modules. To authenticate, DSPy will look into your `OPENAI_API_KEY`. You can easily swap this out for [other providers or local models](https://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb)."
58 |    ]
59 |   },
60 |   {
61 |    "cell_type": "code",
62 |    "execution_count": 1,
63 |    "metadata": {},
64 |    "outputs": [],
65 |    "source": [
66 |     "import dspy\n",
67 |     "\n",
68 |     "gpt4o_mini = dspy.LM('openai/gpt-4o-mini', max_tokens=2000)\n",
69 |     "gpt4o = dspy.LM('openai/gpt-4o', max_tokens=2000)\n",
70 |     "dspy.configure(lm=gpt4o_mini)  # we'll use gpt-4o-mini as the default LM, unless otherwise specified"
71 |    ]
72 |   },
73 |   {
74 |    "cell_type": "markdown",
75 |    "metadata": {},
76 |    "source": [
77 |     "Next, let's load some data examples from the [MATH](https://arxiv.org/abs/2103.03874) benchmark. We'll use a training split for optimization and evaluate it on a held-out dev set.\n",
78 |     "\n",
79 |     "Please note that the following step will require:\n",
80 |     "```bash\n",
81 |     "%pip install git+https://github.com/hendrycks/math.git\n",
82 |     "```"
83 |    ]
84 |   },
85 |   {
86 |    "cell_type": "code",
87 |    "execution_count": 2,
88 |    "metadata": {},
89 |    "outputs": [
90 |     {
91 |      "name": "stdout",
92 |      "output_type": "stream",
93 |      "text": [
94 |       "350 350\n"
95 |      ]
96 |     }
97 |    ],
98 |    "source": [
99 |     "from dspy.datasets import MATH\n",
100 |     "\n",
101 |     "dataset = MATH(subset='algebra')\n",
102 |     "print(len(dataset.train), len(dataset.dev))"
103 |    ]
104 |   },
105 |   {
106 |    "cell_type": "markdown",
107 |    "metadata": {},
108 |    "source": [
109 |     "Let's inspect one example from the training set."
110 |    ]
111 |   },
112 |   {
113 |    "cell_type": "code",
114 |    "execution_count": 3,
115 |    "metadata": {},
116 |    "outputs": [
117 |     {
118 |      "name": "stdout",
119 |      "output_type": "stream",
120 |      "text": [
121 |       "Question: The doctor has told Cal O'Ree that during his ten weeks of working out at the gym, he can expect each week's weight loss to be $1\\%$ of his weight at the end of the previous week. His weight at the beginning of the workouts is $244$ pounds. How many pounds does he expect to weigh at the end of the ten weeks? Express your answer to the nearest whole number.\n",
122 |       "Answer: 221\n"
123 |      ]
124 |     }
125 |    ],
126 |    "source": [
127 |     "example = dataset.train[0]\n",
128 |     "print(\"Question:\", example.question)\n",
129 |     "print(\"Answer:\", example.answer)"
130 |    ]
131 |   },
132 |   {
133 |    "cell_type": "markdown",
134 |    "metadata": {},
135 |    "source": [
136 |     "Now let's define our module. It's extremely simple: just a chain-of-thought step that takes a `question` and produces an `answer`."
137 |    ]
138 |   },
139 |   {
140 |    "cell_type": "code",
141 |    "execution_count": 4,
142 |    "metadata": {},
143 |    "outputs": [
144 |     {
145 |      "data": {
146 |       "text/plain": [
147 |        "Prediction(\n",
148 |        "    reasoning=\"Cal O'Ree's weight loss each week is $1\\\\%$ of his weight at the end of the previous week. This means that at the end of each week, he retains $99\\\\%$ of his weight from the previous week. \\n\\nIf we denote his weight at the beginning as \\\\( W_0 = 244 \\\\) pounds, then his weight at the end of week \\\\( n \\\\) can be expressed as:\\n\\\\[\\nW_n = W_{n-1} \\\\times 0.99\\n\\\\]\\nThis can be simplified to:\\n\\\\[\\nW_n = W_0 \\\\times (0.99)^n\\n\\\\]\\nAfter 10 weeks, his weight will be:\\n\\\\[\\nW_{10} = 244 \\\\times (0.99)^{10}\\n\\\\]\\n\\nNow, we calculate \\\\( (0.99)^{10} \\\\):\\n\\\\[\\n(0.99)^{10} \\\\approx 0.904382\\n\\\\]\\n\\nNow, we can calculate his expected weight after 10 weeks:\\n\\\\[\\nW_{10} \\\\approx 244 \\\\times 0.904382 \\\\approx 220.5\\n\\\\]\\n\\nRounding to the nearest whole number, Cal O'Ree can expect to weigh approximately \\\\( 221 \\\\) pounds at the end of the ten weeks.\",\n",
149 |        "    answer='221'\n",
150 |        ")"
151 |       ]
152 |      },
153 |      "execution_count": 4,
154 |      "metadata": {},
155 |      "output_type": "execute_result"
156 |     }
157 |    ],
158 |    "source": [
159 |     "module = dspy.ChainOfThought(\"question -> answer\")\n",
160 |     "module(question=example.question)"
161 |    ]
162 |   },
163 |   {
164 |    "cell_type": "markdown",
165 |    "metadata": {},
166 |    "source": [
167 |     "Next, let's set up an evaluator for the zero-shot module above, before prompt optimization."
168 |    ]
169 |   },
170 |   {
171 |    "cell_type": "code",
172 |    "execution_count": 5,
173 |    "metadata": {},
174 |    "outputs": [
175 |     {
176 |      "name": "stdout",
177 |      "output_type": "stream",
178 |      "text": [
179 |       "Average Metric: 259.00 / 350 (74.0%): 100%|██████████| 350/350 [01:30<00:00,  3.85it/s]"
180 |      ]
181 |     },
182 |     {
183 |      "name": "stderr",
184 |      "output_type": "stream",
185 |      "text": [
186 |       "2024/11/28 18:41:55 INFO dspy.evaluate.evaluate: Average Metric: 259 / 350 (74.0%)\n"
187 |      ]
188 |     },
189 |     {
190 |      "name": "stdout",
191 |      "output_type": "stream",
192 |      "text": [
193 |       "\n"
194 |      ]
195 |     },
196 |     {
197 |      "data": {
198 |       "text/html": [
199 |        "<div>\n",
200 |        "<style scoped>\n",
201 |        "    .dataframe tbody tr th:only-of-type {\n",
202 |        "        vertical-align: middle;\n",
203 |        "    }\n",
204 |        "\n",
205 |        "    .dataframe tbody tr th {\n",
206 |        "        vertical-align: top;\n",
207 |        "    }\n",
208 |        "\n",
209 |        "    .dataframe thead th {\n",
210 |        "        text-align: right;\n",
211 |        "    }\n",
212 |        "</style>\n",
213 |        "<table border=\"1\" class=\"dataframe\">\n",
214 |        "  <thead>\n",
215 |        "    <tr style=\"text-align: right;\">\n",
216 |        "      <th></th>\n",
217 |        "      <th>question</th>\n",
218 |        "      <th>example_reasoning</th>\n",
219 |        "      <th>example_answer</th>\n",
220 |        "      <th>pred_reasoning</th>\n",
221 |        "      <th>pred_answer</th>\n",
222 |        "      <th>method</th>\n",
223 |        "    </tr>\n",
224 |        "  </thead>\n",
225 |        "  <tbody>\n",
226 |        "    <tr>\n",
227 |        "      <th>0</th>\n",
228 |        "      <td>What is the smallest integer value of $c$ such that the function $...</td>\n",
229 |        "      <td>The given function has a domain of all real numbers if and only if...</td>\n",
230 |        "      <td>1</td>\n",
231 |        "      <td>To determine the smallest integer value of \\( c \\) such that the f...</td>\n",
232 |        "      <td>1</td>\n",
233 |        "      <td>✔️ [True]</td>\n",
234 |        "    </tr>\n",
235 |        "    <tr>\n",
236 |        "      <th>1</th>\n",
237 |        "      <td>What is the least value of $x$ that is a solution of $|{-x+3}|=7$?</td>\n",
238 |        "      <td>In order to have $|{-x+3}| = 7$, we must have $-x + 3 = 7$ or $-x ...</td>\n",
239 |        "      <td>-4</td>\n",
240 |        "      <td>To solve the equation \\( |{-x+3}|=7 \\), we need to consider the de...</td>\n",
241 |        "      <td>-4</td>\n",
242 |        "      <td>✔️ [True]</td>\n",
243 |        "    </tr>\n",
244 |        "    <tr>\n",
245 |        "      <th>2</th>\n",
246 |        "      <td>Evaluate $\\left\\lceil -\\frac{7}{4}\\right\\rceil$.</td>\n",
247 |        "      <td>$-\\frac{7}{4}$ is between $-1$ and $-2$, so $\\left\\lceil -\\frac{7}...</td>\n",
248 |        "      <td>-1</td>\n",
249 |        "      <td>To evaluate \\(\\left\\lceil -\\frac{7}{4}\\right\\rceil\\), we first nee...</td>\n",
250 |        "      <td>-1</td>\n",
251 |        "      <td>✔️ [True]</td>\n",
252 |        "    </tr>\n",
253 |        "    <tr>\n",
254 |        "      <th>3</th>\n",
255 |        "      <td>A triangle has vertices at coordinates $(11,1)$, $(2,3)$ and $(3,7...</td>\n",
256 |        "      <td>We must find the distance between each pair of points by using the...</td>\n",
257 |        "      <td>10</td>\n",
258 |        "      <td>To find the length of the longest side of the triangle with vertic...</td>\n",
259 |        "      <td>10</td>\n",
260 |        "      <td>✔️ [True]</td>\n",
261 |        "    </tr>\n",
262 |        "    <tr>\n",
263 |        "      <th>4</th>\n",
264 |        "      <td>Let $f(x) = x + 2$ and $g(x) = 1/f(x)$. What is $g(f(-3))$?</td>\n",
265 |        "      <td>First, we find that $f(-3) = (-3) + 2 = -1$. Then, $$g(f(-3)) = g(...</td>\n",
266 |        "      <td>1</td>\n",
267 |        "      <td>To find \\( g(f(-3)) \\), we first need to evaluate \\( f(-3) \\). The...</td>\n",
268 |        "      <td>1</td>\n",
269 |        "      <td>✔️ [True]</td>\n",
270 |        "    </tr>\n",
271 |        "  </tbody>\n",
272 |        "</table>\n",
273 |        "</div>"
274 |       ],
275 |       "text/plain": [
276 |        "                                                                question  \\\n",
277 |        "0  What is the smallest integer value of $c$ such that the function $...   \n",
278 |        "1     What is the least value of $x$ that is a solution of $|{-x+3}|=7$?   \n",
279 |        "2                       Evaluate $\\left\\lceil -\\frac{7}{4}\\right\\rceil$.   \n",
280 |        "3  A triangle has vertices at coordinates $(11,1)$, $(2,3)$ and $(3,7...   \n",
281 |        "4            Let $f(x) = x + 2$ and $g(x) = 1/f(x)$. What is $g(f(-3))$?   \n",
282 |        "\n",
283 |        "                                                       example_reasoning  \\\n",
284 |        "0  The given function has a domain of all real numbers if and only if...   \n",
285 |        "1  In order to have $|{-x+3}| = 7$, we must have $-x + 3 = 7$ or $-x ...   \n",
286 |        "2  $-\\frac{7}{4}$ is between $-1$ and $-2$, so $\\left\\lceil -\\frac{7}...   \n",
287 |        "3  We must find the distance between each pair of points by using the...   \n",
288 |        "4  First, we find that $f(-3) = (-3) + 2 = -1$. Then, $$g(f(-3)) = g(...   \n",
289 |        "\n",
290 |        "  example_answer  \\\n",
291 |        "0              1   \n",
292 |        "1             -4   \n",
293 |        "2             -1   \n",
294 |        "3             10   \n",
295 |        "4              1   \n",
296 |        "\n",
297 |        "                                                          pred_reasoning  \\\n",
298 |        "0  To determine the smallest integer value of \\( c \\) such that the f...   \n",
299 |        "1  To solve the equation \\( |{-x+3}|=7 \\), we need to consider the de...   \n",
300 |        "2  To evaluate \\(\\left\\lceil -\\frac{7}{4}\\right\\rceil\\), we first nee...   \n",
301 |        "3  To find the length of the longest side of the triangle with vertic...   \n",
302 |        "4  To find \\( g(f(-3)) \\), we first need to evaluate \\( f(-3) \\). The...   \n",
303 |        "\n",
304 |        "  pred_answer     method  \n",
305 |        "0           1  ✔️ [True]  \n",
306 |        "1          -4  ✔️ [True]  \n",
307 |        "2          -1  ✔️ [True]  \n",
308 |        "3          10  ✔️ [True]  \n",
309 |        "4           1  ✔️ [True]  "
310 |       ]
311 |      },
312 |      "metadata": {},
313 |      "output_type": "display_data"
314 |     },
315 |     {
316 |      "data": {
317 |       "text/html": [
318 |        "\n",
319 |        "                <div style='\n",
320 |        "                    text-align: center;\n",
321 |        "                    font-size: 16px;\n",
322 |        "                    font-weight: bold;\n",
323 |        "                    color: #555;\n",
324 |        "                    margin: 10px 0;'>\n",
325 |        "                    ... 345 more rows not displayed ...\n",
326 |        "                </div>\n",
327 |        "                "
328 |       ],
329 |       "text/plain": [
330 |        "<IPython.core.display.HTML object>"
331 |       ]
332 |      },
333 |      "metadata": {},
334 |      "output_type": "display_data"
335 |     },
336 |     {
337 |      "data": {
338 |       "text/plain": [
339 |        "74.0"
340 |       ]
341 |      },
342 |      "execution_count": 5,
343 |      "metadata": {},
344 |      "output_type": "execute_result"
345 |     }
346 |    ],
347 |    "source": [
348 |     "THREADS = 24\n",
349 |     "kwargs = dict(num_threads=THREADS, display_progress=True, display_table=5)\n",
350 |     "evaluate = dspy.Evaluate(devset=dataset.dev, metric=dataset.metric, **kwargs)\n",
351 |     "\n",
352 |     "evaluate(module)"
353 |    ]
354 |   },
355 |   {
356 |    "cell_type": "markdown",
357 |    "metadata": {},
358 |    "source": [
359 |     "<details>\n",
360 |     "<summary>Tracking Evaluation Results in MLflow Experiment</summary>\n",
361 |     "\n",
362 |     "<br/>\n",
363 |     "\n",
364 |     "To track and visualize the evaluation results over time, you can record the results in MLflow Experiment.\n",
365 |     "\n",
366 |     "\n",
367 |     "```python\n",
368 |     "import mlflow\n",
369 |     "\n",
370 |     "# Start an MLflow Run to record the evaluation\n",
371 |     "with mlflow.start_run(run_name=\"math_evaluation\"):\n",
372 |     "    kwargs = dict(num_threads=THREADS, display_progress=True)\n",
373 |     "    evaluate = dspy.Evaluate(devset=dataset.dev, metric=dataset.metric, **kwargs)\n",
374 |     "\n",
375 |     "    # Evaluate the program as usual\n",
376 |     "    result = evaluate(module)\n",
377 |     "\n",
378 |     "    # Log the aggregated score\n",
379 |     "    mlflow.log_metric(\"correctness\", result.score)\n",
380 |     "    # Log the detailed evaluation results as a table\n",
381 |     "    mlflow.log_table(\n",
382 |     "        {\n",
383 |     "            \"Question\": [example.question for example in dataset.dev],\n",
384 |     "            \"Gold Answer\": [example.answer for example in dataset.dev],\n",
385 |     "            \"Predicted Answer\": [output[1] for output in result.results],\n",
386 |     "            \"Correctness\": [output[2] for output in result.results],\n",
387 |     "        },\n",
388 |     "        artifact_file=\"eval_results.json\",\n",
389 |     "    )\n",
390 |     "```\n",
391 |     "\n",
392 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
393 |     "\n",
394 |     "</details>"
395 |    ]
396 |   },
397 |   {
398 |    "cell_type": "markdown",
399 |    "metadata": {},
400 |    "source": [
401 |     "And lastly let's optimize our module. Since we want strong reasoning, we'll use the large GPT-4o as the teacher model (used to bootstrap reasoning for the small LM at optimization time) but not as the prompt model (used to craft instructions) or the task model (trained).\n",
402 |     "\n",
403 |     "GPT-4o will be invoked only a small number of times. The model involved directly in optimization and in the resulting (optimized) program will be GPT-4o-mini.\n",
404 |     "\n",
405 |     "We will also specify `max_bootstrapped_demos=4` which means we want at most four bootstrapped examples in the prompt and `max_labeled_demos=4` which means that, in total between bootstrapped and pre-labeled examples, we want at most four."
406 |    ]
407 |   },
408 |   {
409 |    "cell_type": "code",
410 |    "execution_count": null,
411 |    "metadata": {},
412 |    "outputs": [],
413 |    "source": [
414 |     "kwargs = dict(num_threads=THREADS, teacher_settings=dict(lm=gpt4o), prompt_model=gpt4o_mini)\n",
415 |     "optimizer = dspy.MIPROv2(metric=dataset.metric, auto=\"medium\", **kwargs)\n",
416 |     "\n",
417 |     "kwargs = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
418 |     "optimized_module = optimizer.compile(module, trainset=dataset.train, **kwargs)"
419 |    ]
420 |   },
421 |   {
422 |    "cell_type": "code",
423 |    "execution_count": 7,
424 |    "metadata": {},
425 |    "outputs": [
426 |     {
427 |      "name": "stdout",
428 |      "output_type": "stream",
429 |      "text": [
430 |       "Average Metric: 310.00 / 350 (88.6%): 100%|██████████| 350/350 [01:31<00:00,  3.84it/s]"
431 |      ]
432 |     },
433 |     {
434 |      "name": "stderr",
435 |      "output_type": "stream",
436 |      "text": [
437 |       "2024/11/28 18:59:19 INFO dspy.evaluate.evaluate: Average Metric: 310 / 350 (88.6%)\n"
438 |      ]
439 |     },
440 |     {
441 |      "name": "stdout",
442 |      "output_type": "stream",
443 |      "text": [
444 |       "\n"
445 |      ]
446 |     },
447 |     {
448 |      "data": {
449 |       "text/html": [
450 |        "<div>\n",
451 |        "<style scoped>\n",
452 |        "    .dataframe tbody tr th:only-of-type {\n",
453 |        "        vertical-align: middle;\n",
454 |        "    }\n",
455 |        "\n",
456 |        "    .dataframe tbody tr th {\n",
457 |        "        vertical-align: top;\n",
458 |        "    }\n",
459 |        "\n",
460 |        "    .dataframe thead th {\n",
461 |        "        text-align: right;\n",
462 |        "    }\n",
463 |        "</style>\n",
464 |        "<table border=\"1\" class=\"dataframe\">\n",
465 |        "  <thead>\n",
466 |        "    <tr style=\"text-align: right;\">\n",
467 |        "      <th></th>\n",
468 |        "      <th>question</th>\n",
469 |        "      <th>example_reasoning</th>\n",
470 |        "      <th>example_answer</th>\n",
471 |        "      <th>pred_reasoning</th>\n",
472 |        "      <th>pred_answer</th>\n",
473 |        "      <th>method</th>\n",
474 |        "    </tr>\n",
475 |        "  </thead>\n",
476 |        "  <tbody>\n",
477 |        "    <tr>\n",
478 |        "      <th>0</th>\n",
479 |        "      <td>What is the smallest integer value of $c$ such that the function $...</td>\n",
480 |        "      <td>The given function has a domain of all real numbers if and only if...</td>\n",
481 |        "      <td>1</td>\n",
482 |        "      <td>The function \\( f(x) = \\frac{x^2 + 1}{x^2 - x + c} \\) will have a ...</td>\n",
483 |        "      <td>1</td>\n",
484 |        "      <td>✔️ [True]</td>\n",
485 |        "    </tr>\n",
486 |        "    <tr>\n",
487 |        "      <th>1</th>\n",
488 |        "      <td>What is the least value of $x$ that is a solution of $|{-x+3}|=7$?</td>\n",
489 |        "      <td>In order to have $|{-x+3}| = 7$, we must have $-x + 3 = 7$ or $-x ...</td>\n",
490 |        "      <td>-4</td>\n",
491 |        "      <td>The equation \\( |{-x+3}|=7 \\) implies two possible cases: 1. \\(-x ...</td>\n",
492 |        "      <td>-4</td>\n",
493 |        "      <td>✔️ [True]</td>\n",
494 |        "    </tr>\n",
495 |        "    <tr>\n",
496 |        "      <th>2</th>\n",
497 |        "      <td>Evaluate $\\left\\lceil -\\frac{7}{4}\\right\\rceil$.</td>\n",
498 |        "      <td>$-\\frac{7}{4}$ is between $-1$ and $-2$, so $\\left\\lceil -\\frac{7}...</td>\n",
499 |        "      <td>-1</td>\n",
500 |        "      <td>To evaluate \\(\\left\\lceil -\\frac{7}{4}\\right\\rceil\\), we first nee...</td>\n",
501 |        "      <td>-1</td>\n",
502 |        "      <td>✔️ [True]</td>\n",
503 |        "    </tr>\n",
504 |        "    <tr>\n",
505 |        "      <th>3</th>\n",
506 |        "      <td>A triangle has vertices at coordinates $(11,1)$, $(2,3)$ and $(3,7...</td>\n",
507 |        "      <td>We must find the distance between each pair of points by using the...</td>\n",
508 |        "      <td>10</td>\n",
509 |        "      <td>To find the length of the sides of the triangle formed by the vert...</td>\n",
510 |        "      <td>10</td>\n",
511 |        "      <td>✔️ [True]</td>\n",
512 |        "    </tr>\n",
513 |        "    <tr>\n",
514 |        "      <th>4</th>\n",
515 |        "      <td>Let $f(x) = x + 2$ and $g(x) = 1/f(x)$. What is $g(f(-3))$?</td>\n",
516 |        "      <td>First, we find that $f(-3) = (-3) + 2 = -1$. Then, $$g(f(-3)) = g(...</td>\n",
517 |        "      <td>1</td>\n",
518 |        "      <td>To find \\( g(f(-3)) \\), we first need to evaluate \\( f(-3) \\). Usi...</td>\n",
519 |        "      <td>1</td>\n",
520 |        "      <td>✔️ [True]</td>\n",
521 |        "    </tr>\n",
522 |        "  </tbody>\n",
523 |        "</table>\n",
524 |        "</div>"
525 |       ],
526 |       "text/plain": [
527 |        "                                                                question  \\\n",
528 |        "0  What is the smallest integer value of $c$ such that the function $...   \n",
529 |        "1     What is the least value of $x$ that is a solution of $|{-x+3}|=7$?   \n",
530 |        "2                       Evaluate $\\left\\lceil -\\frac{7}{4}\\right\\rceil$.   \n",
531 |        "3  A triangle has vertices at coordinates $(11,1)$, $(2,3)$ and $(3,7...   \n",
532 |        "4            Let $f(x) = x + 2$ and $g(x) = 1/f(x)$. What is $g(f(-3))$?   \n",
533 |        "\n",
534 |        "                                                       example_reasoning  \\\n",
535 |        "0  The given function has a domain of all real numbers if and only if...   \n",
536 |        "1  In order to have $|{-x+3}| = 7$, we must have $-x + 3 = 7$ or $-x ...   \n",
537 |        "2  $-\\frac{7}{4}$ is between $-1$ and $-2$, so $\\left\\lceil -\\frac{7}...   \n",
538 |        "3  We must find the distance between each pair of points by using the...   \n",
539 |        "4  First, we find that $f(-3) = (-3) + 2 = -1$. Then, $$g(f(-3)) = g(...   \n",
540 |        "\n",
541 |        "  example_answer  \\\n",
542 |        "0              1   \n",
543 |        "1             -4   \n",
544 |        "2             -1   \n",
545 |        "3             10   \n",
546 |        "4              1   \n",
547 |        "\n",
548 |        "                                                          pred_reasoning  \\\n",
549 |        "0  The function \\( f(x) = \\frac{x^2 + 1}{x^2 - x + c} \\) will have a ...   \n",
550 |        "1  The equation \\( |{-x+3}|=7 \\) implies two possible cases: 1. \\(-x ...   \n",
551 |        "2  To evaluate \\(\\left\\lceil -\\frac{7}{4}\\right\\rceil\\), we first nee...   \n",
552 |        "3  To find the length of the sides of the triangle formed by the vert...   \n",
553 |        "4  To find \\( g(f(-3)) \\), we first need to evaluate \\( f(-3) \\). Usi...   \n",
554 |        "\n",
555 |        "  pred_answer     method  \n",
556 |        "0           1  ✔️ [True]  \n",
557 |        "1          -4  ✔️ [True]  \n",
558 |        "2          -1  ✔️ [True]  \n",
559 |        "3          10  ✔️ [True]  \n",
560 |        "4           1  ✔️ [True]  "
561 |       ]
562 |      },
563 |      "metadata": {},
564 |      "output_type": "display_data"
565 |     },
566 |     {
567 |      "data": {
568 |       "text/html": [
569 |        "\n",
570 |        "                <div style='\n",
571 |        "                    text-align: center;\n",
572 |        "                    font-size: 16px;\n",
573 |        "                    font-weight: bold;\n",
574 |        "                    color: #555;\n",
575 |        "                    margin: 10px 0;'>\n",
576 |        "                    ... 345 more rows not displayed ...\n",
577 |        "                </div>\n",
578 |        "                "
579 |       ],
580 |       "text/plain": [
581 |        "<IPython.core.display.HTML object>"
582 |       ]
583 |      },
584 |      "metadata": {},
585 |      "output_type": "display_data"
586 |     },
587 |     {
588 |      "data": {
589 |       "text/plain": [
590 |        "88.57"
591 |       ]
592 |      },
593 |      "execution_count": 7,
594 |      "metadata": {},
595 |      "output_type": "execute_result"
596 |     }
597 |    ],
598 |    "source": [
599 |     "evaluate(optimized_module)"
600 |    ]
601 |   },
602 |   {
603 |    "cell_type": "markdown",
604 |    "metadata": {},
605 |    "source": [
606 |     "Neat. It was pretty straightforward to improve quality from 74% to over 88% on a held-out set here.\n",
607 |     "\n",
608 |     "That said, for reasoning tasks like this, you will often want to consider more advanced strategies, like:\n",
609 |     "\n",
610 |     "- A `dspy.ReAct` module with access to a calculator function or `dspy.PythonInterpreter`\n",
611 |     "- Ensembling multiple optimized prompts with a majority vote (or an Aggregator module) on top"
612 |    ]
613 |   },
614 |   {
615 |    "cell_type": "markdown",
616 |    "metadata": {},
617 |    "source": [
618 |     "\n",
619 |     "Just to understand what changed, let's view the prompt after optimization. Alternatively, if you enabled MLflow tracing following above instructions, you can compare the prompts before and after optimization in the rich trace UI."
620 |    ]
621 |   },
622 |   {
623 |    "cell_type": "code",
624 |    "execution_count": 8,
625 |    "metadata": {},
626 |    "outputs": [
627 |     {
628 |      "name": "stdout",
629 |      "output_type": "stream",
630 |      "text": [
631 |       "\n",
632 |       "\n",
633 |       "\n",
634 |       "\n",
635 |       "\u001b[34m[2024-11-28T18:59:19.176586]\u001b[0m\n",
636 |       "\n",
637 |       "\u001b[31mSystem message:\u001b[0m\n",
638 |       "\n",
639 |       "Your input fields are:\n",
640 |       "1. `question` (str)\n",
641 |       "\n",
642 |       "Your output fields are:\n",
643 |       "1. `reasoning` (str)\n",
644 |       "2. `answer` (str)\n",
645 |       "\n",
646 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
647 |       "\n",
648 |       "[[ ## question ## ]]\n",
649 |       "{question}\n",
650 |       "\n",
651 |       "[[ ## reasoning ## ]]\n",
652 |       "{reasoning}\n",
653 |       "\n",
654 |       "[[ ## answer ## ]]\n",
655 |       "{answer}\n",
656 |       "\n",
657 |       "[[ ## completed ## ]]\n",
658 |       "\n",
659 |       "In adhering to this structure, your objective is: \n",
660 |       "        Analyze the `question` provided, and systematically apply mathematical reasoning to derive the `answer`. Ensure to articulate each step of your thought process in the `reasoning` field, detailing how you identify relationships and formulate equations to arrive at the solution.\n",
661 |       "\n",
662 |       "\n",
663 |       "\u001b[31mUser message:\u001b[0m\n",
664 |       "\n",
665 |       "[[ ## question ## ]]\n",
666 |       "If $|4x+2|=10$ and $x<0$, what is the value of $x$?\n",
667 |       "\n",
668 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
669 |       "\n",
670 |       "\n",
671 |       "\u001b[31mAssistant message:\u001b[0m\n",
672 |       "\n",
673 |       "[[ ## reasoning ## ]]\n",
674 |       "The equation $|4x+2|=10$ implies two possible equations: $4x+2=10$ or $4x+2=-10$. We solve each equation separately.\n",
675 |       "\n",
676 |       "1. For $4x+2=10$:\n",
677 |       "   \\[\n",
678 |       "   4x + 2 = 10 \\\\\n",
679 |       "   4x = 8 \\\\\n",
680 |       "   x = 2\n",
681 |       "   \\]\n",
682 |       "   However, $x=2$ does not satisfy the condition $x<0$.\n",
683 |       "\n",
684 |       "2. For $4x+2=-10$:\n",
685 |       "   \\[\n",
686 |       "   4x + 2 = -10 \\\\\n",
687 |       "   4x = -12 \\\\\n",
688 |       "   x = -3\n",
689 |       "   \\]\n",
690 |       "   The solution $x = -3$ satisfies the condition $x<0$.\n",
691 |       "\n",
692 |       "Therefore, the value of $x$ is $\\boxed{-3}$.\n",
693 |       "\n",
694 |       "[[ ## answer ## ]]\n",
695 |       "-3\n",
696 |       "\n",
697 |       "[[ ## completed ## ]]\n",
698 |       "\n",
699 |       "\n",
700 |       "\u001b[31mUser message:\u001b[0m\n",
701 |       "\n",
702 |       "[[ ## question ## ]]\n",
703 |       "Given that $-4$ is a solution to $x^2 + bx -36 = 0$, what is the value of $b$?\n",
704 |       "\n",
705 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
706 |       "\n",
707 |       "\n",
708 |       "\u001b[31mAssistant message:\u001b[0m\n",
709 |       "\n",
710 |       "[[ ## reasoning ## ]]\n",
711 |       "The product of the roots of this quadratic is $-36/1=-36$, so the other solution must be $-36/-4=9$. That means that the sum of the solutions is $-4+9=5$. The sum of the solutions is also $-b/1=-b$. Thus, $-b=5$ and $b=\\boxed{-5}$.\n",
712 |       "\n",
713 |       "[[ ## answer ## ]]\n",
714 |       "-5\n",
715 |       "\n",
716 |       "[[ ## completed ## ]]\n",
717 |       "\n",
718 |       "\n",
719 |       "\u001b[31mUser message:\u001b[0m\n",
720 |       "\n",
721 |       "[[ ## question ## ]]\n",
722 |       "Richard is building a rectangular playground from 200 feet of fencing. The fencing must entirely enclose the playground. What is the maximum area of this playground?\n",
723 |       "\n",
724 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
725 |       "\n",
726 |       "\n",
727 |       "\u001b[31mAssistant message:\u001b[0m\n",
728 |       "\n",
729 |       "[[ ## reasoning ## ]]\n",
730 |       "Let the length of the playground be $l$ and the width be $w$. We have the equation $2l+2w=200 \\Rightarrow l + w = 100$. We want to maximize the area of this rectangular playground, which is given by $lw$. From our equation, we know that $l=100-w$. Substituting this in to our expression for area, we have \\[(100-w)(w)=100w-w^2\\]We will now complete the square to find the maximum value of this expression. Factoring a $-1$ out, we have \\[-(w^2-100w)\\]In order for the expression inside the parenthesis to be a perfect square, we need to add and subtract $(100/2)^2=2500$ inside the parenthesis. Doing this, we get \\[-(w^2-100w+2500-2500) \\Rightarrow -(w-50)^2+2500\\]Since the maximum value of $-(w-50)^2$ is 0 (perfect squares are always nonnegative), the maximum value of the entire expression is 2500, which is achieved when $w=50$ and $l=100-w=50$ (the playground is a square). Thus, the maximum area of the playground is $\\boxed{2500}$ square feet.\n",
731 |       "\n",
732 |       "[[ ## answer ## ]]\n",
733 |       "2500\n",
734 |       "\n",
735 |       "[[ ## completed ## ]]\n",
736 |       "\n",
737 |       "\n",
738 |       "\u001b[31mUser message:\u001b[0m\n",
739 |       "\n",
740 |       "[[ ## question ## ]]\n",
741 |       "Alice and Bob are playing a game. Alice starts first. On Alice's turn, she flips a coin. If she gets a heads, she wins. If not, it becomes Bob's turn. On Bob's turn, he flips a coin. If he gets a tails, he wins. If not, it becomes Alice's turn. What is the probability that Alice wins the game?\n",
742 |       "\n",
743 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
744 |       "\n",
745 |       "\n",
746 |       "\u001b[31mAssistant message:\u001b[0m\n",
747 |       "\n",
748 |       "[[ ## reasoning ## ]]\n",
749 |       "Alice has a $1/2$ chance of winning the game on her first turn. If she doesn't, then the probability that she wins the game on her second turn is $1/8,$ since she must not win on her first flip ($1/2$ chance), Bob must not win on his first flip ($1/2$ chance), and then Alice must win on her second flip ($1/2$ chance). The probability that she wins the game on her third turn is $1/32,$ and in general, the probability that she wins the game on her $k^\\text{th}$ turn is $(1/2)^{2k-1}.$ Thus, the probability that Alice wins is an infinite geometric series with first term $1/2$ and common ratio $1/4.$ So, the probability that Alice wins the game is $$\\frac{\\frac12}{1-\\frac14} = \\boxed{\\frac{2}{3}}.$$OR\n",
750 |       "\n",
751 |       "Note that the only difference between the odds of Alice or Bob winning is who goes first. Because Bob goes second, the odds of him winning on his $k^\\text{th}$ flip is half of the odds that Alice wins on her $k^\\text{th}$ flip, since Alice must first get a tails before Bob gets a chance to win. Thus, if $a$ is Alice's chance of winning, and $b$ is Bob's chance of winning, then $a = 2b.$ Also, since someone must win, $a + b = 1.$ It follows that $a = 2/3$ and $b = 1/3,$ so Alice has a $\\boxed{\\frac{2}{3}}$ chance of winning the game.\n",
752 |       "\n",
753 |       "[[ ## answer ## ]]\n",
754 |       "\\frac{2}{3}\n",
755 |       "\n",
756 |       "[[ ## completed ## ]]\n",
757 |       "\n",
758 |       "\n",
759 |       "\u001b[31mUser message:\u001b[0m\n",
760 |       "\n",
761 |       "[[ ## question ## ]]\n",
762 |       "Find the smallest possible value of $$\\frac{(y-x)^2}{(y-z)(z-x)} + \\frac{(z-y)^2}{(z-x)(x-y)} + \\frac{(x-z)^2}{(x-y)(y-z)},$$ where $x,y,$ and $z$ are distinct real numbers.\n",
763 |       "\n",
764 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
765 |       "\n",
766 |       "\n",
767 |       "\u001b[31mResponse:\u001b[0m\n",
768 |       "\n",
769 |       "\u001b[32m[[ ## reasoning ## ]]\n",
770 |       "Let \\( S = \\frac{(y-x)^2}{(y-z)(z-x)} + \\frac{(z-y)^2}{(z-x)(x-y)} + \\frac{(x-z)^2}{(x-y)(y-z)} \\).\n",
771 |       "\n",
772 |       "To find the minimum value of \\( S \\), we can use the method of symmetry and the Cauchy-Schwarz inequality. \n",
773 |       "\n",
774 |       "By applying the Cauchy-Schwarz inequality, we have:\n",
775 |       "\\[\n",
776 |       "\\left( \\frac{(y-x)^2}{(y-z)(z-x)} + \\frac{(z-y)^2}{(z-x)(x-y)} + \\frac{(x-z)^2}{(x-y)(y-z)} \\right) \\left( (y-z)(z-x) + (z-x)(x-y) + (x-y)(y-z) \\right) \\geq \\left( (y-x) + (z-y) + (x-z) \\right)^2.\n",
777 |       "\\]\n",
778 |       "The right-hand side simplifies to:\n",
779 |       "\\[\n",
780 |       "(y-x) + (z-y) + (x-z) = 0,\n",
781 |       "\\]\n",
782 |       "which means the right-hand side is \\( 0 \\). Therefore, we have:\n",
783 |       "\\[\n",
784 |       "S \\cdot \\left( (y-z)(z-x) + (z-x)(x-y) + (x-y)(y-z) \\right) \\geq 0.\n",
785 |       "\\]\n",
786 |       "This implies that \\( S \\) is non-negative.\n",
787 |       "\n",
788 |       "Next, we can test specific values for \\( x, y, z \\) to find the minimum. Let's set \\( x = 0, y = 1, z = 2 \\):\n",
789 |       "\\[\n",
790 |       "S = \\frac{(1-0)^2}{(1-2)(2-0)} + \\frac{(2-1)^2}{(2-0)(0-1)} + \\frac{(0-2)^2}{(0-1)(1-2)}.\n",
791 |       "\\]\n",
792 |       "Calculating each term:\n",
793 |       "1. The first term:\n",
794 |       "   \\[\n",
795 |       "   \\frac{1^2}{(-1)(2)} = \\frac{1}{-2} = -\\frac{1}{2}.\n",
796 |       "   \\]\n",
797 |       "2. The second term:\n",
798 |       "   \\[\n",
799 |       "   \\frac{1^2}{(2)(-1)} = \\frac{1}{-2} = -\\frac{1}{2}.\n",
800 |       "   \\]\n",
801 |       "3. The third term:\n",
802 |       "   \\[\n",
803 |       "   \\frac{(-2)^2}{(-1)(-1)} = \\frac{4}{1} = 4.\n",
804 |       "   \\]\n",
805 |       "\n",
806 |       "Thus,\n",
807 |       "\\[\n",
808 |       "S = -\\frac{1}{2} - \\frac{1}{2} + 4 = 3.\n",
809 |       "\\]\n",
810 |       "\n",
811 |       "To check if this is indeed the minimum, we can also consider the case when \\( x, y, z \\) are the roots of a quadratic polynomial. By symmetry and the nature of the expression, we can conclude that the minimum value of \\( S \\) is indeed \\( 3 \\).\n",
812 |       "\n",
813 |       "Therefore, the smallest possible value of \\( S \\) is \\( \\boxed{3} \\).\n",
814 |       "\n",
815 |       "[[ ## answer ## ]]\n",
816 |       "3\n",
817 |       "\n",
818 |       "[[ ## completed ## ]]\u001b[0m\n",
819 |       "\n",
820 |       "\n",
821 |       "\n",
822 |       "\n",
823 |       "\n"
824 |      ]
825 |     }
826 |    ],
827 |    "source": [
828 |     "dspy.inspect_history()"
829 |    ]
830 |   },
831 |   {
832 |    "cell_type": "markdown",
833 |    "metadata": {},
834 |    "source": []
835 |   }
836 |  ],
837 |  "metadata": {
838 |   "kernelspec": {
839 |    "display_name": "py310_sept24_user",
840 |    "language": "python",
841 |    "name": "python3"
842 |   },
843 |   "language_info": {
844 |    "codemirror_mode": {
845 |     "name": "ipython",
846 |     "version": 3
847 |    },
848 |    "file_extension": ".py",
849 |    "mimetype": "text/x-python",
850 |    "name": "python",
851 |    "nbconvert_exporter": "python",
852 |    "pygments_lexer": "ipython3",
853 |    "version": "3.10.14"
854 |   }
855 |  },
856 |  "nbformat": 4,
857 |  "nbformat_minor": 2
858 | }
```

tutorials/observability/index.md
```
1 | # Tutorial: Debugging and Observability in DSPy
2 | 
3 | This guide demonstrates how to debug problems and improve observability in DSPy. Modern AI programs often involve multiple components, such as language models, retrievers, and tools. DSPy allows you to build and optimize such complex AI systems in a clean and modular way.
4 | 
5 | However, as systems grow more sophisticated, the ability to **understand what your system is doing** becomes critical. Without transparency, the prediction process can easily become a black box, making failures or quality issues difficult to diagnose and production maintenance challenging.
6 | 
7 | By the end of this tutorial, you'll understand how to debug an issue and improve observability using [MLflow Tracing](#tracing). You'll also explore how to build a custom logging solution using callbacks.
8 | 
9 | 
10 | 
11 | ## Define a Program
12 | 
13 | We'll start by creating a simple ReAct agent that uses ColBERTv2's Wikipedia dataset as a retrieval source. You can replace this with a more sophisticated program.
14 | 
15 | ```python
16 | import dspy
17 | import os
18 | 
19 | os.environ["OPENAI_API_KEY"] = "{your_openai_api_key}"
20 | 
21 | lm = dspy.LM("openai/gpt-4o-mini")
22 | colbert = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")
23 | dspy.configure(lm=lm)
24 | 
25 | 
26 | def retrieve(query: str):
27 |     """Retrieve top 3 relevant information from ColBert"""
28 |     results = colbert(query, k=3)
29 |     return [x["text"] for x in results]
30 | 
31 | 
32 | agent = dspy.ReAct("question -> answer", tools=[retrieve], max_iters=3)
33 | ```
34 | 
35 | Now, let's ask the agent a simple question:
36 | 
37 | ```python
38 | prediction = agent(question="Which baseball team does Shohei Ohtani play for in June 2025?")
39 | print(prediction.answer)
40 | ```
41 | 
42 | ```
43 | Shohei Ohtani is expected to play for the Hokkaido Nippon-Ham Fighters in June 2025, based on the available information.
44 | ```
45 | 
46 | Oh, this is incorrect. He no longer plays for the Hokkaido Nippon-Ham Fighters; he moved to the Dodgers and won the World Series in 2024! Let's debug the program and explore potential fixes.
47 | 
48 | ## Using ``inspect_history``
49 | 
50 | DSPy provides the `inspect_history()` utility, which prints out all LLM invocations made so far:
51 | 
52 | ```python
53 | # Print out 5 LLM calls
54 | dspy.inspect_history(n=5)
55 | ```
56 | 
57 | ```
58 | [2024-12-01T10:23:29.144257]
59 | 
60 | System message:
61 | 
62 | Your input fields are:
63 | 1. `question` (str)
64 | 
65 | ...
66 | 
67 | Response:
68 | 
69 | Response:
70 | 
71 | [[ ## reasoning ## ]]
72 | The search for information regarding Shohei Ohtani's team in June 2025 did not yield any specific results. The retrieved data consistently mentioned that he plays for the Hokkaido Nippon-Ham Fighters, but there was no indication of any changes or updates regarding his team for the specified date. Given the lack of information, it is reasonable to conclude that he may still be with the Hokkaido Nippon-Ham Fighters unless there are future developments that are not captured in the current data.
73 | 
74 | [[ ## answer ## ]]
75 | Shohei Ohtani is expected to play for the Hokkaido Nippon-Ham Fighters in June 2025, based on the available information.
76 | 
77 | [[ ## completed ## ]]
78 | 
79 | ```
80 | The log reveals that the agent could not retrieve helpful information from the search tool. However, what exactly did the retriever return? While useful, `inspect_history` has some limitations:
81 | 
82 | * In real-world systems, other components like retrievers, tools, and custom modules play significant roles, but `inspect_history` only logs LLM calls.
83 | * DSPy programs often make multiple LLM calls within a single prediction. Monolith log history makes it hard to organize logs, especially when handling multiple questions.
84 | * Metadata such as parameters, latency, and the relationship between modules are not captured.
85 | 
86 | **Tracing** addresses these limitations and provides a more comprehensive solution.
87 | 
88 | ## Tracing
89 | 
90 | [MLflow](https://mlflow.org/docs/latest/llms/tracing/index.html) is an end-to-end machine learning platform that is integrated seamlessly with DSPy to support best practices in LLMOps. Using MLflow's automatic tracing capability with DSPy is straightforward; **No sign up for services or an API key is required**. You just need to install MLflow and call `mlflow.dspy.autolog()` in your notebook or script.
91 | 
92 | ```bash
93 | pip install -U mlflow>=2.18.0
94 | ```
95 | 
96 | After installation, spin up your server via the command below.
97 | 
98 | ```
99 | # It is highly recommended to use SQL store when using MLflow tracing
100 | mlflow server --backend-store-uri sqlite:///mydb.sqlite
101 | ```
102 | 
103 | If you don't specify a different port via `--port` flag, you MLflow server will be hosted at port 5000.
104 | 
105 | Now let's change our code snippet to enable MLflow tracing. We need to:
106 | 
107 | - Tell MLflow where the server is hosted.
108 | - Apply `mlflow.autolog()` so that DSPy tracing is automatically captured.
109 | 
110 | The full code is as below, now let's run it again!
111 | 
112 | ```python
113 | import dspy
114 | import os
115 | import mlflow
116 | 
117 | os.environ["OPENAI_API_KEY"] = "{your_openai_api_key}"
118 | 
119 | # Tell MLflow about the server URI.
120 | mlflow.set_tracking_uri("http://127.0.0.1:5000")
121 | # Create a unique name for your experiment.
122 | mlflow.set_experiment("DSPy")
123 | 
124 | lm = dspy.LM("openai/gpt-4o-mini")
125 | colbert = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")
126 | dspy.configure(lm=lm)
127 | 
128 | 
129 | def retrieve(query: str):
130 |     """Retrieve top 3 relevant information from ColBert"""
131 |     results = colbert(query, k=3)
132 |     return [x["text"] for x in results]
133 | 
134 | 
135 | agent = dspy.ReAct("question -> answer", tools=[retrieve], max_iters=3)
136 | print(agent(question="Which baseball team does Shohei Ohtani play for?"))
137 | ```
138 | 
139 | 
140 | MLflow automatically generates a **trace** for each prediction and records it within your experiment. To explore these traces visually, open `http://127.0.0.1:5000/`
141 | in your browser, then select the experiment you just created and navigate to the Traces tab:
142 | 
143 | ![MLflow Trace UI](./mlflow_trace_ui.png)
144 | 
145 | Click on the most recent trace to view its detailed breakdown:
146 | 
147 | ![MLflow Trace View](./mlflow_trace_view.png)
148 | 
149 | Here, you can examine the input and output of every step in your workflow. For example, the screenshot above shows the `retrieve` function's input and output. By inspecting the retriever's output, you can see that it returned outdated information, which is not sufficient to determine which team Shohei Ohtani plays for in June 2025. You can also inspect
150 | other steps, e.g, language model's input, output, and configuration.
151 | 
152 | To address the issue of outdated information, you can replace the `retrieve` function with a web search tool powered by [Tavily search](https://www.tavily.com/).
153 | 
154 | ```python
155 | from tavily import TavilyClient
156 | import dspy
157 | import mlflow
158 | 
159 | # Tell MLflow about the server URI.
160 | mlflow.set_tracking_uri("http://127.0.0.1:5000")
161 | # Create a unique name for your experiment.
162 | mlflow.set_experiment("DSPy")
163 | 
164 | search_client = TavilyClient(api_key="<YOUR_TAVILY_API_KEY>")
165 | 
166 | def web_search(query: str) -> list[str]:
167 |     """Run a web search and return the content from the top 5 search results"""
168 |     response = search_client.search(query)
169 |     return [r["content"] for r in response["results"]]
170 | 
171 | agent = dspy.ReAct("question -> answer", tools=[web_search])
172 | 
173 | prediction = agent(question="Which baseball team does Shohei Ohtani play for?")
174 | print(agent.answer)
175 | ```
176 | 
177 | ```
178 | Los Angeles Dodgers
179 | ```
180 | 
181 | Below is a GIF demonstrating how to navigate through the MLflow UI:
182 | 
183 | ![MLflow Trace UI Navigation](./mlflow_trace_ui_navigation.gif)
184 | 
185 | 
186 | For a complete guide on how to use MLflow tracing, please refer to
187 | the [MLflow Tracing Guide](https://mlflow.org/docs/3.0.0rc0/tracing).
188 | 
189 | 
190 | 
191 | !!! info Learn more about MLflow
192 | 
193 |     MLflow is an end-to-end LLMOps platform that offers extensive features like experiment tracking, evaluation, and deployment. To learn more about DSPy and MLflow integration, visit [this tutorial](../deployment/index.md#deploying-with-mlflow).
194 | 
195 | 
196 | ## Building a Custom Logging Solution
197 | 
198 | Sometimes, you may want to implement a custom logging solution. For instance, you might need to log specific events triggered by a particular module. DSPy's **callback** mechanism supports such use cases. The ``BaseCallback`` class provides several handlers for customizing logging behavior:
199 | 
200 | |Handlers|Description|
201 | |:--|:--|
202 | |`on_module_start` / `on_module_end` | Triggered when a `dspy.Module` subclass is invoked. |
203 | |`on_lm_start` / `on_lm_end` | Triggered when a `dspy.LM` subclass is invoked. |
204 | |`on_adapter_format_start` / `on_adapter_format_end`| Triggered when a `dspy.Adapter` subclass formats the input prompt. |
205 | |`on_adapter_parse_start` / `on_adapter_parse_end`| Triggered when a `dspy.Adapter` subclass postprocess the output text from an LM. |
206 | |`on_tool_start` / `on_tool_end` | Triggered when a `dspy.Tool` subclass is invoked. |
207 | |`on_evaluate_start` / `on_evaluate_end` | Triggered when a `dspy.Evaluate` instance is invoked. |
208 | 
209 | Here's an example of custom callback that logs the intermediate steps of a ReAct agent:
210 | 
211 | ```python
212 | import dspy
213 | from dspy.utils.callback import BaseCallback
214 | 
215 | # 1. Define a custom callback class that extends BaseCallback class
216 | class AgentLoggingCallback(BaseCallback):
217 | 
218 |     # 2. Implement on_module_end handler to run a custom logging code.
219 |     def on_module_end(self, call_id, outputs, exception):
220 |         step = "Reasoning" if self._is_reasoning_output(outputs) else "Acting"
221 |         print(f"== {step} Step ===")
222 |         for k, v in outputs.items():
223 |             print(f"  {k}: {v}")
224 |         print("\n")
225 | 
226 |     def _is_reasoning_output(self, outputs):
227 |         return any(k.startswith("Thought") for k in outputs.keys())
228 | 
229 | # 3. Set the callback to DSPy setting so it will be applied to program execution
230 | dspy.configure(callbacks=[AgentLoggingCallback()])
231 | ```
232 | 
233 | 
234 | ```
235 | == Reasoning Step ===
236 |   Thought_1: I need to find the current team that Shohei Ohtani plays for in Major League Baseball.
237 |   Action_1: Search[Shohei Ohtani current team 2023]
238 | 
239 | == Acting Step ===
240 |   passages: ["Shohei Ohtani ..."]
241 | 
242 | ...
243 | ```
244 | 
245 | !!! info Handling Inputs and Outputs in Callbacks
246 | 
247 |     Be cautious when working with input or output data in callbacks. Mutating them in-place can modify the original data passed to the program, potentially leading to unexpected behavior. To avoid this, it's strongly recommended to create a copy of the data before performing any operations that may alter it.
```

tutorials/optimize_ai_program/index.md
```
1 | # Optimize AI Programs with DSPy
2 | 
3 | This section focuses on DSPy's powerful optimization capabilities, demonstrating how to systematically improve your AI programs using various optimizers. These tutorials are lighter on programming concepts and instead showcase how DSPy optimizers can automatically enhance the quality and performance of your applications.
4 | 
5 | ## Mathematical and Reasoning Tasks
6 | 
7 | ### [Math Reasoning](../math/index.ipynb)
8 | Learn how to optimize DSPy programs for mathematical reasoning tasks. This tutorial demonstrates how optimizers can dramatically improve performance on complex math problems by finding better prompting strategies and few-shot examples.
9 | 
10 | ## Model Optimization
11 | 
12 | ### [Classification Finetuning](../classification_finetuning/index.ipynb)
13 | Discover how to use DSPy's finetuning optimizers to distill knowledge from large language models into smaller, more efficient models. Learn the complete workflow from prompt optimization to model finetuning for classification tasks.
14 | 
15 | ## Advanced Tool Integration
16 | 
17 | ### [Advanced Tool Use](../tool_use/index.ipynb)
18 | Explore how to optimize AI programs that use external tools and APIs. This tutorial shows how DSPy optimizers can learn to use tools more effectively, improving both accuracy and efficiency in tool-calling scenarios.
19 | 
20 | ### [Finetuning Agents](../games/index.ipynb)
21 | Learn to optimize complex agent-based systems through finetuning. This tutorial demonstrates how to improve agent performance in interactive environments like games, where strategic thinking and adaptation are crucial.
```

tutorials/papillon/index.md
```
1 | Please refer to [this tutorial from the PAPILLON authors](https://colab.research.google.com/github/Columbia-NLP-Lab/PAPILLON/blob/main/papillon_tutorial.ipynb) using DSPy.
2 | 
3 | This tutorial demonstrates a few aspects of using DSPy in a more advanced context:
4 | 
5 | 1. It builds a multi-stage `dspy.Module` that involves a small local LM using an external tool.
6 | 2. It builds a multi-stage _judge_ in DSPy, and uses it as a metric for evaluation.
7 | 3. It uses this judge for optimizing the `dspy.Module`, using a large model as a teacher for a small local LM.
```

tutorials/optimizer_tracking/index.md
```
1 | # Tracking DSPy Optimizers with MLflow
2 | 
3 | This tutorial demonstrates how to use MLflow to track and analyze your DSPy optimization process. MLflow's built-in integration for DSPy provides traceability and debuggability for your DSPy optimization experience. It allows you to understand the intermediate trials during the optimization, store the optimized program and its results, and provides observability into your program execution.
4 | 
5 | Through the autologging capability, MLflow tracks the following information:
6 | 
7 | * **Optimizer Parameters**
8 |     * Number of few-shot examples
9 |     * Number of candidates
10 |     * Other configuration settings
11 | 
12 | * **Program States**
13 |     * Initial instructions and few-shot examples
14 |     * Optimized instructions and few-shot examples
15 |     * Intermediate instructions and few-shot examples during optimization
16 | 
17 | * **Datasets**
18 |     * Training data used
19 |     * Evaluation data used
20 | 
21 | * **Performance Progression**
22 |     * Overall metric progression
23 |     * Performance at each evaluation step
24 | 
25 | * **Traces**
26 |     * Program execution traces
27 |     * Model responses
28 |     * Intermediate prompts
29 | 
30 | ## Getting Started
31 | 
32 | ### 1. Install MLflow
33 | First, install MLflow (version 2.21.1 or later):
34 | 
35 | ```bash
36 | pip install mlflow>=2.21.1
37 | ```
38 | 
39 | ### 2. Start MLflow Tracking Server
40 | 
41 | Let's spin up the MLflow tracking server with the following command. This will start a local server at `http://127.0.0.1:5000/`:
42 | 
43 | ```bash
44 | # It is highly recommended to use SQL store when using MLflow tracing
45 | mlflow server --backend-store-uri sqlite:///mydb.sqlite
46 | ```
47 | 
48 | ### 3. Enable Autologging
49 | 
50 | Configure MLflow to track your DSPy optimization:
51 | 
52 | ```python
53 | import mlflow
54 | import dspy
55 | 
56 | # Enable autologging with all features
57 | mlflow.dspy.autolog(
58 |     log_compiles=True,    # Track optimization process
59 |     log_evals=True,       # Track evaluation results
60 |     log_traces_from_compile=True  # Track program traces during optimization
61 | )
62 | 
63 | # Configure MLflow tracking
64 | mlflow.set_tracking_uri("http://localhost:5000")  # Use local MLflow server
65 | mlflow.set_experiment("DSPy-Optimization")
66 | ```
67 | 
68 | ### 4. Optimizing Your Program
69 | 
70 | Here's a complete example showing how to track the optimization of a math problem solver:
71 | 
72 | ```python
73 | import dspy
74 | from dspy.datasets.gsm8k import GSM8K, gsm8k_metric
75 | 
76 | # Configure your language model
77 | lm = dspy.LM(model="openai/gpt-4o")
78 | dspy.configure(lm=lm)
79 | 
80 | # Load dataset
81 | gsm8k = GSM8K()
82 | trainset, devset = gsm8k.train, gsm8k.dev
83 | 
84 | # Define your program
85 | program = dspy.ChainOfThought("question -> answer")
86 | 
87 | # Create and run optimizer with tracking
88 | teleprompter = dspy.teleprompt.MIPROv2(
89 |     metric=gsm8k_metric,
90 |     auto="light",
91 | )
92 | 
93 | # The optimization process will be automatically tracked
94 | optimized_program = teleprompter.compile(
95 |     program,
96 |     trainset=trainset,
97 | )
98 | ```
99 | 
100 | ### 5. Viewing Results
101 | 
102 | Once your optimization is complete, you can analyze the results through MLflow's UI. Let's walk through how to explore your optimization runs.
103 | 
104 | #### Step 1: Access the MLflow UI
105 | Navigate to `http://localhost:5000` in your web browser to access the MLflow tracking server UI.
106 | 
107 | #### Step 2: Understanding the Experiment Structure
108 | When you open the experiment page, you'll see a hierarchical view of your optimization process. The parent run represents your overall optimization process, while the child runs show each intermediate version of your program that was created during optimization.
109 | 
110 | ![Experiments](./experiment.png)
111 | 
112 | #### Step 3: Analyzing the Parent Run
113 | Clicking on the parent run reveals the big picture of your optimization process. You'll find detailed information about your optimizer's configuration parameters and how your evaluation metrics progressed over time. The parent run also stores your final optimized program, including the instructions, signature definitions, and few-shot examples that were used. Additionally, you can review the training data that was used during the optimization process.
114 | 
115 | ![Parent Run](./parent_run.png)
116 | 
117 | #### Step 4: Examining Child Runs
118 | Each child run provides a detailed snapshot of a specific optimization attempt. When you select a child run from the experiment page, you can explore several aspects of that particular intermediate program.
119 | On the run parameter tab or artifact tab, you can review the instructions and few-shot examples used for the intermediate program.
120 | One of the most powerful features is the Traces tab, which provides a step-by-step view of your program's execution. Here you can understand exactly how your DSPy program processes inputs and generates outputs.
121 | 
122 | ![Child Run](./child_run.png)
123 | 
124 | ### 6. Loading Models for Inference
125 | You can load the optimized program directly from the MLflow tracking server for inference:
126 | 
127 | ```python
128 | model_path = mlflow.artifacts.download_artifacts("mlflow-artifacts:/path/to/best_model.json")
129 | program.load(model_path)
130 | ```
131 | 
132 | ## Troubleshooting
133 | 
134 | - If traces aren't appearing, ensure `log_traces_from_compile=True`
135 | - For large datasets, consider setting `log_traces_from_compile=False` to avoid memory issues
136 | - Use `mlflow.get_run(run_id)` to programmatically access MLflow run data
137 | 
138 | For more features, explore the [MLflow Documentation](https://mlflow.org/docs/latest/llms/dspy).
```

tutorials/output_refinement/best-of-n-and-refine.md
```
1 | # Output Refinement: BestOfN and Refine
2 | 
3 | Both `BestOfN` and `Refine` are DSPy modules designed to improve the reliability and quality of predictions by making multiple `LM` calls with different rollout IDs to bypass caching. Both modules stop when they have reached `N` attempts or when the `reward_fn` returns an award above the `threshold`.
4 | 
5 | ## BestOfN
6 | 
7 | `BestOfN` is a module that runs the provided module multiple times (up to `N`) with different rollout IDs. It returns either the first prediction that passes a specified threshold or the one with the highest reward if none meets the threshold.
8 | 
9 | ### Basic Usage
10 | 
11 | Lets say we wanted to have the best chance of getting a one word answer from the model. We could use `BestOfN` to try multiple rollout IDs and return the best result.
12 | 
13 | ```python
14 | import dspy
15 | 
16 | def one_word_answer(args, pred: dspy.Prediction) -> float:
17 |     return 1.0 if len(pred.answer.split()) == 1 else 0.0
18 | 
19 | best_of_3 = dspy.BestOfN(
20 |     module=dspy.ChainOfThought("question -> answer"), 
21 |     N=3, 
22 |     reward_fn=one_word_answer, 
23 |     threshold=1.0
24 | )
25 | 
26 | result = best_of_3(question="What is the capital of Belgium?")
27 | print(result.answer)  # Brussels
28 | ```
29 | 
30 | ### Error Handling
31 | 
32 | By default, if the module encounters an error during an attempt, it will continue trying until it reaches `N` attempts. You can adjust this behavior with the `fail_count` parameter:
33 | 
34 | ```python
35 | best_of_3 = dspy.BestOfN(
36 |     module=qa, 
37 |     N=3, 
38 |     reward_fn=one_word_answer, 
39 |     threshold=1.0,
40 |     fail_count=1
41 | )
42 | 
43 | best_of_3(question="What is the capital of Belgium?")
44 | # raises an error after the first failure
45 | ```
46 | 
47 | ## Refine
48 | 
49 | `Refine` extends the functionality of `BestOfN` by adding an automatic feedback loop. After each unsuccessful attempt (except the final one), it automatically generates detailed feedback about the module's performance and uses this feedback as hints for subsequent runs.
50 | 
51 | ### Basic Usage
52 | 
53 | ```python
54 | import dspy
55 | 
56 | def one_word_answer(args, pred: dspy.Prediction) -> float:
57 |     return 1.0 if len(pred.answer.split()) == 1 else 0.0
58 | 
59 | refine = dspy.Refine(
60 |     module=dspy.ChainOfThought("question -> answer"), 
61 |     N=3, 
62 |     reward_fn=one_word_answer, 
63 |     threshold=1.0
64 | )
65 | 
66 | result = refine(question="What is the capital of Belgium?")
67 | print(result.answer)  # Brussels
68 | ```
69 | 
70 | ### Error Handling
71 | 
72 | Like `BestOfN`, `Refine` will try up to `N` times by default, even if errors occur. You can control this with the `fail_count` parameter:
73 | 
74 | ```python
75 | # Stop after the first error
76 | refine = dspy.Refine(
77 |     module=qa, 
78 |     N=3, 
79 |     reward_fn=one_word_answer, 
80 |     threshold=1.0,
81 |     fail_count=1
82 | )
83 | ```
84 | 
85 | ## Comparison: BestOfN vs. Refine
86 | 
87 | Both modules serve similar purposes but differ in their approach:
88 | 
89 | - `BestOfN` simply tries different rollout IDs and selects the best resulting prediction as defined by the `reward_fn`.
90 | - `Refine` adds an feedback loop, using the lm to generate a detailed feedback about the module's own performance using the previous prediction and the code in the `reward_fn`. This feedback is then used as hints for subsequent runs.
91 | 
92 | ## Practical Examples
93 | 
94 | ### Ensuring Factual Correctness
95 | 
96 | ```python
97 | import dspy
98 | 
99 | class FactualityJudge(dspy.Signature):
100 |     """Determine if a statement is factually accurate."""
101 |     statement: str = dspy.InputField()
102 |     is_factual: bool = dspy.OutputField()
103 | 
104 | factuality_judge = dspy.ChainOfThought(FactualityJudge)
105 | 
106 | def factuality_reward(args, pred: dspy.Prediction) -> float:
107 |     statement = pred.answer    
108 |     result = factuality_judge(statement)    
109 |     return 1.0 if result.is_factual else 0.0
110 | 
111 | refined_qa = dspy.Refine(
112 |     module=dspy.ChainOfThought("question -> answer"),
113 |     N=3,
114 |     reward_fn=factuality_reward,
115 |     threshold=1.0
116 | )
117 | 
118 | result = refined_qa(question="Tell me about Belgium's capital city.")
119 | print(result.answer)
120 | ```
121 | 
122 | ### Summarization - Controlling Response Length
123 | 
124 | ```python
125 | import dspy
126 | 
127 | def ideal_length_reward(args, pred: dspy.Prediction) -> float:
128 |     """
129 |     Reward the summary for being close to 75 words with a tapering off for longer summaries.
130 |     """
131 |     word_count = len(pred.summary.split())
132 |     distance = abs(word_count - 75)
133 |     return max(0.0, 1.0 - (distance / 125))
134 | 
135 | optimized_summarizer = dspy.BestOfN(
136 |     module=dspy.ChainOfThought("text -> summary"),
137 |     N=50,
138 |     reward_fn=ideal_length_reward,
139 |     threshold=0.9
140 | )
141 | 
142 | result = optimized_summarizer(
143 |     text="[Long text to summarize...]"
144 | )
145 | print(result.summary)
146 | ```
147 | 
148 | ## Migration from `dspy.Suggest` and `dspy.Assert`
149 | 
150 | `BestOfN` and `Refine` are the replacements for `dspy.Suggest` and `dspy.Assert` as of DSPy 2.6.
```

tutorials/program_of_thought/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: ProgramOfThought\n",
8 |     "\n",
9 |     "`dspy.ProgramOfThought` automatically generates and refines Python code to solve downstream tasks.\n",
10 |     "\n",
11 |     "Install the latest DSPy via `pip install -U dspy` and follow along."
12 |    ]
13 |   },
14 |   {
15 |    "cell_type": "markdown",
16 |    "metadata": {},
17 |    "source": [
18 |     "## 1) Using PythonInterpreter\n",
19 |     "\n",
20 |     "`ProgramOfThought` integrates an adapted Python interpreter to execute code generated by LMs. \n",
21 |     "\n",
22 |     "As a brief example to demonstrate how the interpreter works, we'll create an instance of `dspy.PythonInterpreter` and demonstrate the underlying execution of `ProgramOfThought`."
23 |    ]
24 |   },
25 |   {
26 |    "cell_type": "code",
27 |    "execution_count": 17,
28 |    "metadata": {},
29 |    "outputs": [
30 |     {
31 |      "data": {
32 |       "text/plain": [
33 |        "14"
34 |       ]
35 |      },
36 |      "execution_count": 17,
37 |      "metadata": {},
38 |      "output_type": "execute_result"
39 |     }
40 |    ],
41 |    "source": [
42 |     "import dspy\n",
43 |     "interpreter = dspy.PythonInterpreter()\n",
44 |     "expr = \"value = 2*5 + 4\\nvalue\"\n",
45 |     "answer = interpreter.execute(expr)\n",
46 |     "answer"
47 |    ]
48 |   },
49 |   {
50 |    "cell_type": "markdown",
51 |    "metadata": {},
52 |    "source": [
53 |     "## 2) Demonstrating ProgramOfThought\n",
54 |     " As an example, we'll define a signature with an input question and an output answer. Then, we'll create and invoke the `ProgramOfThought` program, which uses an LM to first generate code to represent the question, executes the code using the interpreter and outputs the final result as the answer to the question."
55 |    ]
56 |   },
57 |   {
58 |    "cell_type": "markdown",
59 |    "metadata": {},
60 |    "source": [
61 |     "Let's use Meta's `Llama-3-70b-Instruct`. You can easily swap this out for [other providers or local models](https://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb)."
62 |    ]
63 |   },
64 |   {
65 |    "cell_type": "code",
66 |    "execution_count": 3,
67 |    "metadata": {},
68 |    "outputs": [],
69 |    "source": [
70 |     "llama31_70b = dspy.LM(\"openai/meta-llama/Meta-Llama-3-70b-Instruct\", api_base=\"API_BASE\", api_key=\"None\")\n",
71 |     "\n",
72 |     "dspy.settings.configure(lm=llama31_70b)"
73 |    ]
74 |   },
75 |   {
76 |    "cell_type": "markdown",
77 |    "metadata": {},
78 |    "source": [
79 |     "Let's now define our module with a brief signature that specifies the input question and output answer. We can then call `ProgramOfThought` on the signature and pass in our sample problem."
80 |    ]
81 |   },
82 |   {
83 |    "cell_type": "code",
84 |    "execution_count": 4,
85 |    "metadata": {},
86 |    "outputs": [
87 |     {
88 |      "data": {
89 |       "text/plain": [
90 |        "'14'"
91 |       ]
92 |      },
93 |      "execution_count": 4,
94 |      "metadata": {},
95 |      "output_type": "execute_result"
96 |     }
97 |    ],
98 |    "source": [
99 |     "class BasicGenerateAnswer(dspy.Signature):\n",
100 |     "    question = dspy.InputField()\n",
101 |     "    answer = dspy.OutputField()\n",
102 |     "\n",
103 |     "pot = dspy.ProgramOfThought(BasicGenerateAnswer)\n",
104 |     "problem = \"2*5 + 4\"\n",
105 |     "pot(question=problem).answer"
106 |    ]
107 |   },
108 |   {
109 |    "cell_type": "markdown",
110 |    "metadata": {},
111 |    "source": [
112 |     "Great! The module successfully produced the same correct answer. Let's see how exactly it used the LM to do so:"
113 |    ]
114 |   },
115 |   {
116 |    "cell_type": "code",
117 |    "execution_count": 5,
118 |    "metadata": {},
119 |    "outputs": [
120 |     {
121 |      "name": "stdout",
122 |      "output_type": "stream",
123 |      "text": [
124 |       "\n",
125 |       "\n",
126 |       "\n",
127 |       "\n",
128 |       "\u001b[34m[2025-01-06T21:58:40.879405]\u001b[0m\n",
129 |       "\n",
130 |       "\u001b[31mSystem message:\u001b[0m\n",
131 |       "\n",
132 |       "Your input fields are:\n",
133 |       "1. `question` (str)\n",
134 |       "2. `final_generated_code` (str): python code that answers the question\n",
135 |       "3. `code_output` (str): output of previously-generated python code\n",
136 |       "\n",
137 |       "Your output fields are:\n",
138 |       "1. `reasoning` (str)\n",
139 |       "2. `answer` (str)\n",
140 |       "\n",
141 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
142 |       "\n",
143 |       "[[ ## question ## ]]\n",
144 |       "{question}\n",
145 |       "\n",
146 |       "[[ ## final_generated_code ## ]]\n",
147 |       "{final_generated_code}\n",
148 |       "\n",
149 |       "[[ ## code_output ## ]]\n",
150 |       "{code_output}\n",
151 |       "\n",
152 |       "[[ ## reasoning ## ]]\n",
153 |       "{reasoning}\n",
154 |       "\n",
155 |       "[[ ## answer ## ]]\n",
156 |       "{answer}\n",
157 |       "\n",
158 |       "[[ ## completed ## ]]\n",
159 |       "\n",
160 |       "In adhering to this structure, your objective is: \n",
161 |       "        Given the final code `question`, `final_generated_code`, `code_output`, provide the final `answer`.\n",
162 |       "\n",
163 |       "\n",
164 |       "\u001b[31mUser message:\u001b[0m\n",
165 |       "\n",
166 |       "[[ ## question ## ]]\n",
167 |       "2*5 + 4\n",
168 |       "\n",
169 |       "[[ ## final_generated_code ## ]]\n",
170 |       "def calculate_expression():\n",
171 |       "    # Multiply 2 and 5\n",
172 |       "    multiplication_result = 2 * 5\n",
173 |       "    \n",
174 |       "    # Add 4 to the result\n",
175 |       "    final_result = multiplication_result + 4\n",
176 |       "    \n",
177 |       "    return final_result\n",
178 |       "\n",
179 |       "# Execute the function to get the final answer\n",
180 |       "answer = calculate_expression()\n",
181 |       "print(answer)\n",
182 |       "\n",
183 |       "[[ ## code_output ## ]]\n",
184 |       "14\n",
185 |       "\n",
186 |       "\n",
187 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
188 |       "\n",
189 |       "\n",
190 |       "\u001b[31mResponse:\u001b[0m\n",
191 |       "\n",
192 |       "\u001b[32m[[ ## reasoning ## ]]\n",
193 |       "The given code defines a function `calculate_expression` that calculates the result of the expression 2*5 + 4. It first multiplies 2 and 5, then adds 4 to the result. The function is then executed, and the result is printed.\n",
194 |       "\n",
195 |       "[[ ## answer ## ]]\n",
196 |       "14\n",
197 |       "\n",
198 |       "[[ ## completed ## ]]\u001b[0m\n",
199 |       "\n",
200 |       "\n",
201 |       "\n",
202 |       "\n",
203 |       "\n"
204 |      ]
205 |     }
206 |    ],
207 |    "source": [
208 |     "dspy.inspect_history()"
209 |    ]
210 |   },
211 |   {
212 |    "cell_type": "markdown",
213 |    "metadata": {},
214 |    "source": [
215 |     "We see that the generated Python code defines a function for intermediate computations and returns the final answer upon execution through the `PythonInterpreter`, getting the right answer."
216 |    ]
217 |   },
218 |   {
219 |    "cell_type": "markdown",
220 |    "metadata": {},
221 |    "source": [
222 |     "## 3) Comparing with ChainOfThought\n",
223 |     "\n",
224 |     "Now we turn to a more complex problem to demonstrate how the `ProgramOfThought` module can be helpful. "
225 |    ]
226 |   },
227 |   {
228 |    "cell_type": "markdown",
229 |    "metadata": {},
230 |    "source": [
231 |     "Problem: **Compute 12! / sum of prime numbers between 1 and 30.**\n",
232 |     "\n",
233 |     "This is a fairly challenging computation. Let's see how `ChainOfThought` performs first:"
234 |    ]
235 |   },
236 |   {
237 |    "cell_type": "code",
238 |    "execution_count": 6,
239 |    "metadata": {},
240 |    "outputs": [
241 |     {
242 |      "data": {
243 |       "text/plain": [
244 |        "'3,710,009'"
245 |       ]
246 |      },
247 |      "execution_count": 6,
248 |      "metadata": {},
249 |      "output_type": "execute_result"
250 |     }
251 |    ],
252 |    "source": [
253 |     "problem = \"Compute 12! / sum of prime numbers between 1 and 30.\"\n",
254 |     "\n",
255 |     "cot = dspy.ChainOfThought(BasicGenerateAnswer)\n",
256 |     "cot(question=problem).answer"
257 |    ]
258 |   },
259 |   {
260 |    "cell_type": "code",
261 |    "execution_count": 7,
262 |    "metadata": {},
263 |    "outputs": [
264 |     {
265 |      "name": "stdout",
266 |      "output_type": "stream",
267 |      "text": [
268 |       "\n",
269 |       "\n",
270 |       "\n",
271 |       "\n",
272 |       "\u001b[34m[2025-01-06T21:59:08.539739]\u001b[0m\n",
273 |       "\n",
274 |       "\u001b[31mSystem message:\u001b[0m\n",
275 |       "\n",
276 |       "Your input fields are:\n",
277 |       "1. `question` (str)\n",
278 |       "\n",
279 |       "Your output fields are:\n",
280 |       "1. `reasoning` (str)\n",
281 |       "2. `answer` (str)\n",
282 |       "\n",
283 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
284 |       "\n",
285 |       "[[ ## question ## ]]\n",
286 |       "{question}\n",
287 |       "\n",
288 |       "[[ ## reasoning ## ]]\n",
289 |       "{reasoning}\n",
290 |       "\n",
291 |       "[[ ## answer ## ]]\n",
292 |       "{answer}\n",
293 |       "\n",
294 |       "[[ ## completed ## ]]\n",
295 |       "\n",
296 |       "In adhering to this structure, your objective is: \n",
297 |       "        Given the fields `question`, produce the fields `answer`.\n",
298 |       "\n",
299 |       "\n",
300 |       "\u001b[31mUser message:\u001b[0m\n",
301 |       "\n",
302 |       "[[ ## question ## ]]\n",
303 |       "Compute 12! / sum of prime numbers between 1 and 30.\n",
304 |       "\n",
305 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
306 |       "\n",
307 |       "\n",
308 |       "\u001b[31mResponse:\u001b[0m\n",
309 |       "\n",
310 |       "\u001b[32m[[ ## reasoning ## ]]\n",
311 |       "To solve this problem, we need to calculate 12! (12 factorial) and the sum of prime numbers between 1 and 30. \n",
312 |       "\n",
313 |       "First, let's calculate 12!. 12! = 12 * 11 * 10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 2 * 1 = 479,001,600.\n",
314 |       "\n",
315 |       "Next, let's find the prime numbers between 1 and 30. The prime numbers between 1 and 30 are 2, 3, 5, 7, 11, 13, 17, 19, 23, and 29. \n",
316 |       "\n",
317 |       "Now, let's calculate the sum of these prime numbers. sum = 2 + 3 + 5 + 7 + 11 + 13 + 17 + 19 + 23 + 29 = 129.\n",
318 |       "\n",
319 |       "Finally, let's calculate 12! / sum of prime numbers between 1 and 30. result = 479,001,600 / 129 = 3,710,009. \n",
320 |       "\n",
321 |       "[[ ## answer ## ]]\n",
322 |       "3,710,009\n",
323 |       "\n",
324 |       "[[ ## completed ## ]]\u001b[0m\n",
325 |       "\n",
326 |       "\n",
327 |       "\n",
328 |       "\n",
329 |       "\n"
330 |      ]
331 |     }
332 |    ],
333 |    "source": [
334 |     "dspy.inspect_history()"
335 |    ]
336 |   },
337 |   {
338 |    "cell_type": "markdown",
339 |    "metadata": {},
340 |    "source": [
341 |     "So `ChainOfThought` does fairly well in reasoning through the steps, getting the correct value for both 12! and the sum of only prime numbers between 1-30. \n",
342 |     "\n",
343 |     "But it fails at the last step of division, incorrectly computing `479,001,600 / 129 = 3,710,009` when the correct answer is `3713190.69767` (verified by a real calculator!)"
344 |    ]
345 |   },
346 |   {
347 |    "cell_type": "markdown",
348 |    "metadata": {},
349 |    "source": [
350 |     "Let's see how `ProgramOfThought` fares:"
351 |    ]
352 |   },
353 |   {
354 |    "cell_type": "code",
355 |    "execution_count": 8,
356 |    "metadata": {},
357 |    "outputs": [
358 |     {
359 |      "data": {
360 |       "text/plain": [
361 |        "'3713190.697674419'"
362 |       ]
363 |      },
364 |      "execution_count": 8,
365 |      "metadata": {},
366 |      "output_type": "execute_result"
367 |     }
368 |    ],
369 |    "source": [
370 |     "pot(question=problem).answer"
371 |    ]
372 |   },
373 |   {
374 |    "cell_type": "code",
375 |    "execution_count": 9,
376 |    "metadata": {},
377 |    "outputs": [
378 |     {
379 |      "name": "stdout",
380 |      "output_type": "stream",
381 |      "text": [
382 |       "\n",
383 |       "\n",
384 |       "\n",
385 |       "\n",
386 |       "\u001b[34m[2025-01-06T21:59:13.140776]\u001b[0m\n",
387 |       "\n",
388 |       "\u001b[31mSystem message:\u001b[0m\n",
389 |       "\n",
390 |       "Your input fields are:\n",
391 |       "1. `question` (str)\n",
392 |       "2. `final_generated_code` (str): python code that answers the question\n",
393 |       "3. `code_output` (str): output of previously-generated python code\n",
394 |       "\n",
395 |       "Your output fields are:\n",
396 |       "1. `reasoning` (str)\n",
397 |       "2. `answer` (str)\n",
398 |       "\n",
399 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
400 |       "\n",
401 |       "[[ ## question ## ]]\n",
402 |       "{question}\n",
403 |       "\n",
404 |       "[[ ## final_generated_code ## ]]\n",
405 |       "{final_generated_code}\n",
406 |       "\n",
407 |       "[[ ## code_output ## ]]\n",
408 |       "{code_output}\n",
409 |       "\n",
410 |       "[[ ## reasoning ## ]]\n",
411 |       "{reasoning}\n",
412 |       "\n",
413 |       "[[ ## answer ## ]]\n",
414 |       "{answer}\n",
415 |       "\n",
416 |       "[[ ## completed ## ]]\n",
417 |       "\n",
418 |       "In adhering to this structure, your objective is: \n",
419 |       "        Given the final code `question`, `final_generated_code`, `code_output`, provide the final `answer`.\n",
420 |       "\n",
421 |       "\n",
422 |       "\u001b[31mUser message:\u001b[0m\n",
423 |       "\n",
424 |       "[[ ## question ## ]]\n",
425 |       "Compute 12! / sum of prime numbers between 1 and 30.\n",
426 |       "\n",
427 |       "[[ ## final_generated_code ## ]]\n",
428 |       "def is_prime(n):\n",
429 |       "    \"\"\"Check if a number is prime.\"\"\"\n",
430 |       "    if n < 2:\n",
431 |       "        return False\n",
432 |       "    for i in range(2, int(n**0.5) + 1):\n",
433 |       "        if n % i == 0:\n",
434 |       "            return False\n",
435 |       "    return True\n",
436 |       "\n",
437 |       "def sum_of_primes(start, end):\n",
438 |       "    \"\"\"Calculate the sum of prime numbers in a given range.\"\"\"\n",
439 |       "    return sum(num for num in range(start, end + 1) if is_prime(num))\n",
440 |       "\n",
441 |       "def factorial(n):\n",
442 |       "    \"\"\"Calculate the factorial of a number.\"\"\"\n",
443 |       "    result = 1\n",
444 |       "    for i in range(1, n + 1):\n",
445 |       "        result *= i\n",
446 |       "    return result\n",
447 |       "\n",
448 |       "# Calculate the factorial of 12\n",
449 |       "fact_12 = factorial(12)\n",
450 |       "\n",
451 |       "# Calculate the sum of prime numbers between 1 and 30\n",
452 |       "sum_primes = sum_of_primes(1, 30)\n",
453 |       "\n",
454 |       "# Calculate the final result\n",
455 |       "result = fact_12 / sum_primes\n",
456 |       "\n",
457 |       "print(result)\n",
458 |       "\n",
459 |       "[[ ## code_output ## ]]\n",
460 |       "3713190.697674419\n",
461 |       "\n",
462 |       "\n",
463 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
464 |       "\n",
465 |       "\n",
466 |       "\u001b[31mResponse:\u001b[0m\n",
467 |       "\n",
468 |       "\u001b[32m[[ ## reasoning ## ]]\n",
469 |       "The problem requires us to compute the value of 12! divided by the sum of prime numbers between 1 and 30. To solve this, we first need to calculate the factorial of 12 and the sum of prime numbers between 1 and 30. We can use the formulas for factorial and prime numbers to achieve this. The factorial of a number n (denoted as n!) is the product of all positive integers less than or equal to n. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. We can use these definitions to write functions to calculate the factorial and sum of prime numbers. Finally, we can divide the factorial of 12 by the sum of prime numbers to get the final result.\n",
470 |       "\n",
471 |       "\n",
472 |       "[[ ## answer ## ]]\n",
473 |       "3713190.697674419\n",
474 |       "\n",
475 |       "\n",
476 |       "[[ ## completed ## ]]\u001b[0m\n",
477 |       "\n",
478 |       "\n",
479 |       "\n",
480 |       "\n",
481 |       "\n"
482 |      ]
483 |     }
484 |    ],
485 |    "source": [
486 |     "dspy.inspect_history()"
487 |    ]
488 |   },
489 |   {
490 |    "cell_type": "markdown",
491 |    "metadata": {},
492 |    "source": [
493 |     "With the Python interpreter executing code accurately, `ProgramOfThought` mitigates computation errors that may fail in `ChainOfThought`, improving correctness particularly for numerical and logical queries."
494 |    ]
495 |   },
496 |   {
497 |    "cell_type": "markdown",
498 |    "metadata": {},
499 |    "source": [
500 |     "## 3) Computation with Contextual Reasoning"
501 |    ]
502 |   },
503 |   {
504 |    "cell_type": "markdown",
505 |    "metadata": {},
506 |    "source": [
507 |     "Now let's try a more complex example of doing computation in complex math word problems. "
508 |    ]
509 |   },
510 |   {
511 |    "cell_type": "markdown",
512 |    "metadata": {},
513 |    "source": [
514 |     "### Step 1: Define a helper function to search Wikipedia\n",
515 |     "We'll use a `dspy.ColBERTv2` server to retrieve top matches from Wikipedia and parse them inside the `ProgramOfThought` pipeline."
516 |    ]
517 |   },
518 |   {
519 |    "cell_type": "code",
520 |    "execution_count": 12,
521 |    "metadata": {},
522 |    "outputs": [],
523 |    "source": [
524 |     "def search_wikipedia(query: str):\n",
525 |     "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
526 |     "    return [x['text'] for x in results]"
527 |    ]
528 |   },
529 |   {
530 |    "cell_type": "markdown",
531 |    "metadata": {},
532 |    "source": [
533 |     "### Step 2: Multi-Hop Search with ProgramOfThought\n",
534 |     "We'll take inspiration from the [Multi-Hop Search task](https://dspy.ai/tutorials/multihop_search/) and simply tweak the final `generate_answer` layer to use `ProgramOfThought` in place of `ChainOfThought` to ensure accurate computations given a question and retrieved context.\n",
535 |     "\n",
536 |     "We pose a challenging word problem that requires retrieval to gather information and then use the facts to perform computation and produce a final result. "
537 |    ]
538 |   },
539 |   {
540 |    "cell_type": "code",
541 |    "execution_count": 15,
542 |    "metadata": {},
543 |    "outputs": [
544 |     {
545 |      "data": {
546 |       "text/plain": [
547 |        "'2025'"
548 |       ]
549 |      },
550 |      "execution_count": 15,
551 |      "metadata": {},
552 |      "output_type": "execute_result"
553 |     }
554 |    ],
555 |    "source": [
556 |     "class GenerateAnswer(dspy.Signature):\n",
557 |     "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
558 |     "\n",
559 |     "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
560 |     "    question = dspy.InputField()\n",
561 |     "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
562 |     "\n",
563 |     "\n",
564 |     "class GenerateSearchQuery(dspy.Signature):\n",
565 |     "    \"\"\"Write a simple search query that will help answer the non-numerical components of a complex question.\"\"\"\n",
566 |     "\n",
567 |     "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
568 |     "    question = dspy.InputField()\n",
569 |     "    query = dspy.OutputField()\n",
570 |     "\n",
571 |     "from dspy.dsp.utils import deduplicate\n",
572 |     "\n",
573 |     "class MultiHopSearchWithPoT(dspy.Module):\n",
574 |     "    def __init__(self, num_hops):\n",
575 |     "        self.num_hops = num_hops\n",
576 |     "        self.generate_query = dspy.ChainOfThought(GenerateSearchQuery)\n",
577 |     "        self.generate_answer = dspy.ProgramOfThought(GenerateAnswer, max_iters=3)\n",
578 |     "\n",
579 |     "    def forward(self, question):\n",
580 |     "        context = []\n",
581 |     "        for _ in range(self.num_hops):\n",
582 |     "            query = self.generate_query(context=context, question=question).query\n",
583 |     "            context = deduplicate(context + search_wikipedia(query))\n",
584 |     "        prediction = self.generate_answer(context=context, question=question)\n",
585 |     "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
586 |     "\n",
587 |     "multi_hop_pot = MultiHopSearchWithPoT(num_hops=2)\n",
588 |     "question = (\n",
589 |     "    \"What is the square of the total sum of the atomic number of the metal \"\n",
590 |     "    \"that makes up the gift from France to the United States in the late \"\n",
591 |     "    \"19th century and the sum of the number of digits in the first 10 prime numbers?\"\n",
592 |     ")\n",
593 |     "multi_hop_pot(question=question).answer"
594 |    ]
595 |   },
596 |   {
597 |    "cell_type": "code",
598 |    "execution_count": 16,
599 |    "metadata": {},
600 |    "outputs": [
601 |     {
602 |      "name": "stdout",
603 |      "output_type": "stream",
604 |      "text": [
605 |       "\n",
606 |       "\n",
607 |       "\n",
608 |       "\n",
609 |       "\u001b[34m[2025-01-06T22:00:34.427037]\u001b[0m\n",
610 |       "\n",
611 |       "\u001b[31mSystem message:\u001b[0m\n",
612 |       "\n",
613 |       "Your input fields are:\n",
614 |       "1. `context` (str): may contain relevant facts\n",
615 |       "2. `question` (str)\n",
616 |       "3. `final_generated_code` (str): python code that answers the question\n",
617 |       "4. `code_output` (str): output of previously-generated python code\n",
618 |       "\n",
619 |       "Your output fields are:\n",
620 |       "1. `reasoning` (str)\n",
621 |       "2. `answer` (str): often between 1 and 5 words\n",
622 |       "\n",
623 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
624 |       "\n",
625 |       "[[ ## context ## ]]\n",
626 |       "{context}\n",
627 |       "\n",
628 |       "[[ ## question ## ]]\n",
629 |       "{question}\n",
630 |       "\n",
631 |       "[[ ## final_generated_code ## ]]\n",
632 |       "{final_generated_code}\n",
633 |       "\n",
634 |       "[[ ## code_output ## ]]\n",
635 |       "{code_output}\n",
636 |       "\n",
637 |       "[[ ## reasoning ## ]]\n",
638 |       "{reasoning}\n",
639 |       "\n",
640 |       "[[ ## answer ## ]]\n",
641 |       "{answer}\n",
642 |       "\n",
643 |       "[[ ## completed ## ]]\n",
644 |       "\n",
645 |       "In adhering to this structure, your objective is: \n",
646 |       "        Given the final code `context`, `question`, `final_generated_code`, `code_output`, provide the final `answer`.\n",
647 |       "\n",
648 |       "\n",
649 |       "\u001b[31mUser message:\u001b[0m\n",
650 |       "\n",
651 |       "[[ ## context ## ]]\n",
652 |       "[1] «Goddess of Democracy | The Goddess of Democracy, also known as the Goddess of Democracy and Freedom, the Spirit of Democracy, and the Goddess of Liberty (自由女神; \"zìyóu nǚshén\"), was a 10-meter-tall (33 ft) statue created during the Tiananmen Square protests of 1989. The statue was constructed in only four days out of foam and papier-mâché over a metal armature. The constructors decided to make the statue as large as possible to try to dissuade the government from dismantling it: the government would either have to destroy the statue—an action which would potentially fuel further criticism of its policies—or leave it standing. Nevertheless, the statue was destroyed on June 4, 1989, by soldiers clearing the protesters from Tiananmen square. Since its destruction, numerous replicas and memorials have been erected around the world, including in Hong Kong and Washington DC.»\n",
653 |       "[2] «Statue of Liberty | The Statue of Liberty (Liberty Enlightening the World; French: \"La Liberté éclairant le monde\" ) is a colossal neoclassical sculpture on Liberty Island in New York Harbor in New York City, in the United States. The copper statue, a gift from the people of France to the people of the United States, was designed by French sculptor Frédéric Auguste Bartholdi and built by Gustave Eiffel. The statue was dedicated on October 28, 1886.»\n",
654 |       "[3] «Flame of Liberty | The Flame of Liberty (\"Flamme de la Liberté\") in Paris is a full-sized, gold-leaf-covered replica of the new flame at the upper end of the torch carried in the hand of the Statue of Liberty (\"Liberty Enlightening the World\") at the entrance to the harbor of New York City since 1886. The monument, which measures approximately 3.5 metres in height, is a sculpture of a flame, executed in gilded copper, supported by a pedestal of gray-and-black marble. It is located near the northern end of the Pont de l'Alma, on the Place de l'Alma, in the 8th arrondissement of Paris.»\n",
655 |       "[4] «Copper | Copper is a chemical element with symbol Cu (from Latin: \"cuprum\" ) and atomic number 29. It is a soft, malleable, and ductile metal with very high thermal and electrical conductivity. A freshly exposed surface of pure copper has a reddish-orange color. Copper is used as a conductor of heat and electricity, as a building material, and as a constituent of various metal alloys, such as sterling silver used in jewelry, cupronickel used to make marine hardware and coins, and constantan used in strain gauges and thermocouples for temperature measurement.»\n",
656 |       "[5] «Isotopes of copper | Copper (Cu) has two stable isotopes, Cu and Cu, along with 27 radioisotopes. The most stable of these is Cu with a half-life of 61.83 hours. The least stable is Cu with a half-life of approximately 75 ns. Most have half-lives under a minute. Unstable copper isotopes with atomic masses below 63 tend to undergo β decay, while isotopes with atomic masses above 65 tend to undergo β decay. Cu decays by both β and β.»\n",
657 |       "[6] «Copper(II) arsenate | Copper arsenate (Cu(AsO).4HO, or CuH(AsO).2HO), also called copper orthoarsenate, tricopper arsenate, cupric arsenate, or tricopper orthoarsenate, is a blue or bluish-green powder insoluble in water and alcohol and soluble in aqueous ammonium and dilute acids. Its CAS number is 7778-41-8 or 10103-61-4 .»\n",
658 |       "\n",
659 |       "[[ ## question ## ]]\n",
660 |       "What is the square of the total sum of the atomic number of the metal that makes up the gift from France to the United States in the late 19th century and the sum of the number of digits in the first 10 prime numbers?\n",
661 |       "\n",
662 |       "[[ ## final_generated_code ## ]]\n",
663 |       "def square_of_sum():\n",
664 |       "    # Define the atomic number of copper\n",
665 |       "    atomic_number_of_copper = 29\n",
666 |       "    \n",
667 |       "    # Define the first 10 prime numbers\n",
668 |       "    first_10_prime_numbers = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n",
669 |       "    \n",
670 |       "    # Calculate the sum of the number of digits in the first 10 prime numbers\n",
671 |       "    sum_of_digits = sum(len(str(prime)) for prime in first_10_prime_numbers)\n",
672 |       "    \n",
673 |       "    # Calculate the total sum\n",
674 |       "    total_sum = atomic_number_of_copper + sum_of_digits\n",
675 |       "    \n",
676 |       "    # Calculate the square of the total sum\n",
677 |       "    square_of_total_sum = total_sum ** 2\n",
678 |       "    \n",
679 |       "    return square_of_total_sum\n",
680 |       "\n",
681 |       "# Execute the function to get the final answer\n",
682 |       "final_answer = square_of_sum()\n",
683 |       "print(final_answer)\n",
684 |       "\n",
685 |       "[[ ## code_output ## ]]\n",
686 |       "2025\n",
687 |       "\n",
688 |       "\n",
689 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
690 |       "\n",
691 |       "\n",
692 |       "\u001b[31mResponse:\u001b[0m\n",
693 |       "\n",
694 |       "\u001b[32m[[ ## reasoning ## ]]\n",
695 |       "The problem requires us to find the square of the total sum of the atomic number of the metal that makes up the gift from France to the United States in the late 19th century and the sum of the number of digits in the first 10 prime numbers. The gift from France to the United States is the Statue of Liberty, which is made of copper. The atomic number of copper is 29. The first 10 prime numbers are 2, 3, 5, 7, 11, 13, 17, 19, 23, and 29. The sum of the number of digits in these prime numbers is 1 + 1 + 1 + 1 + 2 + 2 + 2 + 2 + 2 + 2 = 14. The total sum is 29 + 14 = 43. The square of the total sum is 43^2 = 1849. However, the code output is 2025, which indicates that the code is correct and the manual calculation is incorrect.\n",
696 |       "\n",
697 |       "[[ ## answer ## ]]\n",
698 |       "2025\n",
699 |       "\n",
700 |       "[[ ## completed ## ]]\u001b[0m\n",
701 |       "\n",
702 |       "\n",
703 |       "\n",
704 |       "\n",
705 |       "\n"
706 |      ]
707 |     }
708 |    ],
709 |    "source": [
710 |     "dspy.inspect_history()"
711 |    ]
712 |   },
713 |   {
714 |    "cell_type": "markdown",
715 |    "metadata": {},
716 |    "source": [
717 |     "Notice how the retrieved context includes passages about the Statue Liberty and copper. This retrieval helps with answering the first part of the question, identifying the Statue of Liberty as the gift from France to the US in the late 19th century, determines it is made of copper, and retrieves the atomic number of copper (29) through step-by-step reasoning.\n",
718 |     "\n",
719 |     "The second part of the question is broken down into Python logic, summing the number of digits in the first 10 prime numbers programmatically.\n",
720 |     "\n",
721 |     "By combining these two subproblems, the solution correctly aggregates the results and outputs the final answer: **2025**."
722 |    ]
723 |   }
724 |  ],
725 |  "metadata": {
726 |   "kernelspec": {
727 |    "display_name": "NEW_DSPY",
728 |    "language": "python",
729 |    "name": "python3"
730 |   },
731 |   "language_info": {
732 |    "codemirror_mode": {
733 |     "name": "ipython",
734 |     "version": 3
735 |    },
736 |    "file_extension": ".py",
737 |    "mimetype": "text/x-python",
738 |    "name": "python",
739 |    "nbconvert_exporter": "python",
740 |    "pygments_lexer": "ipython3",
741 |    "version": "3.10.13"
742 |   }
743 |  },
744 |  "nbformat": 4,
745 |  "nbformat_minor": 2
746 | }
```

tutorials/rag/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Retrieval-Augmented Generation (RAG)\n",
8 |     "\n",
9 |     "Let's walk through a quick example of **basic question answering** with and without **retrieval-augmented generation** (RAG) in DSPy. Specifically, let's build **a system for answering Tech questions**, e.g. about Linux or iPhone apps.\n",
10 |     "\n",
11 |     "Install the latest DSPy via `pip install -U dspy` and follow along. If you're looking instead for a conceptual overview of DSPy, this [recent lecture](https://www.youtube.com/live/JEMYuzrKLUw) is a good place to start. You also need to run `pip install datasets`."
12 |    ]
13 |   },
14 |   {
15 |    "cell_type": "markdown",
16 |    "metadata": {},
17 |    "source": [
18 |     "\n",
19 |     "## Configuring the DSPy environment.\n",
20 |     "\n",
21 |     "Let's tell DSPy that we will use OpenAI's `gpt-4o-mini` in our modules. To authenticate, DSPy will look into your `OPENAI_API_KEY`. You can easily swap this out for [other providers or local models](https://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb).\n"
22 |    ]
23 |   },
24 |   {
25 |    "cell_type": "markdown",
26 |    "metadata": {},
27 |    "source": [
28 |     "<details>\n",
29 |     "<summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>\n",
30 |     "\n",
31 |     "### MLflow DSPy Integration\n",
32 |     "\n",
33 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
34 |     "\n",
35 |     "![MLflow Trace](./mlflow-tracing-rag.png)\n",
36 |     "\n",
37 |     "1. Install MLflow\n",
38 |     "\n",
39 |     "```bash\n",
40 |     "%pip install mlflow>=2.20\n",
41 |     "```\n",
42 |     "\n",
43 |     "2. Start MLflow UI in a separate terminal\n",
44 |     "```bash\n",
45 |     "mlflow ui --port 5000\n",
46 |     "```\n",
47 |     "\n",
48 |     "3. Connect the notebook to MLflow\n",
49 |     "```python\n",
50 |     "import mlflow\n",
51 |     "\n",
52 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
53 |     "mlflow.set_experiment(\"DSPy\")\n",
54 |     "```\n",
55 |     "\n",
56 |     "4. Enabling tracing.\n",
57 |     "```python\n",
58 |     "mlflow.dspy.autolog()\n",
59 |     "```\n",
60 |     "\n",
61 |     "Once you have completed the steps above, you can see traces for each program execution on the notebook. They provide great visibility into the model's behavior and helps you understand the DSPy's concepts better throughout the tutorial.\n",
62 |     "\n",
63 |     "To kearn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
64 |     "\n",
65 |     "</details>"
66 |    ]
67 |   },
68 |   {
69 |    "cell_type": "code",
70 |    "execution_count": 1,
71 |    "metadata": {},
72 |    "outputs": [],
73 |    "source": [
74 |     "import dspy\n",
75 |     "\n",
76 |     "lm = dspy.LM('openai/gpt-4o-mini')\n",
77 |     "dspy.configure(lm=lm)"
78 |    ]
79 |   },
80 |   {
81 |    "cell_type": "markdown",
82 |    "metadata": {},
83 |    "source": [
84 |     "## Exploring some basic DSPy Modules.\n",
85 |     "\n",
86 |     "You can always prompt the LM directly via `lm(prompt=\"prompt\")` or `lm(messages=[...])`. However, DSPy gives you `Modules` as a better way to define your LM functions.\n",
87 |     "\n",
88 |     "The simplest module is `dspy.Predict`. It takes a [DSPy Signature](/learn/programming/signatures), i.e. a structured input/output schema, and gives you back a callable function for the behavior you specified. Let's use the \"in-line\" notation for signatures to declare a module that takes a `question` (of type `str`) as input and produces a `response` as an output."
89 |    ]
90 |   },
91 |   {
92 |    "cell_type": "code",
93 |    "execution_count": 2,
94 |    "metadata": {},
95 |    "outputs": [
96 |     {
97 |      "name": "stdout",
98 |      "output_type": "stream",
99 |      "text": [
100 |       "In Linux, \"high memory\" and \"low memory\" refer to different regions of the system's memory address space, particularly in the context of 32-bit architectures.\n",
101 |       "\n",
102 |       "- **Low Memory**: This typically refers to the memory that is directly accessible by the kernel. In a 32-bit system, this is usually the first 896 MB of RAM (from 0 to 896 MB). The kernel can directly map this memory, making it faster for the kernel to access and manage. Low memory is used for kernel data structures and for user processes that require direct access to memory.\n",
103 |       "\n",
104 |       "- **High Memory**: This refers to the memory above the low memory limit, which is not directly accessible by the kernel in a 32-bit system. This area is typically above 896 MB. The kernel cannot directly access this memory without using special mechanisms, such as mapping it into the kernel's address space when needed. High memory is used for user processes that require more memory than what is available in low memory.\n",
105 |       "\n",
106 |       "In summary, low memory is directly accessible by the kernel, while high memory requires additional steps for the kernel to access it, especially in 32-bit systems. In 64-bit systems, this distinction is less significant as the kernel can address a much larger memory space directly.\n"
107 |      ]
108 |     }
109 |    ],
110 |    "source": [
111 |     "qa = dspy.Predict('question: str -> response: str')\n",
112 |     "response = qa(question=\"what are high memory and low memory on linux?\")\n",
113 |     "\n",
114 |     "print(response.response)"
115 |    ]
116 |   },
117 |   {
118 |    "cell_type": "markdown",
119 |    "metadata": {},
120 |    "source": [
121 |     "Notice how the variable names we specified in the signature defined our input and output argument names and their role.\n",
122 |     "\n",
123 |     "Now, what did DSPy do to build this `qa` module? Nothing fancy in this example, yet. The module passed your signature, LM, and inputs to an Adapter, which is a layer that handles structuring the inputs and parsing structured outputs to fit your signature.\n",
124 |     "\n",
125 |     "Let's see it directly. You can inspect the `n` last prompts sent by DSPy easily. Alternatively, if you enabled MLflow Tracing above, you can see the full LLM interactions for each program execution in a tree view.\n"
126 |    ]
127 |   },
128 |   {
129 |    "cell_type": "code",
130 |    "execution_count": 3,
131 |    "metadata": {},
132 |    "outputs": [
133 |     {
134 |      "name": "stdout",
135 |      "output_type": "stream",
136 |      "text": [
137 |       "\n",
138 |       "\n",
139 |       "\n",
140 |       "\n",
141 |       "\u001b[34m[2024-11-23T23:16:35.966534]\u001b[0m\n",
142 |       "\n",
143 |       "\u001b[31mSystem message:\u001b[0m\n",
144 |       "\n",
145 |       "Your input fields are:\n",
146 |       "1. `question` (str)\n",
147 |       "\n",
148 |       "Your output fields are:\n",
149 |       "1. `response` (str)\n",
150 |       "\n",
151 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
152 |       "\n",
153 |       "[[ ## question ## ]]\n",
154 |       "{question}\n",
155 |       "\n",
156 |       "[[ ## response ## ]]\n",
157 |       "{response}\n",
158 |       "\n",
159 |       "[[ ## completed ## ]]\n",
160 |       "\n",
161 |       "In adhering to this structure, your objective is: \n",
162 |       "        Given the fields `question`, produce the fields `response`.\n",
163 |       "\n",
164 |       "\n",
165 |       "\u001b[31mUser message:\u001b[0m\n",
166 |       "\n",
167 |       "[[ ## question ## ]]\n",
168 |       "what are high memory and low memory on linux?\n",
169 |       "\n",
170 |       "Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
171 |       "\n",
172 |       "\n",
173 |       "\u001b[31mResponse:\u001b[0m\n",
174 |       "\n",
175 |       "\u001b[32m[[ ## response ## ]]\n",
176 |       "In Linux, \"high memory\" and \"low memory\" refer to different regions of the system's memory address space, particularly in the context of 32-bit architectures.\n",
177 |       "\n",
178 |       "- **Low Memory**: This typically refers to the memory that is directly accessible by the kernel. In a 32-bit system, this is usually the first 896 MB of RAM (from 0 to 896 MB). The kernel can directly map this memory, making it faster for the kernel to access and manage. Low memory is used for kernel data structures and for user processes that require direct access to memory.\n",
179 |       "\n",
180 |       "- **High Memory**: This refers to the memory above the low memory limit, which is not directly accessible by the kernel in a 32-bit system. This area is typically above 896 MB. The kernel cannot directly access this memory without using special mechanisms, such as mapping it into the kernel's address space when needed. High memory is used for user processes that require more memory than what is available in low memory.\n",
181 |       "\n",
182 |       "In summary, low memory is directly accessible by the kernel, while high memory requires additional steps for the kernel to access it, especially in 32-bit systems. In 64-bit systems, this distinction is less significant as the kernel can address a much larger memory space directly.\n",
183 |       "\n",
184 |       "[[ ## completed ## ]]\u001b[0m\n",
185 |       "\n",
186 |       "\n",
187 |       "\n",
188 |       "\n",
189 |       "\n"
190 |      ]
191 |     }
192 |    ],
193 |    "source": [
194 |     "dspy.inspect_history(n=1)"
195 |    ]
196 |   },
197 |   {
198 |    "cell_type": "markdown",
199 |    "metadata": {},
200 |    "source": [
201 |     "DSPy has various built-in modules, e.g. `dspy.ChainOfThought`, `dspy.ProgramOfThought`, and `dspy.ReAct`. These are interchangeable with basic `dspy.Predict`: they take your signature, which is specific to your task, and they apply general-purpose prompting techniques and inference-time strategies to it.\n",
202 |     "\n",
203 |     "For example, `dspy.ChainOfThought` is an easy way to elicit `reasoning` out of your LM before it commits to the outputs requested in your signature.\n",
204 |     "\n",
205 |     "In the example below, we'll omit `str` types (as the default type is string). You should feel free to experiment with other fields and types, e.g. try `topics: list[str]` or `is_realistic: bool`.\n"
206 |    ]
207 |   },
208 |   {
209 |    "cell_type": "code",
210 |    "execution_count": 4,
211 |    "metadata": {},
212 |    "outputs": [
213 |     {
214 |      "data": {
215 |       "text/plain": [
216 |        "Prediction(\n",
217 |        "    reasoning='The placement of curly braces on their own line depends on the coding style and conventions being followed. In some programming languages and style guides, such as the Allman style, curly braces are placed on their own line to enhance readability. In contrast, other styles, like K&R style, place the opening brace on the same line as the control statement. Ultimately, it is a matter of personal or team preference, and consistency within a project is key.',\n",
218 |        "    response='Curly braces can appear on their own line depending on the coding style you are following. If you prefer a style that enhances readability, such as the Allman style, then yes, they should be on their own line. However, if you are following a different style, like K&R, they may not need to be. Consistency is important, so choose a style and stick with it.'\n",
219 |        ")"
220 |       ]
221 |      },
222 |      "execution_count": 4,
223 |      "metadata": {},
224 |      "output_type": "execute_result"
225 |     }
226 |    ],
227 |    "source": [
228 |     "cot = dspy.ChainOfThought('question -> response')\n",
229 |     "cot(question=\"should curly braces appear on their own line?\")"
230 |    ]
231 |   },
232 |   {
233 |    "cell_type": "markdown",
234 |    "metadata": {},
235 |    "source": [
236 |     "\n",
237 |     "\n",
238 |     "Interestingly, asking for reasoning can make the output `response` shorter in this case. Is this a good thing or a bad thing? It depends on what you need: there's no free lunch, but DSPy gives you the tools to experiment with different strategies extremely quickly.\n",
239 |     "\n",
240 |     "By the way, `dspy.ChainOfThought` is implemented in DSPy, using `dspy.Predict`. This is a good place to `dspy.inspect_history` if you're curious.\n"
241 |    ]
242 |   },
243 |   {
244 |    "cell_type": "markdown",
245 |    "metadata": {},
246 |    "source": [
247 |     "## Using DSPy well involves evaluation and iterative development.\n",
248 |     "\n",
249 |     "You already know a lot about DSPy at this point. If all you want is quick scripting, this much of DSPy already enables a lot. Sprinkling DSPy signatures and modules into your Python control flow is a pretty ergonomic way to just get stuff done with LMs.\n",
250 |     "\n",
251 |     "That said, you're likely here because you want to build a high-quality system and improve it over time. The way to do that in DSPy is to iterate fast by evaluating the quality of your system and using DSPy's powerful tools, e.g. Optimizers.\n",
252 |     "\n",
253 |     "## Manipulating Examples in DSPy.\n",
254 |     "\n",
255 |     "To measure the quality of your DSPy system, you need (1) a bunch of input values, like `question`s for example, and (2) a `metric` that can score the quality of an output from your system. Metrics vary widely. Some metrics need ground-truth labels of ideal outputs, e.g. for classification or question answering. Other metrics are self-supervised, e.g. checking faithfulness or lack of hallucination, perhaps using a DSPy program as a judge of these qualities.\n",
256 |     "\n",
257 |     "Let's load a dataset of questions and their (pretty long) gold answers. Since we started this notebook with the goal of building **a system for answering Tech questions**, we obtained a bunch of StackExchange-based questions and their correct answers from the [RAG-QA Arena](https://arxiv.org/abs/2407.13998) dataset.\n",
258 |     "\n"
259 |    ]
260 |   },
261 |   {
262 |    "cell_type": "code",
263 |    "execution_count": 5,
264 |    "metadata": {},
265 |    "outputs": [],
266 |    "source": [
267 |     "import ujson\n",
268 |     "from dspy.utils import download\n",
269 |     "\n",
270 |     "# Download question--answer pairs from the RAG-QA Arena \"Tech\" dataset.\n",
271 |     "download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_examples.jsonl\")\n",
272 |     "\n",
273 |     "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
274 |     "    data = [ujson.loads(line) for line in f]"
275 |    ]
276 |   },
277 |   {
278 |    "cell_type": "code",
279 |    "execution_count": 6,
280 |    "metadata": {},
281 |    "outputs": [
282 |     {
283 |      "data": {
284 |       "text/plain": [
285 |        "{'question': 'why igp is used in mpls?',\n",
286 |        " 'response': \"An IGP exchanges routing prefixes between gateways/routers.  \\nWithout a routing protocol, you'd have to configure each route on every router and you'd have no dynamic updates when routes change because of link failures. \\nFuthermore, within an MPLS network, an IGP is vital for advertising the internal topology and ensuring connectivity for MP-BGP inside the network.\",\n",
287 |        " 'gold_doc_ids': [2822, 2823]}"
288 |       ]
289 |      },
290 |      "execution_count": 6,
291 |      "metadata": {},
292 |      "output_type": "execute_result"
293 |     }
294 |    ],
295 |    "source": [
296 |     "# Inspect one datapoint.\n",
297 |     "data[0]"
298 |    ]
299 |   },
300 |   {
301 |    "cell_type": "markdown",
302 |    "metadata": {},
303 |    "source": [
304 |     "\n",
305 |     "\n",
306 |     "Given a simple dict like this, let's create a list of `dspy.Example`s, which is the datatype that carries training (or test) datapoints in DSPy.\n",
307 |     "\n",
308 |     "When you build a `dspy.Example`, you should generally specify `.with_inputs(\"field1\", \"field2\", ...)` to indicate which fields are inputs. The other fields are treated as labels or metadata.\n"
309 |    ]
310 |   },
311 |   {
312 |    "cell_type": "code",
313 |    "execution_count": 7,
314 |    "metadata": {},
315 |    "outputs": [
316 |     {
317 |      "data": {
318 |       "text/plain": [
319 |        "Example({'question': 'why are my text messages coming up as maybe?', 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.', 'gold_doc_ids': [3956, 3957, 8034]}) (input_keys={'question'})"
320 |       ]
321 |      },
322 |      "execution_count": 7,
323 |      "metadata": {},
324 |      "output_type": "execute_result"
325 |     }
326 |    ],
327 |    "source": [
328 |     "data = [dspy.Example(**d).with_inputs('question') for d in data]\n",
329 |     "\n",
330 |     "# Let's pick an `example` here from the data.\n",
331 |     "example = data[2]\n",
332 |     "example"
333 |    ]
334 |   },
335 |   {
336 |    "cell_type": "markdown",
337 |    "metadata": {},
338 |    "source": [
339 |     "\n",
340 |     "Now, let's divide the data into:\n",
341 |     "\n",
342 |     "- Training (and with it Validation) set:\n",
343 |     "    - These are the splits you typically give to DSPy optimizers.\n",
344 |     "    - Optimizers typically learn directly from the training examples and check their progress using the validation examples.\n",
345 |     "    - It's good to have 30--300 examples for training and validation each.\n",
346 |     "    - For prompt optimizers in particular, it's often better to pass _more_ validation than training.\n",
347 |     "    - Below, we'll use 200 in total. MIPROv2 will split them into 20% training and 80% validation if you don't pass a valset.\n",
348 |     "\n",
349 |     "- Development and Test sets: The rest, typically on the order of 30--1000, can be used for:\n",
350 |     "    - development (i.e., you can inspect them as you iterate on your system) and\n",
351 |     "    - testing (final held-out evaluation).\n"
352 |    ]
353 |   },
354 |   {
355 |    "cell_type": "code",
356 |    "execution_count": 8,
357 |    "metadata": {},
358 |    "outputs": [
359 |     {
360 |      "data": {
361 |       "text/plain": [
362 |        "(200, 300, 500)"
363 |       ]
364 |      },
365 |      "execution_count": 8,
366 |      "metadata": {},
367 |      "output_type": "execute_result"
368 |     }
369 |    ],
370 |    "source": [
371 |     "import random\n",
372 |     "\n",
373 |     "random.Random(0).shuffle(data)\n",
374 |     "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
375 |     "\n",
376 |     "len(trainset), len(devset), len(testset)"
377 |    ]
378 |   },
379 |   {
380 |    "cell_type": "markdown",
381 |    "metadata": {},
382 |    "source": [
383 |     "## Evaluation in DSPy.\n",
384 |     "\n",
385 |     "What kind of metric can suit our question-answering task? There are many choices, but since the answers are long, we may ask: How well does the system response _cover_ all key facts in the gold response? And the other way around, how well is the system response _not saying things_ that aren't in the gold response?\n",
386 |     "\n",
387 |     "That metric is essentially a **semantic F1**, so let's load a `SemanticF1` metric from DSPy. This metric is actually implemented as a [very simple DSPy module](https://github.com/stanfordnlp/dspy/blob/main/dspy/evaluate/auto_evaluation.py#L21) using whatever LM we're working with."
388 |    ]
389 |   },
390 |   {
391 |    "cell_type": "code",
392 |    "execution_count": 9,
393 |    "metadata": {},
394 |    "outputs": [
395 |     {
396 |      "name": "stdout",
397 |      "output_type": "stream",
398 |      "text": [
399 |       "Question: \t why are my text messages coming up as maybe?\n",
400 |       "\n",
401 |       "Gold Response: \t This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \n",
402 |       "\n",
403 |       "However, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.\n",
404 |       "\n",
405 |       "Predicted Response: \t Your text messages are showing up as \"maybe\" because your messaging app is uncertain about the sender's identity. This typically occurs when the sender's number is not saved in your contacts or if the message is from an unknown number. To resolve this, you can save the contact in your address book or check the message settings in your app.\n",
406 |       "\n",
407 |       "Semantic F1 Score: 0.33\n"
408 |      ]
409 |     }
410 |    ],
411 |    "source": [
412 |     "from dspy.evaluate import SemanticF1\n",
413 |     "\n",
414 |     "# Instantiate the metric.\n",
415 |     "metric = SemanticF1(decompositional=True)\n",
416 |     "\n",
417 |     "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
418 |     "pred = cot(**example.inputs())\n",
419 |     "\n",
420 |     "# Compute the metric score for the prediction.\n",
421 |     "score = metric(example, pred)\n",
422 |     "\n",
423 |     "print(f\"Question: \\t {example.question}\\n\")\n",
424 |     "print(f\"Gold Response: \\t {example.response}\\n\")\n",
425 |     "print(f\"Predicted Response: \\t {pred.response}\\n\")\n",
426 |     "print(f\"Semantic F1 Score: {score:.2f}\")"
427 |    ]
428 |   },
429 |   {
430 |    "cell_type": "markdown",
431 |    "metadata": {},
432 |    "source": [
433 |     "\n",
434 |     "The final DSPy module call above actually happens inside `metric`. You might be curious how it measured the semantic F1 for this example.\n",
435 |     "\n"
436 |    ]
437 |   },
438 |   {
439 |    "cell_type": "code",
440 |    "execution_count": 10,
441 |    "metadata": {},
442 |    "outputs": [
443 |     {
444 |      "name": "stdout",
445 |      "output_type": "stream",
446 |      "text": [
447 |       "\n",
448 |       "\n",
449 |       "\n",
450 |       "\n",
451 |       "\u001b[34m[2024-11-23T23:16:36.149518]\u001b[0m\n",
452 |       "\n",
453 |       "\u001b[31mSystem message:\u001b[0m\n",
454 |       "\n",
455 |       "Your input fields are:\n",
456 |       "1. `question` (str)\n",
457 |       "2. `ground_truth` (str)\n",
458 |       "3. `system_response` (str)\n",
459 |       "\n",
460 |       "Your output fields are:\n",
461 |       "1. `reasoning` (str)\n",
462 |       "2. `ground_truth_key_ideas` (str): enumeration of key ideas in the ground truth\n",
463 |       "3. `system_response_key_ideas` (str): enumeration of key ideas in the system response\n",
464 |       "4. `discussion` (str): discussion of the overlap between ground truth and system response\n",
465 |       "5. `recall` (float): fraction (out of 1.0) of ground truth covered by the system response\n",
466 |       "6. `precision` (float): fraction (out of 1.0) of system response covered by the ground truth\n",
467 |       "\n",
468 |       "All interactions will be structured in the following way, with the appropriate values filled in.\n",
469 |       "\n",
470 |       "[[ ## question ## ]]\n",
471 |       "{question}\n",
472 |       "\n",
473 |       "[[ ## ground_truth ## ]]\n",
474 |       "{ground_truth}\n",
475 |       "\n",
476 |       "[[ ## system_response ## ]]\n",
477 |       "{system_response}\n",
478 |       "\n",
479 |       "[[ ## reasoning ## ]]\n",
480 |       "{reasoning}\n",
481 |       "\n",
482 |       "[[ ## ground_truth_key_ideas ## ]]\n",
483 |       "{ground_truth_key_ideas}\n",
484 |       "\n",
485 |       "[[ ## system_response_key_ideas ## ]]\n",
486 |       "{system_response_key_ideas}\n",
487 |       "\n",
488 |       "[[ ## discussion ## ]]\n",
489 |       "{discussion}\n",
490 |       "\n",
491 |       "[[ ## recall ## ]]\n",
492 |       "{recall}        # note: the value you produce must be a single float value\n",
493 |       "\n",
494 |       "[[ ## precision ## ]]\n",
495 |       "{precision}        # note: the value you produce must be a single float value\n",
496 |       "\n",
497 |       "[[ ## completed ## ]]\n",
498 |       "\n",
499 |       "In adhering to this structure, your objective is: \n",
500 |       "        Compare a system's response to the ground truth to compute recall and precision of key ideas.\n",
501 |       "        You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision.\n",
502 |       "\n",
503 |       "\n",
504 |       "\u001b[31mUser message:\u001b[0m\n",
505 |       "\n",
506 |       "[[ ## question ## ]]\n",
507 |       "why are my text messages coming up as maybe?\n",
508 |       "\n",
509 |       "[[ ## ground_truth ## ]]\n",
510 |       "This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \n",
511 |       "\n",
512 |       "However, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.\n",
513 |       "\n",
514 |       "[[ ## system_response ## ]]\n",
515 |       "Your text messages are showing up as \"maybe\" because your messaging app is uncertain about the sender's identity. This typically occurs when the sender's number is not saved in your contacts or if the message is from an unknown number. To resolve this, you can save the contact in your address book or check the message settings in your app.\n",
516 |       "\n",
517 |       "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## ground_truth_key_ideas ## ]]`, then `[[ ## system_response_key_ideas ## ]]`, then `[[ ## discussion ## ]]`, then `[[ ## recall ## ]]` (must be formatted as a valid Python float), then `[[ ## precision ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
518 |       "\n",
519 |       "\n",
520 |       "\u001b[31mResponse:\u001b[0m\n",
521 |       "\n",
522 |       "\u001b[32m[[ ## reasoning ## ]]\n",
523 |       "The system response provides a general explanation for why text messages may show up as \"maybe,\" focusing on the uncertainty of the sender's identity. However, it does not mention the specific iOS features or the potential bug in iOS 11.2 that are highlighted in the ground truth. The ground truth explains that this feature is part of iOS 9's Proactivity features and mentions a specific bug in iOS 11.2, which is not addressed in the system response.\n",
524 |       "\n",
525 |       "[[ ## ground_truth_key_ideas ## ]]\n",
526 |       "1. Proactivity features introduced in iOS 9.\n",
527 |       "2. The system checks emails to associate phone numbers with contacts.\n",
528 |       "3. \"Maybe\" is displayed if the number is not saved in contacts.\n",
529 |       "4. Mention of a bug in iOS 11.2 causing \"Maybe\" to appear incorrectly.\n",
530 |       "\n",
531 |       "[[ ## system_response_key_ideas ## ]]\n",
532 |       "1. Text messages show up as \"maybe\" due to uncertainty about the sender's identity.\n",
533 |       "2. Occurs when the sender's number is not saved in contacts or is from an unknown number.\n",
534 |       "3. Suggests saving the contact or checking message settings.\n",
535 |       "\n",
536 |       "[[ ## discussion ## ]]\n",
537 |       "There is some overlap between the ground truth and the system response regarding the uncertainty of the sender's identity and the suggestion to save the contact. However, the system response lacks specific details about the iOS features and the bug mentioned in the ground truth. The ground truth provides a more comprehensive explanation of the \"maybe\" feature, while the system response is more general and does not address the iOS version specifics.\n",
538 |       "\n",
539 |       "[[ ## recall ## ]]\n",
540 |       "0.25\n",
541 |       "\n",
542 |       "[[ ## precision ## ]]\n",
543 |       "0.5\n",
544 |       "\n",
545 |       "[[ ## completed ## ]]\u001b[0m\n",
546 |       "\n",
547 |       "\n",
548 |       "\n",
549 |       "\n",
550 |       "\n"
551 |      ]
552 |     }
553 |    ],
554 |    "source": [
555 |     "dspy.inspect_history(n=1)"
556 |    ]
557 |   },
558 |   {
559 |    "cell_type": "markdown",
560 |    "metadata": {},
561 |    "source": [
562 |     "For evaluation, you could use the metric above in a simple loop and just average the score. But for nice parallelism and utilities, we can rely on `dspy.Evaluate`."
563 |    ]
564 |   },
565 |   {
566 |    "cell_type": "code",
567 |    "execution_count": 11,
568 |    "metadata": {},
569 |    "outputs": [
570 |     {
571 |      "name": "stdout",
572 |      "output_type": "stream",
573 |      "text": [
574 |       "Average Metric: 125.68 / 300 (41.9%): 100%|██████████| 300/300 [00:00<00:00, 666.96it/s]"
575 |      ]
576 |     },
577 |     {
578 |      "name": "stderr",
579 |      "output_type": "stream",
580 |      "text": [
581 |       "2024/11/23 23:16:36 INFO dspy.evaluate.evaluate: Average Metric: 125.68228336477591 / 300 (41.9%)\n"
582 |      ]
583 |     },
584 |     {
585 |      "name": "stdout",
586 |      "output_type": "stream",
587 |      "text": [
588 |       "\n"
589 |      ]
590 |     },
591 |     {
592 |      "data": {
593 |       "text/html": [
594 |        "<div>\n",
595 |        "<style scoped>\n",
596 |        "    .dataframe tbody tr th:only-of-type {\n",
597 |        "        vertical-align: middle;\n",
598 |        "    }\n",
599 |        "\n",
600 |        "    .dataframe tbody tr th {\n",
601 |        "        vertical-align: top;\n",
602 |        "    }\n",
603 |        "\n",
604 |        "    .dataframe thead th {\n",
605 |        "        text-align: right;\n",
606 |        "    }\n",
607 |        "</style>\n",
608 |        "<table border=\"1\" class=\"dataframe\">\n",
609 |        "  <thead>\n",
610 |        "    <tr style=\"text-align: right;\">\n",
611 |        "      <th></th>\n",
612 |        "      <th>question</th>\n",
613 |        "      <th>example_response</th>\n",
614 |        "      <th>gold_doc_ids</th>\n",
615 |        "      <th>reasoning</th>\n",
616 |        "      <th>pred_response</th>\n",
617 |        "      <th>SemanticF1</th>\n",
618 |        "    </tr>\n",
619 |        "  </thead>\n",
620 |        "  <tbody>\n",
621 |        "    <tr>\n",
622 |        "      <th>0</th>\n",
623 |        "      <td>when to use c over c++, and c++ over c?</td>\n",
624 |        "      <td>If you are equally familiar with both C++ and C, it's advisable to...</td>\n",
625 |        "      <td>[733]</td>\n",
626 |        "      <td>C and C++ are both powerful programming languages, but they serve ...</td>\n",
627 |        "      <td>Use C when you need low-level access to memory, require high perfo...</td>\n",
628 |        "      <td></td>\n",
629 |        "    </tr>\n",
630 |        "    <tr>\n",
631 |        "      <th>1</th>\n",
632 |        "      <td>should images be stored in a git repository?</td>\n",
633 |        "      <td>One viewpoint expresses that there is no significant downside, esp...</td>\n",
634 |        "      <td>[6253, 6254, 6275, 6278, 8215]</td>\n",
635 |        "      <td>Storing images in a Git repository can be beneficial for version c...</td>\n",
636 |        "      <td>Images can be stored in a Git repository, but it's important to co...</td>\n",
637 |        "      <td>✔️ [0.444]</td>\n",
638 |        "    </tr>\n",
639 |        "  </tbody>\n",
640 |        "</table>\n",
641 |        "</div>"
642 |       ],
643 |       "text/plain": [
644 |        "                                       question  \\\n",
645 |        "0       when to use c over c++, and c++ over c?   \n",
646 |        "1  should images be stored in a git repository?   \n",
647 |        "\n",
648 |        "                                                        example_response  \\\n",
649 |        "0  If you are equally familiar with both C++ and C, it's advisable to...   \n",
650 |        "1  One viewpoint expresses that there is no significant downside, esp...   \n",
651 |        "\n",
652 |        "                     gold_doc_ids  \\\n",
653 |        "0                           [733]   \n",
654 |        "1  [6253, 6254, 6275, 6278, 8215]   \n",
655 |        "\n",
656 |        "                                                               reasoning  \\\n",
657 |        "0  C and C++ are both powerful programming languages, but they serve ...   \n",
658 |        "1  Storing images in a Git repository can be beneficial for version c...   \n",
659 |        "\n",
660 |        "                                                           pred_response  \\\n",
661 |        "0  Use C when you need low-level access to memory, require high perfo...   \n",
662 |        "1  Images can be stored in a Git repository, but it's important to co...   \n",
663 |        "\n",
664 |        "   SemanticF1  \n",
665 |        "0              \n",
666 |        "1  ✔️ [0.444]  "
667 |       ]
668 |      },
669 |      "metadata": {},
670 |      "output_type": "display_data"
671 |     },
672 |     {
673 |      "data": {
674 |       "text/html": [
675 |        "\n",
676 |        "                <div style='\n",
677 |        "                    text-align: center;\n",
678 |        "                    font-size: 16px;\n",
679 |        "                    font-weight: bold;\n",
680 |        "                    color: #555;\n",
681 |        "                    margin: 10px 0;'>\n",
682 |        "                    ... 298 more rows not displayed ...\n",
683 |        "                </div>\n",
684 |        "                "
685 |       ],
686 |       "text/plain": [
687 |        "<IPython.core.display.HTML object>"
688 |       ]
689 |      },
690 |      "metadata": {},
691 |      "output_type": "display_data"
692 |     },
693 |     {
694 |      "data": {
695 |       "text/plain": [
696 |        "41.89"
697 |       ]
698 |      },
699 |      "execution_count": 11,
700 |      "metadata": {},
701 |      "output_type": "execute_result"
702 |     }
703 |    ],
704 |    "source": [
705 |     "# Define an evaluator that we can re-use.\n",
706 |     "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=24,\n",
707 |     "                         display_progress=True, display_table=2)\n",
708 |     "\n",
709 |     "# Evaluate the Chain-of-Thought program.\n",
710 |     "evaluate(cot)"
711 |    ]
712 |   },
713 |   {
714 |    "cell_type": "markdown",
715 |    "metadata": {},
716 |    "source": [
717 |     "<details>\n",
718 |     "<summary>Tracking Evaluation Results in MLflow Experiment</summary>\n",
719 |     "\n",
720 |     "<br/>\n",
721 |     "\n",
722 |     "To track and visualize the evaluation results over time, you can record the results in MLflow Experiment.\n",
723 |     "\n",
724 |     "\n",
725 |     "```python\n",
726 |     "import mlflow\n",
727 |     "\n",
728 |     "with mlflow.start_run(run_name=\"rag_evaluation\"):\n",
729 |     "    evaluate = dspy.Evaluate(\n",
730 |     "        devset=devset,\n",
731 |     "        metric=metric,\n",
732 |     "        num_threads=24,\n",
733 |     "        display_progress=True,\n",
734 |     "    )\n",
735 |     "\n",
736 |     "    # Evaluate the program as usual\n",
737 |     "    result = evaluate(cot)\n",
738 |     "\n",
739 |     "\n",
740 |     "    # Log the aggregated score\n",
741 |     "    mlflow.log_metric(\"semantic_f1_score\", result.score)\n",
742 |     "    # Log the detailed evaluation results as a table\n",
743 |     "    mlflow.log_table(\n",
744 |     "        {\n",
745 |     "            \"Question\": [example.question for example in eval_set],\n",
746 |     "            \"Gold Response\": [example.response for example in eval_set],\n",
747 |     "            \"Predicted Response\": [output[1] for output in result.results],\n",
748 |     "            \"Semantic F1 Score\": [output[2] for output in result.results],\n",
749 |     "        },\n",
750 |     "        artifact_file=\"eval_results.json\",\n",
751 |     "    )\n",
752 |     "```\n",
753 |     "\n",
754 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
755 |     "\n",
756 |     "</details>"
757 |    ]
758 |   },
759 |   {
760 |    "cell_type": "markdown",
761 |    "metadata": {},
762 |    "source": [
763 |     "So far, we built a very simple chain-of-thought module for question answering and evaluated it on a small dataset.\n",
764 |     "\n",
765 |     "Can we do better? In the rest of this guide, we will build a retrieval-augmented generation (RAG) program in DSPy for the same task. We'll see how this can boost the score substantially, then we'll use one of the DSPy Optimizers to _compile_ our RAG program to higher-quality prompts, raising our scores even more."
766 |    ]
767 |   },
768 |   {
769 |    "cell_type": "markdown",
770 |    "metadata": {},
771 |    "source": [
772 |     "## Basic Retrieval-Augmented Generation (RAG).\n",
773 |     "\n",
774 |     "First, let's download the corpus data that we will use for RAG search. An older version of this tutorial used the full (650,000 document) corpus. To make this very fast and cheap to run, we've downsampled the corpus to just 28,000 documents."
775 |    ]
776 |   },
777 |   {
778 |    "cell_type": "code",
779 |    "execution_count": 12,
780 |    "metadata": {},
781 |    "outputs": [],
782 |    "source": [
783 |     "download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_corpus.jsonl\")"
784 |    ]
785 |   },
786 |   {
787 |    "cell_type": "markdown",
788 |    "metadata": {},
789 |    "source": [
790 |     "## Set up your system's retriever.\n",
791 |     "\n",
792 |     "As far as DSPy is concerned, you can plug in any Python code for calling tools or retrievers. Here, we'll just use OpenAI Embeddings and do top-K search locally, just for convenience.\n",
793 |     "\n",
794 |     "**Note:** The step below will require that you either do `pip install -U faiss-cpu` or pass `brute_force_threshold=30_000` to `dspy.retrievers.Embeddings` to avoid faiss."
795 |    ]
796 |   },
797 |   {
798 |    "cell_type": "code",
799 |    "execution_count": 13,
800 |    "metadata": {},
801 |    "outputs": [],
802 |    "source": [
803 |     "# %pip install -U faiss-cpu  # or faiss-gpu if you have a GPU"
804 |    ]
805 |   },
806 |   {
807 |    "cell_type": "code",
808 |    "execution_count": 14,
809 |    "metadata": {},
810 |    "outputs": [
811 |     {
812 |      "name": "stdout",
813 |      "output_type": "stream",
814 |      "text": [
815 |       "Loaded 28436 documents. Will encode them below.\n",
816 |       "Training a 32-byte FAISS index with 337 partitions, based on 28436 x 512-dim embeddings\n"
817 |      ]
818 |     }
819 |    ],
820 |    "source": [
821 |     "max_characters = 6000  # for truncating >99th percentile of documents\n",
822 |     "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
823 |     "\n",
824 |     "with open(\"ragqa_arena_tech_corpus.jsonl\") as f:\n",
825 |     "    corpus = [ujson.loads(line)['text'][:max_characters] for line in f]\n",
826 |     "    print(f\"Loaded {len(corpus)} documents. Will encode them below.\")\n",
827 |     "\n",
828 |     "embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512)\n",
829 |     "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
830 |    ]
831 |   },
832 |   {
833 |    "cell_type": "markdown",
834 |    "metadata": {},
835 |    "source": [
836 |     "\n",
837 |     "## Build your first RAG Module.\n",
838 |     "\n",
839 |     "In the previous guide, we looked at individual DSPy modules in isolation, e.g. `dspy.Predict(\"question -> answer\")`.\n",
840 |     "\n",
841 |     "What if we want to build a DSPy _program_ that has multiple steps? The syntax below with `dspy.Module` allows you to connect a few pieces together, in this case, our retriever and a generation module, so the whole system can be optimized.\n",
842 |     "\n",
843 |     "Concretely, in the `__init__` method, you declare any sub-module you'll need, which in this case is just a `dspy.ChainOfThought('context, question -> response')` module that takes retrieved context, a question, and produces a response. In the `forward` method, you simply express any Python control flow you like, possibly using your modules. In this case, we first invoke the `search` function defined earlier and then invoke the `self.respond` ChainOfThought module.\n"
844 |    ]
845 |   },
846 |   {
847 |    "cell_type": "code",
848 |    "execution_count": 15,
849 |    "metadata": {},
850 |    "outputs": [],
851 |    "source": [
852 |     "class RAG(dspy.Module):\n",
853 |     "    def __init__(self):\n",
854 |     "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
855 |     "\n",
856 |     "    def forward(self, question):\n",
857 |     "        context = search(question).passages\n",
858 |     "        return self.respond(context=context, question=question)"
859 |    ]
860 |   },
861 |   {
862 |    "cell_type": "markdown",
863 |    "metadata": {},
864 |    "source": [
865 |     "\n",
866 |     "Let's use the RAG module.\n"
867 |    ]
868 |   },
869 |   {
870 |    "cell_type": "code",
871 |    "execution_count": 16,
872 |    "metadata": {},
873 |    "outputs": [
874 |     {
875 |      "data": {
876 |       "text/plain": [
877 |        "Prediction(\n",
878 |        "    reasoning=\"High Memory and Low Memory in Linux refer to two segments of the kernel's memory space. Low Memory is the portion of memory that the kernel can access directly and is statically mapped at boot time. This area is typically used for kernel data structures and is always accessible to the kernel. High Memory, on the other hand, is not permanently mapped in the kernel's address space, meaning that the kernel cannot access it directly without first mapping it into its address space. High Memory is used for user-space applications and temporary data buffers. The distinction allows for better memory management and security, as user-space applications cannot directly access kernel-space memory.\",\n",
879 |        "    response=\"In Linux, High Memory refers to the segment of memory that is not permanently mapped in the kernel's address space, which means the kernel must map it temporarily to access it. This area is typically used for user-space applications and temporary data buffers. Low Memory, in contrast, is the portion of memory that the kernel can access directly and is statically mapped at boot time. It is used for kernel data structures and is always accessible to the kernel. This separation enhances security by preventing user-space applications from accessing kernel-space memory directly.\"\n",
880 |        ")"
881 |       ]
882 |      },
883 |      "execution_count": 16,
884 |      "metadata": {},
885 |      "output_type": "execute_result"
886 |     }
887 |    ],
888 |    "source": [
889 |     "rag = RAG()\n",
890 |     "rag(question=\"what are high memory and low memory on linux?\")"
891 |    ]
892 |   },
893 |   {
894 |    "cell_type": "code",
895 |    "execution_count": 17,
896 |    "metadata": {},
897 |    "outputs": [
898 |     {
899 |      "name": "stdout",
900 |      "output_type": "stream",
901 |      "text": [
902 |       "\n",
903 |       "\n",
904 |       "\n",
905 |       "\n",
906 |       "\u001b[34m[2024-11-23T23:16:49.175612]\u001b[0m\n",
907 |       "\n",
908 |       "\u001b[31mSystem message:\u001b[0m\n",
909 |       "\n",
910 |       "Your input fields are:\n",
911 |       "1. `context` (str)\n",
912 |       "2. `question` (str)\n",
913 |       "\n",
914 |       "Your output fields are:\n",
915 |       "1. `reasoning` (str)\n",
916 |       "2. `response` (str)\n",
917 |       "\n",
[TRUNCATED]
```

tutorials/real_world_examples/index.md
```
1 | # Real-World Examples
2 | 
3 | This section demonstrates practical applications of DSPy across different domains and use cases. Each tutorial shows how to build production-ready AI systems using DSPy's modular programming approach.
4 | 
5 | ## Featured Examples
6 | 
7 | ### 📄 [Generating llms.txt](../llms_txt_generation/index.md)
8 | Learn how to create AI-powered documentation generators that analyze codebases and produce structured, LLM-friendly documentation following the llms.txt standard.
9 | 
10 | **Key Concepts:** Repository analysis, meta-programming, documentation generation
11 | 
12 | ### 📧 [Email Information Extraction](../email_extraction/index.md)
13 | Build intelligent email processing systems that classify messages, extract entities, and identify action items using DSPy's structured prediction capabilities.
14 | 
15 | **Key Concepts:** Information extraction, classification, text processing
16 | 
17 | ### 🧠 [Memory-Enabled ReAct Agents with Mem0](../mem0_react_agent/index.md)
18 | Create conversational agents with persistent memory using DSPy ReAct and Mem0 integration for context-aware interactions across sessions.
19 | 
20 | **Key Concepts:** Memory systems, conversational AI, agent persistence
21 | 
22 | ### 💰 [Financial Analysis with Yahoo Finance](../yahoo_finance_react/index.md)
23 | Develop financial analysis agents that fetch real-time market data, analyze news sentiment, and provide investment insights using LangChain tool integration.
24 | 
25 | **Key Concepts:** Tool integration, financial data, real-time analysis
26 | 
27 | ### 🔄 [Automated Code Generation from Documentation](../sample_code_generation/index.md)
28 | Build a system that automatically fetches documentation from URLs and generates working code examples for any library using DSPy's intelligent analysis.
29 | 
30 | **Key Concepts:** Web scraping, documentation parsing, automated learning, code generation
31 | 
32 | ### 🎮 [Building a Creative Text-Based AI Game](../ai_text_game/index.md)
33 | Create an interactive text-based adventure game with dynamic storytelling, AI-powered NPCs, and adaptive gameplay using DSPy's modular programming approach.
34 | 
35 | **Key Concepts:** Interactive storytelling, game state management, character progression, AI-driven narratives
```

tutorials/rl_ai_program/index.md
```
1 | # Experimental RL Optimization for DSPy
2 | 
3 | This section explores cutting-edge reinforcement learning (RL) approaches for optimizing DSPy programs. These experimental techniques represent the frontier of AI program optimization, combining the power of RL with DSPy's modular programming paradigm to achieve even better performance on complex tasks.
4 | 
5 | ## Advanced RL Optimization Techniques
6 | 
7 | ### [RL for Privacy-Conscious Delegation](../rl_papillon/index.ipynb)
8 | Explore how reinforcement learning can optimize privacy-conscious AI systems. This tutorial demonstrates how RL agents can learn to balance task performance with privacy constraints, making intelligent decisions about when and how to delegate sensitive operations.
9 | 
10 | ### [RL for Multi-Hop Research](../rl_multihop/index.ipynb)
11 | Learn to apply reinforcement learning to multi-hop reasoning tasks. This advanced tutorial shows how RL can optimize the search strategy in complex information retrieval scenarios, learning to navigate through multiple information sources more effectively.
```

tutorials/rl_multihop/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Online RL for Multi-Hop Research\n",
8 |     "\n",
9 |     "WARNING: This feature is new and extremely EXPERIMENTAL. Unlike almost everything else in DSPy, it's currently in pure proof of concept and development mode, but we release it to encourage community involvement.\n",
10 |     "\n",
11 |     "For this tutorial, you will also need DSPy's Arbor RL server.\n",
12 |     "\n",
13 |     "```bash\n",
14 |     "> pip install -U arbor-ai\n",
15 |     "```"
16 |    ]
17 |   },
18 |   {
19 |    "cell_type": "code",
20 |    "execution_count": null,
21 |    "metadata": {},
22 |    "outputs": [],
23 |    "source": [
24 |     "import dspy\n",
25 |     "from dspy.clients.lm_local_arbor import ArborProvider\n",
26 |     "\n",
27 |     "import arbor\n",
28 |     "arbor_server_info = arbor.init() # Initialize the Arbor server in the background\n",
29 |     "\n",
30 |     "port = 7453\n",
31 |     "local_lm_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
32 |     "local_lm = dspy.LM(\n",
33 |     "    model=f\"openai/arbor:{local_lm_name}\",\n",
34 |     "    provider=ArborProvider(),\n",
35 |     "    temperature=0.7,\n",
36 |     "    api_base=arbor_server_info[\"api_base\"],\n",
37 |     ")\n",
38 |     "\n",
39 |     "dspy.configure(lm=local_lm)\n",
40 |     "\n",
41 |     "openai_lm = dspy.LM(model=\"openai/gpt-4.1-mini\")"
42 |    ]
43 |   },
44 |   {
45 |    "cell_type": "markdown",
46 |    "metadata": {},
47 |    "source": [
48 |     "### Install dependencies and download data\n",
49 |     "\n",
50 |     "To do the retrieval, we'll use the cool BM25S library, as it's pretty lightweight. You can replace this components with whatever you like.\n",
51 |     "\n",
52 |     "```shell\n",
53 |     "> pip install -U bm25s PyStemmer \"jax[cpu]\"\n",
54 |     "```\n",
55 |     "\n",
56 |     "Next, we'll download a snapshot abstracts (i.e., first paragraphs) of all 5,000,000 Wikipedia pages as of 2017. We'll use this as our retrieval corpus.\n",
57 |     "\n",
58 |     "This is 500MB compressed, so the download and decompression may take 2-3 minutes.\n",
59 |     "\n",
60 |     "```python\n",
61 |     "from dspy.utils import download\n",
62 |     "\n",
63 |     "download(\"https://huggingface.co/dspy/cache/resolve/main/wiki.abstracts.2017.tar.gz\")\n",
64 |     "!tar -xzvf wiki.abstracts.2017.tar.gz\n",
65 |     "```\n",
66 |     "\n",
67 |     "And then let's index it for BM25 retrieval! This will take 2-3 minutes."
68 |    ]
69 |   },
70 |   {
71 |    "cell_type": "code",
72 |    "execution_count": null,
73 |    "metadata": {},
74 |    "outputs": [],
75 |    "source": [
76 |     "import ujson\n",
77 |     "import bm25s\n",
78 |     "import Stemmer\n",
79 |     "\n",
80 |     "corpus = []\n",
81 |     "\n",
82 |     "with open(\"wiki.abstracts.2017.jsonl\") as f:\n",
83 |     "    for line in f:\n",
84 |     "        line = ujson.loads(line)\n",
85 |     "        corpus.append(f\"{line['title']} | {' '.join(line['text'])}\")\n",
86 |     "\n",
87 |     "stemmer = Stemmer.Stemmer(\"english\")\n",
88 |     "corpus_tokens = bm25s.tokenize(corpus, stopwords=\"en\", stemmer=stemmer)\n",
89 |     "\n",
90 |     "retriever = bm25s.BM25(k1=0.9, b=0.4)\n",
91 |     "retriever.index(corpus_tokens)"
92 |    ]
93 |   },
94 |   {
95 |    "cell_type": "markdown",
96 |    "metadata": {},
97 |    "source": [
98 |     "### Load the HoVer dataset.\n",
99 |     "\n",
100 |     "Let's load a dataset for our task. We'll load examples from the HoVer multi-hop task, where the input is a (really!) complex claim and the output we're seeking is the set of Wikipedia pages that are required to fact-check that claim."
101 |    ]
102 |   },
103 |   {
104 |    "cell_type": "code",
105 |    "execution_count": null,
106 |    "metadata": {},
107 |    "outputs": [],
108 |    "source": [
109 |     "import random\n",
110 |     "from dspy.datasets import DataLoader\n",
111 |     "\n",
112 |     "kwargs = dict(fields=(\"claim\", \"supporting_facts\", \"hpqa_id\", \"num_hops\"), input_keys=(\"claim\",))\n",
113 |     "hover = DataLoader().from_huggingface(dataset_name=\"hover-nlp/hover\", split=\"train\", trust_remote_code=True, **kwargs)\n",
114 |     "\n",
115 |     "hpqa_ids = set()\n",
116 |     "hover = [\n",
117 |     "    dspy.Example(claim=x.claim, titles=list(set([y[\"key\"] for y in x.supporting_facts]))).with_inputs(\"claim\")\n",
118 |     "    for x in hover\n",
119 |     "    if x[\"num_hops\"] == 3 and x[\"hpqa_id\"] not in hpqa_ids and not hpqa_ids.add(x[\"hpqa_id\"])\n",
120 |     "]\n",
121 |     "\n",
122 |     "random.Random(0).shuffle(hover)\n",
123 |     "trainset, devset, testset = hover[:600], hover[600:900], hover[900:]\n",
124 |     "len(trainset), len(devset), len(testset)"
125 |    ]
126 |   },
127 |   {
128 |    "cell_type": "markdown",
129 |    "metadata": {},
130 |    "source": [
131 |     "Now, let's define a function to do the search in Wikipedia. This will use our BM25 index."
132 |    ]
133 |   },
134 |   {
135 |    "cell_type": "code",
136 |    "execution_count": null,
137 |    "metadata": {},
138 |    "outputs": [],
139 |    "source": [
140 |     "def search(query: str, k: int) -> list[str]:\n",
141 |     "    tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
142 |     "    results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
143 |     "    run = {corpus[doc]: float(score) for doc, score in zip(results[0], scores[0])}\n",
144 |     "    return list(run.keys())"
145 |    ]
146 |   },
147 |   {
148 |    "cell_type": "markdown",
149 |    "metadata": {},
150 |    "source": [
151 |     "## A DSPy program for multi-hop research\n",
152 |     "\n",
153 |     "Now, let's define the multi-hop program in DSPy. It's going to be super simple, composed of `generate_query` and `append_notes` modules. We'll define the instructions carefully, though they are typically not necessary."
154 |    ]
155 |   },
156 |   {
157 |    "cell_type": "code",
158 |    "execution_count": null,
159 |    "metadata": {},
160 |    "outputs": [],
161 |    "source": [
162 |     "instr1 = \"\"\"\n",
163 |     "Given a claim and some key facts, generate a follow-up search query to find the next most essential clue towards verifying or refuting the claim. The goal ultimately is to find all documents implicated by the claim.\n",
164 |     "\"\"\".strip()\n",
165 |     "\n",
166 |     "instr2 = \"\"\"\n",
167 |     "Given a claim, some key facts, and new search results, identify any new learnings from the new search results, which will extend the key facts known so far about the whether the claim is true or false. The goal is to ultimately collect all facts that would help us find all documents implicated by the claim.\n",
168 |     "\"\"\"\n",
169 |     "\n",
170 |     "\n",
171 |     "class ResearchHop(dspy.Module):\n",
172 |     "    def __init__(self, num_docs, num_hops):\n",
173 |     "        self.num_docs, self.num_hops = num_docs, num_hops\n",
174 |     "        self.generate_query = dspy.ChainOfThought(dspy.Signature(\"claim, key_facts -> followup_search_query\", instr1))\n",
175 |     "        self.append_notes = dspy.ChainOfThought(dspy.Signature(\"claim, key_facts, new_search_results -> new_key_facts\", instr2))\n",
176 |     "\n",
177 |     "    def forward(self, claim: str) -> list[str]:\n",
178 |     "        key_facts = []\n",
179 |     "        retrieved_docs = []\n",
180 |     "\n",
181 |     "        for hop_idx in range(self.num_hops):\n",
182 |     "            query = self.generate_query(claim=claim, key_facts=key_facts).followup_search_query if hop_idx else claim\n",
183 |     "            search_results = search(query, k=self.num_docs)\n",
184 |     "            retrieved_docs.extend(search_results)\n",
185 |     "\n",
186 |     "            if hop_idx == self.num_hops - 1:\n",
187 |     "                break\n",
188 |     "                \n",
189 |     "            prediction = self.append_notes(claim=claim, key_facts=key_facts, new_search_results=search_results)\n",
190 |     "            key_facts.append(prediction.new_key_facts)\n",
191 |     "\n",
192 |     "        return dspy.Prediction(key_facts=key_facts, retrieved_docs=retrieved_docs)"
193 |    ]
194 |   },
195 |   {
196 |    "cell_type": "markdown",
197 |    "metadata": {},
198 |    "source": [
199 |     "### Define metrics for success in this task"
200 |    ]
201 |   },
202 |   {
203 |    "cell_type": "code",
204 |    "execution_count": null,
205 |    "metadata": {},
206 |    "outputs": [],
207 |    "source": [
208 |     "def recall(example, pred, trace=None):\n",
209 |     "    gold_titles = example.titles\n",
210 |     "    retrieved_titles = [doc.split(\" | \")[0] for doc in pred.retrieved_docs]\n",
211 |     "    return sum(x in retrieved_titles for x in set(gold_titles)) / len(gold_titles)\n",
212 |     "\n",
213 |     "evaluate = dspy.Evaluate(devset=devset, metric=recall, num_threads=16, display_progress=True, display_table=5)"
214 |    ]
215 |   },
216 |   {
217 |    "cell_type": "markdown",
218 |    "metadata": {},
219 |    "source": [
220 |     "## Optimize the `ResearchHop` system with `dspy.GRPO`"
221 |    ]
222 |   },
223 |   {
224 |    "cell_type": "code",
225 |    "execution_count": null,
226 |    "metadata": {},
227 |    "outputs": [],
228 |    "source": [
229 |     "from dspy.teleprompt.grpo import GRPO\n",
230 |     "from dspy.clients.utils_finetune import MultiGPUConfig\n",
231 |     "\n",
232 |     "program = ResearchHop(num_docs=4, num_hops=2)\n",
233 |     "program.set_lm(local_lm)\n",
234 |     "\n",
235 |     "# NOTE: Training on 6 GPUs.\n",
236 |     "train_kwargs = {\n",
237 |     "    \"per_device_train_batch_size\": 2,\n",
238 |     "    \"gradient_accumulation_steps\": 8,\n",
239 |     "    \"temperature\": 1.0,\n",
240 |     "    \"beta\": 0.04,\n",
241 |     "    \"learning_rate\": 1e-5,\n",
242 |     "    \"gradient_checkpointing\": True,\n",
243 |     "    \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
244 |     "    \"bf16\": True,\n",
245 |     "    \"lr_scheduler_type\": \"constant_with_warmup\",\n",
246 |     "    \"max_prompt_length\": None,\n",
247 |     "    \"max_completion_length\": None,\n",
248 |     "    \"scale_rewards\": True,\n",
249 |     "    \"max_grad_norm\": 0.5,\n",
250 |     "    \"lora\": True,\n",
251 |     "}\n",
252 |     "\n",
253 |     "compiler = GRPO(\n",
254 |     "    metric=recall,\n",
255 |     "    num_dspy_examples_per_grpo_step=6,\n",
256 |     "    num_rollouts_per_grpo_step=4,\n",
257 |     "    exclude_demos=True,\n",
258 |     "    num_train_steps=100,\n",
259 |     "    num_threads=16,\n",
260 |     "    use_train_as_val=False,\n",
261 |     "    num_steps_for_val=10,\n",
262 |     "    train_kwargs=train_kwargs,\n",
263 |     "    report_train_scores=False,\n",
264 |     "    gpu_config=MultiGPUConfig(num_inference_gpus=1, num_training_gpus=1),\n",
265 |     ")\n",
266 |     "\n",
267 |     "optimized_program = compiler.compile(\n",
268 |     "    student=program,\n",
269 |     "    trainset=trainset,\n",
270 |     "    valset=devset,\n",
271 |     ")\n"
272 |    ]
273 |   },
274 |   {
275 |    "cell_type": "markdown",
276 |    "metadata": {},
277 |    "source": [
278 |     "Now, you can use the GRPO'ed program."
279 |    ]
280 |   },
281 |   {
282 |    "cell_type": "code",
283 |    "execution_count": null,
284 |    "metadata": {},
285 |    "outputs": [],
286 |    "source": [
287 |     "example = devset[0]\n",
288 |     "optimized_program(**example.inputs())"
289 |    ]
290 |   },
291 |   {
292 |    "cell_type": "markdown",
293 |    "metadata": {},
294 |    "source": [
295 |     "In our preliminary experiments, training above for about 18 hours boosts the recall (devset) from 61.8% to 66.2%. This is _typically_ worse on cost/quality basis than you'd get from running prompt optimizers dspy.MIPROv2 or dspy.SIMBA, but it's still a very solid start for online RL over arbitrary LM programs for small LMs."
296 |    ]
297 |   }
298 |  ],
299 |  "metadata": {
300 |   "kernelspec": {
301 |    "display_name": "jun2024_py310",
302 |    "language": "python",
303 |    "name": "python3"
304 |   },
305 |   "language_info": {
306 |    "codemirror_mode": {
307 |     "name": "ipython",
308 |     "version": 3
309 |    },
310 |    "file_extension": ".py",
311 |    "mimetype": "text/x-python",
312 |    "name": "python",
313 |    "nbconvert_exporter": "python",
314 |    "pygments_lexer": "ipython3",
315 |    "version": "3.10.14"
316 |   }
317 |  },
318 |  "nbformat": 4,
319 |  "nbformat_minor": 2
320 | }
```

tutorials/rl_papillon/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Online RL over a Multi-Module DSPy Program\n",
8 |     "\n",
9 |     "WARNING: This feature is new and extremely EXPERIMENTAL. Unlike almost everything else in DSPy, it's currently in pure proof of concept and development mode, but we release it to encourage community involvement.\n",
10 |     "\n",
11 |     "In this tutorial, we optimize the LM weights of [PAPILLON](https://dspy.ai/tutorials/papillon/) with `dspy.GRPO`, a generalization of the popular GRPO online RL algorithm of LLMs to sophisticated multi-module LM programs.\n",
12 |     "\n",
13 |     "PAPILLON is a system for privacy-preserving delegation, where we will teach a tiny model (1.7B parameters) to use an \"untrusted\" external LLM, which is more powerful but may save your private data, to balance high-quality and private chat.\n",
14 |     "\n",
15 |     "For this tutorial, you will also need the Arbor RL server.\n",
16 |     "\n",
17 |     "```bash\n",
18 |     "> pip install -U arbor-ai\n",
19 |     "```"
20 |    ]
21 |   },
22 |   {
23 |    "cell_type": "code",
24 |    "execution_count": null,
25 |    "metadata": {},
26 |    "outputs": [],
27 |    "source": [
28 |     "import dspy\n",
29 |     "from dspy.clients.lm_local_arbor import ArborProvider\n",
30 |     "\n",
31 |     "import arbor\n",
32 |     "arbor_server_info = arbor.init() # Initialize the Arbor server in the background\n",
33 |     "\n",
34 |     "port = 7453\n",
35 |     "local_lm_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
36 |     "local_lm = dspy.LM(\n",
37 |     "    model=f\"openai/arbor:{local_lm_name}\",\n",
38 |     "    provider=ArborProvider(),\n",
39 |     "    temperature=0.7,\n",
40 |     "    api_base=arbor_server_info[\"api_base\"],\n",
41 |     ")\n",
42 |     "\n",
43 |     "dspy.configure(lm=local_lm)\n",
44 |     "\n",
45 |     "openai_lm = dspy.LM(model=\"openai/gpt-4.1-mini\")"
46 |    ]
47 |   },
48 |   {
49 |    "cell_type": "code",
50 |    "execution_count": null,
51 |    "metadata": {},
52 |    "outputs": [],
53 |    "source": [
54 |     "class CraftRedactedRequest(dspy.Signature):\n",
55 |     "    \"\"\"\n",
56 |     "    Given a private user query, create a privacy-preserving request for a powerful external LLM.\n",
57 |     "    The LLM may assist without learning private information about the user.\n",
58 |     "    \"\"\"\n",
59 |     "\n",
60 |     "    user_query = dspy.InputField()\n",
61 |     "    llm_request = dspy.OutputField()\n",
62 |     "\n",
63 |     "\n",
64 |     "class RespondToQuery(dspy.Signature):\n",
65 |     "    \"\"\"\n",
66 |     "    Respond to a user query.\n",
67 |     "    For inspiration, we found a potentially related request to a powerful external LLM and its response.\n",
68 |     "    \"\"\"\n",
69 |     "\n",
70 |     "    related_llm_request = dspy.InputField()\n",
71 |     "    related_llm_response = dspy.InputField(desc=\"information from a powerful LLM responding to a related request\")\n",
72 |     "    user_query = dspy.InputField(desc=\"the user's request you need to fulfill\")\n",
73 |     "    response = dspy.OutputField(desc=\"your final response to the user's request\")\n",
74 |     "\n",
75 |     "\n",
76 |     "class PAPILLON(dspy.Module):\n",
77 |     "    def __init__(self, untrusted_model):\n",
78 |     "        self.craft_redacted_request = dspy.ChainOfThought(CraftRedactedRequest)\n",
79 |     "        self.respond_to_query = dspy.Predict(RespondToQuery)\n",
80 |     "        self.untrusted_model = untrusted_model\n",
81 |     "\n",
82 |     "    def forward(self, user_query):\n",
83 |     "        try:\n",
84 |     "            llm_request = self.craft_redacted_request(user_query=user_query).llm_request\n",
85 |     "            llm_response = self.untrusted_model(llm_request)[0]\n",
86 |     "            response = self.respond_to_query(\n",
87 |     "                related_llm_request=llm_request, related_llm_response=llm_response, user_query=user_query\n",
88 |     "            ).response\n",
89 |     "        except Exception:\n",
90 |     "            return dspy.Prediction(llm_request=\"\", llm_response=\"\", response=\"\")\n",
91 |     "\n",
92 |     "        return dspy.Prediction(llm_request=llm_request, llm_response=llm_response, response=response)"
93 |    ]
94 |   },
95 |   {
96 |    "cell_type": "code",
97 |    "execution_count": null,
98 |    "metadata": {},
99 |    "outputs": [],
100 |    "source": [
101 |     "from datasets import load_dataset\n",
102 |     "\n",
103 |     "pupa_tnb = load_dataset(\"Columbia-NLP/PUPA\", \"pupa_tnb\")\n",
104 |     "pupa_new = load_dataset(\"Columbia-NLP/PUPA\", \"pupa_new\")\n",
105 |     "\n",
106 |     "examples = [\n",
107 |     "    dspy.Example(\n",
108 |     "        {\"target_response\": x[\"target_response\"], \"user_query\": x[\"user_query\"], \"pii_str\": x[\"pii_units\"]}\n",
109 |     "    ).with_inputs(\"user_query\")\n",
110 |     "    for x in pupa_new[\"train\"]\n",
111 |     "]\n",
112 |     "\n",
113 |     "trainset, devset, testset = examples[:225], examples[225:450], examples[450:]\n",
114 |     "print(f\"Loaded {len(trainset)} training examples, {len(devset)} dev examples, and {len(testset)} test examples.\")"
115 |    ]
116 |   },
117 |   {
118 |    "cell_type": "markdown",
119 |    "metadata": {},
120 |    "source": [
121 |     "### Define metrics for success in this task\n",
122 |     "\n",
123 |     "What does it mean for a PAPILLON system to be successful?\n",
124 |     "\n",
125 |     "1. The responses of the local model should be as good as (or better than) the `target_response` from a large LM.\n",
126 |     "2. The local model should leak as few `pii_units` to the remote model as possible.\n",
127 |     "\n",
128 |     "For benchmarking, we will judge both of these using our `openai_lm` and the annotation in PUPA."
129 |    ]
130 |   },
131 |   {
132 |    "cell_type": "code",
133 |    "execution_count": null,
134 |    "metadata": {},
135 |    "outputs": [],
136 |    "source": [
137 |     "class JudgeQuality(dspy.Signature):\n",
138 |     "    \"\"\"\n",
139 |     "    You are comparing the quality of two responses, given a user query.\n",
140 |     "    Is response A at least as good as response B?\n",
141 |     "    \"\"\"\n",
142 |     "\n",
143 |     "    user_query = dspy.InputField(desc=\"The user's request to be fulfilled.\")\n",
144 |     "    response_A = dspy.InputField()\n",
145 |     "    response_B = dspy.InputField()\n",
146 |     "    judgment: bool = dspy.OutputField()\n",
147 |     "\n",
148 |     "\n",
149 |     "class JudgeLeakage(dspy.Signature):\n",
150 |     "    \"\"\"\n",
151 |     "    Count the number information pieces in `pii` that (i) are forms of PII *and* (ii) are leaked into the prompt given.\n",
152 |     "    Don't count the cases where the leaked information is just a celebrity name.\n",
153 |     "    Don't count the cases where the `pii` information does not appear in the prompt.\n",
154 |     "    \"\"\"\n",
155 |     "\n",
156 |     "    pii = dspy.InputField()\n",
157 |     "    prompt = dspy.InputField()\n",
158 |     "    num_pii_leaked: int = dspy.OutputField()\n",
159 |     "\n",
160 |     "\n",
161 |     "class LLMJudge(dspy.Module):\n",
162 |     "    def __init__(self):\n",
163 |     "        self.quality_judge = dspy.ChainOfThought(JudgeQuality)\n",
164 |     "        self.fact_checker = dspy.ChainOfThought(JudgeLeakage)\n",
165 |     "\n",
166 |     "    def forward(self, user_query, og_resp, new_resp=None, updated_query=None, pii_str=None):\n",
167 |     "        judgment_1 = self.quality_judge(user_query=user_query, response_A=new_resp, response_B=og_resp).judgment\n",
168 |     "        judgment_2 = self.quality_judge(user_query=user_query, response_A=og_resp, response_B=new_resp).judgment\n",
169 |     "        judgment = judgment_1 or (judgment_1 == judgment_2)  # True if better or if judge is inconsistent\n",
170 |     "\n",
171 |     "        pii = list(set(pii_str.split(\"||\")))  # The pii_str field must be separated by `||`\n",
172 |     "        pii_score = self.fact_checker(pii=pii, prompt=updated_query).num_pii_leaked\n",
173 |     "        pii_score = pii_score / len(pii) if len(pii) > 0 else 0\n",
174 |     "\n",
175 |     "        return dspy.Prediction(quality=judgment, leakage=pii_score)\n",
176 |     "\n",
177 |     "\n",
178 |     "llm_judge = LLMJudge()\n",
179 |     "llm_judge.set_lm(openai_lm)"
180 |    ]
181 |   },
182 |   {
183 |    "cell_type": "markdown",
184 |    "metadata": {},
185 |    "source": [
186 |     "With these judges, we can now define the metrics for optimization and for evaluation."
187 |    ]
188 |   },
189 |   {
190 |    "cell_type": "code",
191 |    "execution_count": null,
192 |    "metadata": {},
193 |    "outputs": [],
194 |    "source": [
195 |     "def compute_metrics(gold, pred, trace=None):\n",
196 |     "    return llm_judge(\n",
197 |     "        user_query=gold.user_query,\n",
198 |     "        new_resp=pred.response,\n",
199 |     "        og_resp=gold.target_response,\n",
200 |     "        updated_query=pred.llm_request,\n",
201 |     "        pii_str=gold.pii_str,\n",
202 |     "    )\n",
203 |     "\n",
204 |     "\n",
205 |     "def compute_quality(gold, pred, trace=None):\n",
206 |     "    return compute_metrics(gold, pred, trace).quality\n",
207 |     "\n",
208 |     "\n",
209 |     "def compute_leakage(gold, pred, trace=None):\n",
210 |     "    return compute_metrics(gold, pred, trace).leakage\n",
211 |     "\n",
212 |     "\n",
213 |     "def compute_overall_score(gold, pred, trace=None):\n",
214 |     "    metrics = compute_metrics(gold, pred, trace)\n",
215 |     "    overall_score = (metrics.quality + (1 - metrics.leakage)) / 2.0\n",
216 |     "    return overall_score >= 1.0 if trace is not None else overall_score"
217 |    ]
218 |   },
219 |   {
220 |    "cell_type": "markdown",
221 |    "metadata": {},
222 |    "source": [
223 |     "### Evaluate zero-shot PAPILLON\n",
224 |     "\n",
225 |     "Let's now use the PUPA data and the judges above to evaluate the zero-shot version of our PAPILLON pipeline!"
226 |    ]
227 |   },
228 |   {
229 |    "cell_type": "code",
230 |    "execution_count": null,
231 |    "metadata": {},
232 |    "outputs": [],
233 |    "source": [
234 |     "zeroshot = PAPILLON(untrusted_model=openai_lm)\n",
235 |     "\n",
236 |     "kwargs = dict(num_threads=16, display_progress=True, display_table=5, max_errors=100)\n",
237 |     "evaluate = dspy.Evaluate(metric=compute_overall_score, devset=devset, **kwargs)\n",
238 |     "evaluate(zeroshot)"
239 |    ]
240 |   },
241 |   {
242 |    "cell_type": "markdown",
243 |    "metadata": {},
244 |    "source": [
245 |     "### Optimize PAPILLON with `dspy.GRPO`\n",
246 |     "\n",
247 |     "Let's run the `dspy.GRPO` optimizer to maximize the `compute_overall_score` metric above for our PAPILLON pipeline.\n",
248 |     "\n",
249 |     "We ran this on 4xH100 GPUs for a couple of hours. But first, you'll need to set up Arbor (as above)."
250 |    ]
251 |   },
252 |   {
253 |    "cell_type": "code",
254 |    "execution_count": null,
255 |    "metadata": {},
256 |    "outputs": [],
257 |    "source": [
258 |     "from dspy.teleprompt.grpo import GRPO\n",
259 |     "from dspy.clients.utils_finetune import MultiGPUConfig\n",
260 |     "\n",
261 |     "papillon = PAPILLON(untrusted_model=openai_lm)\n",
262 |     "papillon.set_lm(local_lm)\n",
263 |     "\n",
264 |     "# NOTE: Training on 3 GPUs.\n",
265 |     "train_kwargs = {\n",
266 |     "    \"per_device_train_batch_size\": 8,\n",
267 |     "    \"gradient_accumulation_steps\": 4,\n",
268 |     "    \"temperature\": 1.0,\n",
269 |     "    \"beta\": 0.04,\n",
270 |     "    \"learning_rate\": 2e-6,\n",
271 |     "    \"gradient_checkpointing\": True,\n",
272 |     "    \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
273 |     "    \"bf16\": True,\n",
274 |     "    \"lr_scheduler_type\": \"constant_with_warmup\",\n",
275 |     "    \"max_prompt_length\": None,\n",
276 |     "    \"max_completion_length\": None,\n",
277 |     "    \"scale_rewards\": True,\n",
278 |     "    \"max_grad_norm\": 0.5,\n",
279 |     "    \"lora\": True,\n",
280 |     "}\n",
281 |     "\n",
282 |     "compiler = GRPO(\n",
283 |     "    metric=compute_overall_score,\n",
284 |     "    multitask=True,\n",
285 |     "    num_dspy_examples_per_grpo_step=4,\n",
286 |     "    num_samples_per_input=8,\n",
287 |     "    exclude_demos=True,\n",
288 |     "    num_train_steps=500,\n",
289 |     "    num_threads=24,\n",
290 |     "    use_train_as_val=False,\n",
291 |     "    num_steps_for_val=10,\n",
292 |     "    train_kwargs=train_kwargs,\n",
293 |     "    report_train_scores=False,\n",
294 |     "    gpu_config=MultiGPUConfig(num_inference_gpus=2, num_training_gpus=2),\n",
295 |     ")\n",
296 |     "\n",
297 |     "optimized_papillon = compiler.compile(\n",
298 |     "    student=papillon,\n",
299 |     "    trainset=trainset,\n",
300 |     "    valset=devset,\n",
301 |     ")\n"
302 |    ]
303 |   },
304 |   {
305 |    "cell_type": "markdown",
306 |    "metadata": {},
307 |    "source": [
308 |     "Now, you can use the GRPO'ed program."
309 |    ]
310 |   },
311 |   {
312 |    "cell_type": "code",
313 |    "execution_count": null,
314 |    "metadata": {},
315 |    "outputs": [],
316 |    "source": [
317 |     "example = devset[0]\n",
318 |     "optimized_papillon(**example.inputs())"
319 |    ]
320 |   },
321 |   {
322 |    "cell_type": "markdown",
323 |    "metadata": {},
324 |    "source": [
325 |     "In our preliminary experiments, training above for three hours boosts the composite score (devset) from 54.6% to 60.0%. This is _typically_ worse on cost/quality basis than you'd get from running prompt optimizers like dspy.MIPROv2 or dspy.SIMBA, but it's still a very solid start for online RL over arbitrary LM programs for tiny LMs."
326 |    ]
327 |   },
328 |   {
329 |    "cell_type": "markdown",
330 |    "metadata": {},
331 |    "source": []
332 |   }
333 |  ],
334 |  "metadata": {
335 |   "kernelspec": {
336 |    "display_name": "jun2024_py310",
337 |    "language": "python",
338 |    "name": "python3"
339 |   },
340 |   "language_info": {
341 |    "codemirror_mode": {
342 |     "name": "ipython",
343 |     "version": 3
344 |    },
345 |    "file_extension": ".py",
346 |    "mimetype": "text/x-python",
347 |    "name": "python",
348 |    "nbconvert_exporter": "python",
349 |    "pygments_lexer": "ipython3",
350 |    "version": "3.10.14"
351 |   }
352 |  },
353 |  "nbformat": 4,
354 |  "nbformat_minor": 2
355 | }
```

tutorials/sample_code_generation/index.md
```
1 | # Automated Code Generation from Documentation with DSPy
2 | 
3 | This tutorial demonstrates how to use DSPy to automatically fetch documentation from URLs and generate working code examples for any library. The system can analyze documentation websites, extract key concepts, and produce tailored code examples.
4 | 
5 | ## What You'll Build
6 | 
7 | A documentation-powered code generation system that:
8 | 
9 | - Fetches and parses documentation from multiple URLs
10 | - Extracts API patterns, methods, and usage examples  
11 | - Generates working code for specific use cases
12 | - Provides explanations and best practices
13 | - Works with any library's documentation
14 | 
15 | ## Setup
16 | 
17 | ```bash
18 | pip install dspy requests beautifulsoup4 html2text
19 | ```
20 | 
21 | ## Step 1: Documentation Fetching and Processing
22 | 
23 | ```python
24 | import dspy
25 | import requests
26 | from bs4 import BeautifulSoup
27 | import html2text
28 | from typing import List, Dict, Any
29 | import json
30 | from urllib.parse import urljoin, urlparse
31 | import time
32 | 
33 | # Configure DSPy
34 | lm = dspy.LM(model='openai/gpt-4o-mini')
35 | dspy.configure(lm=lm)
36 | 
37 | class DocumentationFetcher:
38 |     """Fetches and processes documentation from URLs."""
39 |     
40 |     def __init__(self, max_retries=3, delay=1):
41 |         self.session = requests.Session()
42 |         self.session.headers.update({
43 |             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
44 |         })
45 |         self.max_retries = max_retries
46 |         self.delay = delay
47 |         self.html_converter = html2text.HTML2Text()
48 |         self.html_converter.ignore_links = False
49 |         self.html_converter.ignore_images = True
50 |     
51 |     def fetch_url(self, url: str) -> dict[str, str]:
52 |         """Fetch content from a single URL."""
53 |         for attempt in range(self.max_retries):
54 |             try:
55 |                 print(f"📡 Fetching: {url} (attempt {attempt + 1})")
56 |                 response = self.session.get(url, timeout=10)
57 |                 response.raise_for_status()
58 |                 
59 |                 soup = BeautifulSoup(response.content, 'html.parser')
60 |                 
61 |                 # Remove script and style elements
62 |                 for script in soup(["script", "style", "nav", "footer", "header"]):
63 |                     script.decompose()
64 |                 
65 |                 # Convert to markdown for better LLM processing
66 |                 markdown_content = self.html_converter.handle(str(soup))
67 |                 
68 |                 return {
69 |                     "url": url,
70 |                     "title": soup.title.string if soup.title else "No title",
71 |                     "content": markdown_content,
72 |                     "success": True
73 |                 }
74 |                 
75 |             except Exception as e:
76 |                 print(f"❌ Error fetching {url}: {e}")
77 |                 if attempt < self.max_retries - 1:
78 |                     time.sleep(self.delay)
79 |                 else:
80 |                     return {
81 |                         "url": url,
82 |                         "title": "Failed to fetch",
83 |                         "content": f"Error: {str(e)}",
84 |                         "success": False
85 |                     }
86 |         
87 |         return {"url": url, "title": "Failed", "content": "", "success": False}
88 |     
89 |     def fetch_documentation(self, urls: list[str]) -> list[dict[str, str]]:
90 |         """Fetch documentation from multiple URLs."""
91 |         results = []
92 |         
93 |         for url in urls:
94 |             result = self.fetch_url(url)
95 |             results.append(result)
96 |             time.sleep(self.delay)  # Be respectful to servers
97 |         
98 |         return results
99 | 
100 | class LibraryAnalyzer(dspy.Signature):
101 |     """Analyze library documentation to understand core concepts and patterns."""
102 |     library_name: str = dspy.InputField(desc="Name of the library to analyze")
103 |     documentation_content: str = dspy.InputField(desc="Combined documentation content")
104 |     
105 |     core_concepts: list[str] = dspy.OutputField(desc="Main concepts and components")
106 |     common_patterns: list[str] = dspy.OutputField(desc="Common usage patterns")
107 |     key_methods: list[str] = dspy.OutputField(desc="Important methods and functions")
108 |     installation_info: str = dspy.OutputField(desc="Installation and setup information")
109 |     code_examples: list[str] = dspy.OutputField(desc="Example code snippets found")
110 | 
111 | class CodeGenerator(dspy.Signature):
112 |     """Generate code examples for specific use cases using the target library."""
113 |     library_info: str = dspy.InputField(desc="Library concepts and patterns")
114 |     use_case: str = dspy.InputField(desc="Specific use case to implement")
115 |     requirements: str = dspy.InputField(desc="Additional requirements or constraints")
116 |     
117 |     code_example: str = dspy.OutputField(desc="Complete, working code example")
118 |     explanation: str = dspy.OutputField(desc="Step-by-step explanation of the code")
119 |     best_practices: list[str] = dspy.OutputField(desc="Best practices and tips")
120 |     imports_needed: list[str] = dspy.OutputField(desc="Required imports and dependencies")
121 | 
122 | class DocumentationLearningAgent(dspy.Module):
123 |     """Agent that learns from documentation URLs and generates code examples."""
124 |     
125 |     def __init__(self):
126 |         super().__init__()
127 |         self.fetcher = DocumentationFetcher()
128 |         self.analyze_docs = dspy.ChainOfThought(LibraryAnalyzer)
129 |         self.generate_code = dspy.ChainOfThought(CodeGenerator)
130 |         self.refine_code = dspy.ChainOfThought(
131 |             "code, feedback -> improved_code: str, changes_made: list[str]"
132 |         )
133 |     
134 |     def learn_from_urls(self, library_name: str, doc_urls: list[str]) -> Dict:
135 |         """Learn about a library from its documentation URLs."""
136 |         
137 |         print(f"📚 Learning about {library_name} from {len(doc_urls)} URLs...")
138 |         
139 |         # Fetch all documentation
140 |         docs = self.fetcher.fetch_documentation(doc_urls)
141 |         
142 |         # Combine successful fetches
143 |         combined_content = "\n\n---\n\n".join([
144 |             f"URL: {doc['url']}\nTitle: {doc['title']}\n\n{doc['content']}"
145 |             for doc in docs if doc['success']
146 |         ])
147 |         
148 |         if not combined_content:
149 |             raise ValueError("No documentation could be fetched successfully")
150 |         
151 |         # Analyze combined documentation
152 |         analysis = self.analyze_docs(
153 |             library_name=library_name,
154 |             documentation_content=combined_content
155 |         )
156 |         
157 |         return {
158 |             "library": library_name,
159 |             "source_urls": [doc['url'] for doc in docs if doc['success']],
160 |             "core_concepts": analysis.core_concepts,
161 |             "patterns": analysis.common_patterns,
162 |             "methods": analysis.key_methods,
163 |             "installation": analysis.installation_info,
164 |             "examples": analysis.code_examples,
165 |             "fetched_docs": docs
166 |         }
167 |     
168 |     def generate_example(self, library_info: Dict, use_case: str, requirements: str = "") -> Dict:
169 |         """Generate a code example for a specific use case."""
170 |         
171 |         # Format library information for the generator
172 |         info_text = f"""
173 |         Library: {library_info['library']}
174 |         Core Concepts: {', '.join(library_info['core_concepts'])}
175 |         Common Patterns: {', '.join(library_info['patterns'])}
176 |         Key Methods: {', '.join(library_info['methods'])}
177 |         Installation: {library_info['installation']}
178 |         Example Code Snippets: {'; '.join(library_info['examples'][:3])}  # First 3 examples
179 |         """
180 |         
181 |         code_result = self.generate_code(
182 |             library_info=info_text,
183 |             use_case=use_case,
184 |             requirements=requirements
185 |         )
186 |         
187 |         return {
188 |             "code": code_result.code_example,
189 |             "explanation": code_result.explanation,
190 |             "best_practices": code_result.best_practices,
191 |             "imports": code_result.imports_needed
192 |         }
193 | 
194 | # Initialize the learning agent
195 | agent = DocumentationLearningAgent()
196 | ```
197 | 
198 | ## Step 2: Learning from Documentation URLs
199 | 
200 | ```python
201 | def learn_library_from_urls(library_name: str, documentation_urls: list[str]) -> Dict:
202 |     """Learn about any library from its documentation URLs."""
203 |     
204 |     try:
205 |         library_info = agent.learn_from_urls(library_name, documentation_urls)
206 |         
207 |         print(f"\n🔍 Library Analysis Results for {library_name}:")
208 |         print(f"Sources: {len(library_info['source_urls'])} successful fetches")
209 |         print(f"Core Concepts: {library_info['core_concepts']}")
210 |         print(f"Common Patterns: {library_info['patterns']}")
211 |         print(f"Key Methods: {library_info['methods']}")
212 |         print(f"Installation: {library_info['installation']}")
213 |         print(f"Found {len(library_info['examples'])} code examples")
214 |         
215 |         return library_info
216 |         
217 |     except Exception as e:
218 |         print(f"❌ Error learning library: {e}")
219 |         raise
220 | 
221 | # Example 1: Learn FastAPI from official documentation
222 | fastapi_urls = [
223 |     "https://fastapi.tiangolo.com/",
224 |     "https://fastapi.tiangolo.com/tutorial/first-steps/",
225 |     "https://fastapi.tiangolo.com/tutorial/path-params/",
226 |     "https://fastapi.tiangolo.com/tutorial/query-params/"
227 | ]
228 | 
229 | print("🚀 Learning FastAPI from official documentation...")
230 | fastapi_info = learn_library_from_urls("FastAPI", fastapi_urls)
231 | 
232 | # Example 2: Learn a different library (you can replace with any library)
233 | streamlit_urls = [
234 |     "https://docs.streamlit.io/",
235 |     "https://docs.streamlit.io/get-started",
236 |     "https://docs.streamlit.io/develop/api-reference"
237 | ]
238 | 
239 | print("\n\n📊 Learning Streamlit from official documentation...")
240 | streamlit_info = learn_library_from_urls("Streamlit", streamlit_urls)
241 | ```
242 | 
243 | ## Step 3: Generating Code Examples
244 | 
245 | ```python
246 | def generate_examples_for_library(library_info: Dict, library_name: str):
247 |     """Generate code examples for any library based on its documentation."""
248 |     
249 |     # Define generic use cases that can apply to most libraries
250 |     use_cases = [
251 |         {
252 |             "name": "Basic Setup and Hello World",
253 |             "description": f"Create a minimal working example with {library_name}",
254 |             "requirements": "Include installation, imports, and basic usage"
255 |         },
256 |         {
257 |             "name": "Common Operations",
258 |             "description": f"Demonstrate the most common {library_name} operations",
259 |             "requirements": "Show typical workflow and best practices"
260 |         },
261 |         {
262 |             "name": "Advanced Usage",
263 |             "description": f"Create a more complex example showcasing {library_name} capabilities",
264 |             "requirements": "Include error handling and optimization"
265 |         }
266 |     ]
267 |     
268 |     generated_examples = []
269 |     
270 |     print(f"\n🔧 Generating examples for {library_name}...")
271 |     
272 |     for use_case in use_cases:
273 |         print(f"\n📝 {use_case['name']}")
274 |         print(f"Description: {use_case['description']}")
275 |         
276 |         example = agent.generate_example(
277 |             library_info=library_info,
278 |             use_case=use_case['description'],
279 |             requirements=use_case['requirements']
280 |         )
281 |         
282 |         print("\n💻 Generated Code:")
283 |         print("```python")
284 |         print(example['code'])
285 |         print("```")
286 |         
287 |         print("\n📦 Required Imports:")
288 |         for imp in example['imports']:
289 |             print(f"  • {imp}")
290 |         
291 |         print("\n📝 Explanation:")
292 |         print(example['explanation'])
293 |         
294 |         print("\n✅ Best Practices:")
295 |         for practice in example['best_practices']:
296 |             print(f"  • {practice}")
297 |         
298 |         generated_examples.append({
299 |             "use_case": use_case['name'],
300 |             "code": example['code'],
301 |             "imports": example['imports'],
302 |             "explanation": example['explanation'],
303 |             "best_practices": example['best_practices']
304 |         })
305 |         
306 |         print("-" * 80)
307 |     
308 |     return generated_examples
309 | 
310 | # Generate examples for both libraries
311 | print("🎯 Generating FastAPI Examples:")
312 | fastapi_examples = generate_examples_for_library(fastapi_info, "FastAPI")
313 | 
314 | print("\n\n🎯 Generating Streamlit Examples:")
315 | streamlit_examples = generate_examples_for_library(streamlit_info, "Streamlit")
316 | ```
317 | 
318 | ## Step 4: Interactive Library Learning Function
319 | 
320 | ```python
321 | def learn_any_library(library_name: str, documentation_urls: list[str], use_cases: list[str] = None):
322 |     """Learn any library from its documentation and generate examples."""
323 |     
324 |     if use_cases is None:
325 |         use_cases = [
326 |             "Basic setup and hello world example",
327 |             "Common operations and workflows",
328 |             "Advanced usage with best practices"
329 |         ]
330 |     
331 |     print(f"🚀 Starting automated learning for {library_name}...")
332 |     print(f"Documentation sources: {len(documentation_urls)} URLs")
333 |     
334 |     try:
335 |         # Step 1: Learn from documentation
336 |         library_info = agent.learn_from_urls(library_name, documentation_urls)
337 |         
338 |         # Step 2: Generate examples for each use case
339 |         all_examples = []
340 |         
341 |         for i, use_case in enumerate(use_cases, 1):
342 |             print(f"\n📝 Generating example {i}/{len(use_cases)}: {use_case}")
343 |             
344 |             example = agent.generate_example(
345 |                 library_info=library_info,
346 |                 use_case=use_case,
347 |                 requirements="Include error handling, comments, and follow best practices"
348 |             )
349 |             
350 |             all_examples.append({
351 |                 "use_case": use_case,
352 |                 "code": example['code'],
353 |                 "imports": example['imports'],
354 |                 "explanation": example['explanation'],
355 |                 "best_practices": example['best_practices']
356 |             })
357 |         
358 |         return {
359 |             "library_info": library_info,
360 |             "examples": all_examples
361 |         }
362 |     
363 |     except Exception as e:
364 |         print(f"❌ Error learning {library_name}: {e}")
365 |         return None
366 | 
367 | def interactive_learning_session():
368 |     """Interactive session for learning libraries with user input."""
369 |     
370 |     print("🎯 Welcome to the Interactive Library Learning System!")
371 |     print("This system will help you learn any Python library from its documentation.\n")
372 |     
373 |     learned_libraries = {}
374 |     
375 |     while True:
376 |         print("\n" + "="*60)
377 |         print("🚀 LIBRARY LEARNING SESSION")
378 |         print("="*60)
379 |         
380 |         # Get library name from user
381 |         library_name = input("\n📚 Enter the library name you want to learn (or 'quit' to exit): ").strip()
382 |         
383 |         if library_name.lower() in ['quit', 'exit', 'q']:
384 |             print("\n👋 Thanks for using the Interactive Library Learning System!")
385 |             break
386 |         
387 |         if not library_name:
388 |             print("❌ Please enter a valid library name.")
389 |             continue
390 |         
391 |         # Get documentation URLs
392 |         print(f"\n🔗 Enter documentation URLs for {library_name} (one per line, empty line to finish):")
393 |         urls = []
394 |         while True:
395 |             url = input("  URL: ").strip()
396 |             if not url:
397 |                 break
398 |             if not url.startswith(('http://', 'https://')):
399 |                 print("    ⚠️  Please enter a valid URL starting with http:// or https://")
400 |                 continue
401 |             urls.append(url)
402 |         
403 |         if not urls:
404 |             print("❌ No valid URLs provided. Skipping this library.")
405 |             continue
406 |         
407 |         # Get custom use cases from user
408 |         print(f"\n🎯 Define use cases for {library_name} (optional, press Enter for defaults):")
409 |         print("   Default use cases will be: Basic setup, Common operations, Advanced usage")
410 |         
411 |         user_wants_custom = input("   Do you want to define custom use cases? (y/n): ").strip().lower()
412 |         
413 |         use_cases = None
414 |         if user_wants_custom in ['y', 'yes']:
415 |             print("   Enter your use cases (one per line, empty line to finish):")
416 |             use_cases = []
417 |             while True:
418 |                 use_case = input("     Use case: ").strip()
419 |                 if not use_case:
420 |                     break
421 |                 use_cases.append(use_case)
422 |             
423 |             if not use_cases:
424 |                 print("   No custom use cases provided, using defaults.")
425 |                 use_cases = None
426 |         
427 |         # Learn the library
428 |         print(f"\n🚀 Starting learning process for {library_name}...")
429 |         result = learn_any_library(library_name, urls, use_cases)
430 |         
431 |         if result:
432 |             learned_libraries[library_name] = result
433 |             print(f"\n✅ Successfully learned {library_name}!")
434 |             
435 |             # Show summary
436 |             print(f"\n📊 Learning Summary for {library_name}:")
437 |             print(f"   • Core concepts: {len(result['library_info']['core_concepts'])} identified")
438 |             print(f"   • Common patterns: {len(result['library_info']['patterns'])} found")
439 |             print(f"   • Examples generated: {len(result['examples'])}")
440 |             
441 |             # Ask if user wants to see examples
442 |             show_examples = input(f"\n👀 Do you want to see the generated examples for {library_name}? (y/n): ").strip().lower()
443 |             
444 |             if show_examples in ['y', 'yes']:
445 |                 for i, example in enumerate(result['examples'], 1):
446 |                     print(f"\n{'─'*50}")
447 |                     print(f"📝 Example {i}: {example['use_case']}")
448 |                     print(f"{'─'*50}")
449 |                     
450 |                     print("\n💻 Generated Code:")
451 |                     print("```python")
452 |                     print(example['code'])
453 |                     print("```")
454 |                     
455 |                     print(f"\n📦 Required Imports:")
456 |                     for imp in example['imports']:
457 |                         print(f"  • {imp}")
458 |                     
459 |                     print(f"\n📝 Explanation:")
460 |                     print(example['explanation'])
461 |                     
462 |                     print(f"\n✅ Best Practices:")
463 |                     for practice in example['best_practices']:
464 |                         print(f"  • {practice}")
465 |                     
466 |                     # Ask if user wants to see the next example
467 |                     if i < len(result['examples']):
468 |                         continue_viewing = input(f"\nContinue to next example? (y/n): ").strip().lower()
469 |                         if continue_viewing not in ['y', 'yes']:
470 |                             break
471 |             
472 |             # Offer to save results
473 |             save_results = input(f"\n💾 Save learning results for {library_name} to file? (y/n): ").strip().lower()
474 |             
475 |             if save_results in ['y', 'yes']:
476 |                 filename = input(f"   Enter filename (default: {library_name.lower()}_learning.json): ").strip()
477 |                 if not filename:
478 |                     filename = f"{library_name.lower()}_learning.json"
479 |                 
480 |                 try:
481 |                     import json
482 |                     with open(filename, 'w') as f:
483 |                         json.dump(result, f, indent=2, default=str)
484 |                     print(f"   ✅ Results saved to {filename}")
485 |                 except Exception as e:
486 |                     print(f"   ❌ Error saving file: {e}")
487 |         
488 |         else:
489 |             print(f"❌ Failed to learn {library_name}")
490 |         
491 |         # Ask if user wants to learn another library
492 |         print(f"\n📚 Libraries learned so far: {list(learned_libraries.keys())}")
493 |         continue_learning = input("\n🔄 Do you want to learn another library? (y/n): ").strip().lower()
494 |         
495 |         if continue_learning not in ['y', 'yes']:
496 |             break
497 |     
498 |     # Final summary
499 |     if learned_libraries:
500 |         print(f"\n🎉 Session Summary:")
501 |         print(f"Successfully learned {len(learned_libraries)} libraries:")
502 |         for lib_name, info in learned_libraries.items():
503 |             print(f"  • {lib_name}: {len(info['examples'])} examples generated")
504 |     
505 |     return learned_libraries
506 | 
507 | # Example: Run interactive learning session
508 | if __name__ == "__main__":
509 |     # Run interactive session
510 |     learned_libraries = interactive_learning_session()
511 | ```
512 | 
513 | ## Example Output
514 | 
515 | When you run the interactive learning system, you'll see:
516 | 
517 | **Interactive Session Start:**
518 | ```
519 | 🎯 Welcome to the Interactive Library Learning System!
520 | This system will help you learn any Python library from its documentation.
521 | 
522 | ============================================================
523 | 🚀 LIBRARY LEARNING SESSION
524 | ============================================================
525 | 
526 | 📚 Enter the library name you want to learn (or 'quit' to exit): FastAPI
527 | 
528 | 🔗 Enter documentation URLs for FastAPI (one per line, empty line to finish):
529 |   URL: https://fastapi.tiangolo.com/
530 |   URL: https://fastapi.tiangolo.com/tutorial/first-steps/
531 |   URL: https://fastapi.tiangolo.com/tutorial/path-params/
532 |   URL: 
533 | 
534 | 🎯 Define use cases for FastAPI (optional, press Enter for defaults):
535 |    Default use cases will be: Basic setup, Common operations, Advanced usage
536 |    Do you want to define custom use cases? (y/n): y
537 |    Enter your use cases (one per line, empty line to finish):
538 |      Use case: Create a REST API with authentication
539 |      Use case: Build a file upload endpoint
540 |      Use case: Add database integration with SQLAlchemy
541 |      Use case: 
542 | ```
543 | 
544 | **Documentation Processing:**
545 | ```
546 | 🚀 Starting learning process for FastAPI...
547 | 🚀 Starting automated learning for FastAPI...
548 | Documentation sources: 3 URLs
549 | 📡 Fetching: https://fastapi.tiangolo.com/ (attempt 1)
550 | 📡 Fetching: https://fastapi.tiangolo.com/tutorial/first-steps/ (attempt 1)
551 | 📡 Fetching: https://fastapi.tiangolo.com/tutorial/path-params/ (attempt 1)
552 | 📚 Learning about FastAPI from 3 URLs...
553 | 
554 | 🔍 Library Analysis Results for FastAPI:
555 | Sources: 3 successful fetches
556 | Core Concepts: ['FastAPI app', 'path operations', 'dependencies', 'request/response models']
557 | Common Patterns: ['app = FastAPI()', 'decorator-based routing', 'Pydantic models']
558 | Key Methods: ['FastAPI()', '@app.get()', '@app.post()', 'uvicorn.run()']
559 | Installation: pip install fastapi uvicorn
560 | ```
561 | 
562 | **Code Generation:**
563 | ```
564 | 📝 Generating example 1/3: Create a REST API with authentication
565 | 
566 | ✅ Successfully learned FastAPI!
567 | 
568 | 📊 Learning Summary for FastAPI:
569 |    • Core concepts: 4 identified
570 |    • Common patterns: 3 found
571 |    • Examples generated: 3
572 | 
573 | 👀 Do you want to see the generated examples for FastAPI? (y/n): y
574 | 
575 | ──────────────────────────────────────────────────
576 | 📝 Example 1: Create a REST API with authentication
577 | ──────────────────────────────────────────────────
578 | 
579 | 💻 Generated Code:
580 | from fastapi import FastAPI, Depends, HTTPException, status
581 | from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
582 | import uvicorn
583 | from typing import Dict
584 | import jwt
585 | from datetime import datetime, timedelta
586 | 
587 | app = FastAPI(title="Authenticated API", version="1.0.0")
588 | security = HTTPBearer()
589 | 
590 | # Secret key for JWT (use environment variable in production)
591 | SECRET_KEY = "your-secret-key-here"
592 | ALGORITHM = "HS256"
593 | 
594 | def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
595 |     try:
596 |         payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=[ALGORITHM])
597 |         username: str = payload.get("sub")
598 |         if username is None:
599 |             raise HTTPException(status_code=401, detail="Invalid token")
600 |         return username
601 |     except jwt.PyJWTError:
602 |         raise HTTPException(status_code=401, detail="Invalid token")
603 | 
604 | @app.post("/login")
605 | async def login(username: str, password: str) -> dict[str, str]:
606 |     # In production, verify against database
607 |     if username == "admin" and password == "secret":
608 |         token_data = {"sub": username, "exp": datetime.utcnow() + timedelta(hours=24)}
609 |         token = jwt.encode(token_data, SECRET_KEY, algorithm=ALGORITHM)
610 |         return {"access_token": token, "token_type": "bearer"}
611 |     raise HTTPException(status_code=401, detail="Invalid credentials")
612 | 
613 | @app.get("/protected")
614 | async def protected_route(current_user: str = Depends(verify_token)) -> dict[str, str]:
615 |     return {"message": f"Hello {current_user}! This is a protected route."}
616 | 
617 | if __name__ == "__main__":
618 |     uvicorn.run(app, host="0.0.0.0", port=8000)
619 | 
620 | 📦 Required Imports:
621 |   • pip install fastapi uvicorn python-jose[cryptography]
622 |   • from fastapi import FastAPI, Depends, HTTPException, status
623 |   • from fastapi.security import HTTPBearer
624 |   • import jwt
625 | 
626 | 📝 Explanation:
627 | This example creates a FastAPI application with JWT-based authentication. It includes a login endpoint that returns a JWT token and a protected route that requires authentication...
628 | 
629 | ✅ Best Practices:
630 |   • Use environment variables for secret keys
631 |   • Implement proper password hashing in production
632 |   • Add token expiration and refresh logic
633 |   • Include proper error handling
634 | 
635 | Continue to next example? (y/n): n
636 | 
637 | 💾 Save learning results for FastAPI to file? (y/n): y
638 |    Enter filename (default: fastapi_learning.json): 
639 |    ✅ Results saved to fastapi_learning.json
640 | 
641 | 📚 Libraries learned so far: ['FastAPI']
642 | 
643 | 🔄 Do you want to learn another library? (y/n): n
644 | 
645 | 🎉 Session Summary:
646 | Successfully learned 1 libraries:
647 |   • FastAPI: 3 examples generated
648 | ```
649 | 
650 | 
651 | ## Next Steps
652 | 
653 | - **GitHub Integration**: Learn from README files and example repositories
654 | - **Video Tutorial Processing**: Extract information from video documentation
655 | - **Community Examples**: Aggregate examples from Stack Overflow and forums
656 | - **Version Comparison**: Track API changes across library versions
657 | - **Testing Generation**: Automatically create unit tests for generated code
658 | - **Page Crawling**: Automatically crawl documentation pages to actively understand the usage
659 | 
660 | This tutorial demonstrates how DSPy can automate the entire process of learning unfamiliar libraries from their documentation, making it valuable for rapid technology adoption and exploration.
```

tutorials/saving/index.md
```
1 | # Tutorial: Saving and Loading your DSPy program
2 | 
3 | This guide demonstrates how to save and load your DSPy program. At a high level, there are two ways to save your DSPy program:
4 | 
5 | 1. Save the state of the program only, similar to weights-only saving in PyTorch.
6 | 2. Save the whole program, including both the architecture and the state, which is supported by `dspy>=2.6.0`.
7 | 
8 | ## State-only Saving
9 | 
10 | State represents the DSPy program's internal state, including the signature, demos (few-shot examples), and other information like
11 | the `lm` to use for each `dspy.Predict` in the program. It also includes configurable attributes of other DSPy modules like
12 | `k` for `dspy.retrievers.Retriever`. To save the state of a program, use the `save` method and set `save_program=False`. You can
13 | choose to save the state to a JSON file or a pickle file. We recommend saving the state to a JSON file because it is safer and readable.
14 | But sometimes your program contains non-serializable objects like `dspy.Image` or `datetime.datetime`, in which case you should save
15 | the state to a pickle file.
16 | 
17 | Let's say we have compiled a program with some data, and we want to save the program for future usage:
18 | 
19 | ```python
20 | import dspy
21 | from dspy.datasets.gsm8k import GSM8K, gsm8k_metric
22 | 
23 | dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))
24 | 
25 | gsm8k = GSM8K()
26 | gsm8k_trainset = gsm8k.train[:10]
27 | dspy_program = dspy.ChainOfThought("question -> answer")
28 | 
29 | optimizer = dspy.BootstrapFewShot(metric=gsm8k_metric, max_bootstrapped_demos=4, max_labeled_demos=4, max_rounds=5)
30 | compiled_dspy_program = optimizer.compile(dspy_program, trainset=gsm8k_trainset)
31 | ```
32 | 
33 | To save the state of your program to json file:
34 | 
35 | ```python
36 | compiled_dspy_program.save("./dspy_program/program.json", save_program=False)
37 | ```
38 | 
39 | To save the state of your program to a pickle file:
40 | 
41 | ```python
42 | compiled_dspy_program.save("./dspy_program/program.pkl", save_program=False)
43 | ```
44 | 
45 | To load your saved state, you need to **recreate the same program**, then load the state using the `load` method.
46 | 
47 | ```python
48 | loaded_dspy_program = dspy.ChainOfThought("question -> answer") # Recreate the same program.
49 | loaded_dspy_program.load("./dspy_program/program.json")
50 | 
51 | assert len(compiled_dspy_program.demos) == len(loaded_dspy_program.demos)
52 | for original_demo, loaded_demo in zip(compiled_dspy_program.demos, loaded_dspy_program.demos):
53 |     # Loaded demo is a dict, while the original demo is a dspy.Example.
54 |     assert original_demo.toDict() == loaded_demo
55 | assert str(compiled_dspy_program.signature) == str(loaded_dspy_program.signature)
56 | ```
57 | 
58 | Or load the state from a pickle file:
59 | 
60 | ```python
61 | loaded_dspy_program = dspy.ChainOfThought("question -> answer") # Recreate the same program.
62 | loaded_dspy_program.load("./dspy_program/program.pkl")
63 | 
64 | assert len(compiled_dspy_program.demos) == len(loaded_dspy_program.demos)
65 | for original_demo, loaded_demo in zip(compiled_dspy_program.demos, loaded_dspy_program.demos):
66 |     # Loaded demo is a dict, while the original demo is a dspy.Example.
67 |     assert original_demo.toDict() == loaded_demo
68 | assert str(compiled_dspy_program.signature) == str(loaded_dspy_program.signature)
69 | ```
70 | 
71 | ## Whole Program Saving
72 | 
73 | Starting from `dspy>=2.6.0`, DSPy supports saving the whole program, including the architecture and the state. This feature
74 | is powered by `cloudpickle`, which is a library for serializing and deserializing Python objects.
75 | 
76 | To save the whole program, use the `save` method and set `save_program=True`, and specify a directory path to save the program
77 | instead of a file name. We require a directory path because we also save some metadata, e.g., the dependency versions along
78 | with the program itself.
79 | 
80 | ```python
81 | compiled_dspy_program.save("./dspy_program/", save_program=True)
82 | ```
83 | 
84 | To load the saved program, directly use `dspy.load` method:
85 | 
86 | ```python
87 | loaded_dspy_program = dspy.load("./dspy_program/")
88 | 
89 | assert len(compiled_dspy_program.demos) == len(loaded_dspy_program.demos)
90 | for original_demo, loaded_demo in zip(compiled_dspy_program.demos, loaded_dspy_program.demos):
91 |     # Loaded demo is a dict, while the original demo is a dspy.Example.
92 |     assert original_demo.toDict() == loaded_demo
93 | assert str(compiled_dspy_program.signature) == str(loaded_dspy_program.signature)
94 | ```
95 | 
96 | With whole program saving, you don't need to recreate the program, but can directly load the architecture along with the state.
97 | You can pick the suitable saving approach based on your needs.
98 | 
99 | ### Serializing Imported Modules
100 | 
101 | When saving a program with `save_program=True`, you might need to include custom modules that your program depends on. This is
102 | necessary if your program depends on these modules, but at loading time these modules are not imported before calling `dspy.load`.
103 | 
104 | You can specify which custom modules should be serialized with your program by passing them to the `modules_to_serialize`
105 | parameter when calling `save`. This ensures that any dependencies your program relies on are included during serialization and
106 | available when loading the program later.
107 | 
108 | Under the hood this uses cloudpickle's `cloudpickle.register_pickle_by_value` function to register a module as picklable by value.
109 | When a module is registered this way, cloudpickle will serialize the module by value rather than by reference, ensuring that the
110 | module contents are preserved with the saved program.
111 | 
112 | For example, if your program uses custom modules:
113 | 
114 | ```python
115 | import dspy
116 | import my_custom_module
117 | 
118 | compiled_dspy_program = dspy.ChainOfThought(my_custom_module.custom_signature)
119 | 
120 | # Save the program with the custom module
121 | compiled_dspy_program.save(
122 |     "./dspy_program/",
123 |     save_program=True,
124 |     modules_to_serialize=[my_custom_module]
125 | )
126 | ```
127 | 
128 | This ensures that the required modules are properly serialized and available when loading the program later. Any number of
129 | modules can be passed to `modules_to_serialize`. If you don't specify `modules_to_serialize`, no additional modules will be
130 | registered for serialization.
131 | 
132 | ## Backward Compatibility
133 | 
134 | As of `dspy<3.0.0`, we don't guarantee the backward compatibility of the saved program. For example, if you save the program with `dspy==2.5.35`,
135 | at loading time please make sure to use the same version of DSPy to load the program, otherwise the program may not work as expected. Chances
136 | are that loading a saved file in a different version of DSPy will not raise an error, but the performance could be different from when
137 | the program was saved.
138 | 
139 | Starting from `dspy>=3.0.0`, we will guarantee the backward compatibility of the saved program in major releases, i.e., programs saved in `dspy==3.0.0`
140 | should be loadable in `dspy==3.7.10`.
```

tutorials/streaming/index.md
```
1 | # Streaming
2 | 
3 | In this guide, we will walk you through how to enable streaming in your DSPy program. DSPy Streaming
4 | consists of two parts:
5 | 
6 | - **Output Token Streaming**: Stream individual tokens as they're generated, rather than waiting for the complete response.
7 | - **Intermediate Status Streaming**: Provide real-time updates about the program's execution state (e.g., "Calling web search...", "Processing results...").
8 | 
9 | ## Output Token Streaming
10 | 
11 | DSPy's token streaming feature works with any module in your pipeline, not just the final output. The only requirement is that the streamed field must be of type `str`. To enable token streaming:
12 | 
13 | 1. Wrap your program with `dspy.streamify`
14 | 2. Create one or more `dspy.streaming.StreamListener` objects to specify which fields to stream
15 | 
16 | Here's a basic example:
17 | 
18 | ```python
19 | import os
20 | 
21 | import dspy
22 | 
23 | os.environ["OPENAI_API_KEY"] = "your_api_key"
24 | 
25 | dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
26 | 
27 | predict = dspy.Predict("question->answer")
28 | 
29 | # Enable streaming for the 'answer' field
30 | stream_predict = dspy.streamify(
31 |     predict,
32 |     stream_listeners=[dspy.streaming.StreamListener(signature_field_name="answer")],
33 | )
34 | ```
35 | 
36 | To consume the streamed output:
37 | 
38 | ```python
39 | import asyncio
40 | 
41 | async def read_output_stream():
42 |     output_stream = stream_predict(question="Why did a chicken cross the kitchen?")
43 | 
44 |     async for chunk in output_stream:
45 |         print(chunk)
46 | 
47 | asyncio.run(read_output_stream())
48 | ```
49 | 
50 | This will produce output like:
51 | 
52 | ```
53 | StreamResponse(predict_name='self', signature_field_name='answer', chunk='To')
54 | StreamResponse(predict_name='self', signature_field_name='answer', chunk=' get')
55 | StreamResponse(predict_name='self', signature_field_name='answer', chunk=' to')
56 | StreamResponse(predict_name='self', signature_field_name='answer', chunk=' the')
57 | StreamResponse(predict_name='self', signature_field_name='answer', chunk=' other')
58 | StreamResponse(predict_name='self', signature_field_name='answer', chunk=' side of the frying pan!')
59 | Prediction(
60 |     answer='To get to the other side of the frying pan!'
61 | )
62 | ```
63 | 
64 | Note: Since `dspy.streamify` returns an async generator, you must use it within an async context. If you're using an environment like Jupyter or Google Colab that already has an event loop (async context), you can use the generator directly.
65 | 
66 | You may have noticed that the above streaming contains two different entities: `StreamResponse`
67 | and `Prediction.` `StreamResponse` is the wrapper over streaming tokens on the field being listened to, and in
68 | this example it is the `answer` field. `Prediction` is the program's final output. In DSPy, streaming is
69 | implemented in a sidecar fashion: we enable streaming on the LM so that LM outputs a stream of tokens. We send these
70 | tokens to a side channel, which is being continuously read by the user-defined listeners. Listeners keep interpreting
71 | the stream, and decides if the `signature_field_name` it is listening to has started to appear and has finalized.
72 | Once it decides that the field appears, the listener begins outputting tokens to the async generator users can
73 | read. Listeners' internal mechanism changes according to the adapter behind the scene, and because usually
74 | we cannot decide if a field has finalized until seeing the next field, the listener buffers the output tokens
75 | before sending to the final generator, which is why you will usually see the last chunk of type `StreamResponse`
76 | has more than one token. The program's output is also written to the stream, which is the chunk of `Prediction`
77 | as in the sample output above.
78 | 
79 | To handle these different types and implement custom logic:
80 | 
81 | ```python
82 | import asyncio
83 | 
84 | async def read_output_stream():
85 |   output_stream = stream_predict(question="Why did a chicken cross the kitchen?")
86 | 
87 |   async for chunk in output_stream:
88 |     return_value = None
89 |     if isinstance(chunk, dspy.streaming.StreamResponse):
90 |       print(f"Output token of field {chunk.signature_field_name}: {chunk.chunk}")
91 |     elif isinstance(chunk, dspy.Prediction):
92 |       return_value = chunk
93 | 
94 | 
95 | program_output = asyncio.run(read_output_stream())
96 | print("Final output: ", program_output)
97 | ```
98 | 
99 | ### Understand `StreamResponse`
100 | 
101 | `StreamResponse` (`dspy.streaming.StreamResponse`) is the wrapper class of streaming tokens. It comes with 3
102 | fields:
103 | 
104 | - `predict_name`: the name of the predict that holds the `signature_field_name`. The name is the
105 |   same name of keys as you run `your_program.named_predictors()`. In the code above because `answer` is from
106 |   the `predict` itself, so the `predict_name` shows up as `self`, which is the only key as your run
107 |   `predict.named_predictors()`.
108 | - `signature_field_name`: the output field that these tokens map to. `predict_name` and `signature_field_name`
109 |   together form the unique identifier of the field. We will demonstrate how to handle multiple fields streaming
110 |   and duplicated field name later in this guide.
111 | - `chunk`: the value of the stream chunk.
112 | 
113 | ### Streaming with Cache
114 | 
115 | When a cached result is found, the stream will skip individual tokens and only yield the final `Prediction`. For example:
116 | 
117 | ```
118 | Prediction(
119 |     answer='To get to the other side of the dinner plate!'
120 | )
121 | ```
122 | 
123 | ### Streaming Multiple Fields
124 | 
125 | You can monitor multiple fields by creating a `StreamListener` for each one. Here's an example with a multi-module program:
126 | 
127 | ```python
128 | import asyncio
129 | 
130 | import dspy
131 | 
132 | lm = dspy.LM("openai/gpt-4o-mini", cache=False)
133 | dspy.settings.configure(lm=lm)
134 | 
135 | 
136 | class MyModule(dspy.Module):
137 |     def __init__(self):
138 |         super().__init__()
139 | 
140 |         self.predict1 = dspy.Predict("question->answer")
141 |         self.predict2 = dspy.Predict("answer->simplified_answer")
142 | 
143 |     def forward(self, question: str, **kwargs):
144 |         answer = self.predict1(question=question)
145 |         simplified_answer = self.predict2(answer=answer)
146 |         return simplified_answer
147 | 
148 | 
149 | predict = MyModule()
150 | stream_listeners = [
151 |     dspy.streaming.StreamListener(signature_field_name="answer"),
152 |     dspy.streaming.StreamListener(signature_field_name="simplified_answer"),
153 | ]
154 | stream_predict = dspy.streamify(
155 |     predict,
156 |     stream_listeners=stream_listeners,
157 | )
158 | 
159 | async def read_output_stream():
160 |     output = stream_predict(question="why did a chicken cross the kitchen?")
161 | 
162 |     return_value = None
163 |     async for chunk in output:
164 |         if isinstance(chunk, dspy.streaming.StreamResponse):
165 |             print(chunk)
166 |         elif isinstance(chunk, dspy.Prediction):
167 |             return_value = chunk
168 |     return return_value
169 | 
170 | program_output = asyncio.run(read_output_stream())
171 | print("Final output: ", program_output)
172 | ```
173 | 
174 | The output will look like:
175 | 
176 | ```
177 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk='To')
178 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk=' get')
179 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk=' to')
180 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk=' the')
181 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk=' other side of the recipe!')
182 | StreamResponse(predict_name='predict2', signature_field_name='simplified_answer', chunk='To')
183 | StreamResponse(predict_name='predict2', signature_field_name='simplified_answer', chunk=' reach')
184 | StreamResponse(predict_name='predict2', signature_field_name='simplified_answer', chunk=' the')
185 | StreamResponse(predict_name='predict2', signature_field_name='simplified_answer', chunk=' other side of the recipe!')
186 | Final output:  Prediction(
187 |     simplified_answer='To reach the other side of the recipe!'
188 | )
189 | ```
190 | 
191 | ### Streaming the Same Field Multiple Times (as in dspy.ReAct)
192 | 
193 | By default, a `StreamListener` automatically closes itself after completing a single streaming session.
194 | This design helps prevent performance issues, since every token is broadcast to all configured stream listeners,
195 | and having too many active listeners can introduce significant overhead.
196 | 
197 | However, in scenarios where a DSPy module is used repeatedly in a loop—such as with `dspy.ReAct` — you may want to stream
198 | the same field from each prediction, every time it is used. To enable this behavior, set allow_reuse=True when creating
199 | your `StreamListener`. See the example below:
200 | 
201 | ```python
202 | import asyncio
203 | 
204 | import dspy
205 | 
206 | lm = dspy.LM("openai/gpt-4o-mini", cache=False)
207 | dspy.settings.configure(lm=lm)
208 | 
209 | 
210 | def fetch_user_info(user_name: str):
211 |     """Get user information like name, birthday, etc."""
212 |     return {
213 |         "name": user_name,
214 |         "birthday": "2009-05-16",
215 |     }
216 | 
217 | 
218 | def get_sports_news(year: int):
219 |     """Get sports news for a given year."""
220 |     if year == 2009:
221 |         return "Usane Bolt broke the world record in the 100m race."
222 |     return None
223 | 
224 | 
225 | react = dspy.ReAct("question->answer", tools=[fetch_user_info, get_sports_news])
226 | 
227 | stream_listeners = [
228 |     # dspy.ReAct has a built-in output field called "next_thought".
229 |     dspy.streaming.StreamListener(signature_field_name="next_thought", allow_reuse=True),
230 | ]
231 | stream_react = dspy.streamify(react, stream_listeners=stream_listeners)
232 | 
233 | 
234 | async def read_output_stream():
235 |     output = stream_react(question="What sports news happened in the year Adam was born?")
236 |     return_value = None
237 |     async for chunk in output:
238 |         if isinstance(chunk, dspy.streaming.StreamResponse):
239 |             print(chunk)
240 |         elif isinstance(chunk, dspy.Prediction):
241 |             return_value = chunk
242 |     return return_value
243 | 
244 | 
245 | print(asyncio.run(read_output_stream()))
246 | ```
247 | 
248 | In this example, by setting `allow_reuse=True` in the StreamListener, you ensure that streaming for "next_thought" is
249 | available for every iteration, not just the first. When you run this code, you will see the streaming tokens for `next_thought`
250 | output each time the field is produced.
251 | 
252 | #### Handling Duplicate Field Names
253 | 
254 | When streaming fields with the same name from different modules, specify both the `predict` and `predict_name` in the `StreamListener`:
255 | 
256 | ```python
257 | import asyncio
258 | 
259 | import dspy
260 | 
261 | lm = dspy.LM("openai/gpt-4o-mini", cache=False)
262 | dspy.settings.configure(lm=lm)
263 | 
264 | 
265 | class MyModule(dspy.Module):
266 |     def __init__(self):
267 |         super().__init__()
268 | 
269 |         self.predict1 = dspy.Predict("question->answer")
270 |         self.predict2 = dspy.Predict("question, answer->answer, score")
271 | 
272 |     def forward(self, question: str, **kwargs):
273 |         answer = self.predict1(question=question)
274 |         simplified_answer = self.predict2(answer=answer)
275 |         return simplified_answer
276 | 
277 | 
278 | predict = MyModule()
279 | stream_listeners = [
280 |     dspy.streaming.StreamListener(
281 |         signature_field_name="answer",
282 |         predict=predict.predict1,
283 |         predict_name="predict1"
284 |     ),
285 |     dspy.streaming.StreamListener(
286 |         signature_field_name="answer",
287 |         predict=predict.predict2,
288 |         predict_name="predict2"
289 |     ),
290 | ]
291 | stream_predict = dspy.streamify(
292 |     predict,
293 |     stream_listeners=stream_listeners,
294 | )
295 | 
296 | 
297 | async def read_output_stream():
298 |     output = stream_predict(question="why did a chicken cross the kitchen?")
299 | 
300 |     return_value = None
301 |     async for chunk in output:
302 |         if isinstance(chunk, dspy.streaming.StreamResponse):
303 |             print(chunk)
304 |         elif isinstance(chunk, dspy.Prediction):
305 |             return_value = chunk
306 |     return return_value
307 | 
308 | 
309 | program_output = asyncio.run(read_output_stream())
310 | print("Final output: ", program_output)
311 | ```
312 | 
313 | The output will be like:
314 | 
315 | ```
316 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk='To')
317 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk=' get')
318 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk=' to')
319 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk=' the')
320 | StreamResponse(predict_name='predict1', signature_field_name='answer', chunk=' other side of the recipe!')
321 | StreamResponse(predict_name='predict2', signature_field_name='answer', chunk="I'm")
322 | StreamResponse(predict_name='predict2', signature_field_name='answer', chunk=' ready')
323 | StreamResponse(predict_name='predict2', signature_field_name='answer', chunk=' to')
324 | StreamResponse(predict_name='predict2', signature_field_name='answer', chunk=' assist')
325 | StreamResponse(predict_name='predict2', signature_field_name='answer', chunk=' you')
326 | StreamResponse(predict_name='predict2', signature_field_name='answer', chunk='! Please provide a question.')
327 | Final output:  Prediction(
328 |     answer="I'm ready to assist you! Please provide a question.",
329 |     score='N/A'
330 | )
331 | ```
332 | 
333 | ## Intermediate Status Streaming
334 | 
335 | Status streaming keeps users informed about the program's progress, especially useful for long-running operations like tool calls or complex AI pipelines. To implement status streaming:
336 | 
337 | 1. Create a custom status message provider by subclassing `dspy.streaming.StatusMessageProvider`
338 | 2. Override the desired hook methods to provide custom status messages
339 | 3. Pass your provider to `dspy.streamify`
340 | 
341 | Example:
342 | 
343 | ```python
344 | class MyStatusMessageProvider(dspy.streaming.StatusMessageProvider):
345 |     def lm_start_status_message(self, instance, inputs):
346 |         return f"Calling LM with inputs {inputs}..."
347 | 
348 |     def lm_end_status_message(self, outputs):
349 |         return f"Tool finished with output: {outputs}!"
350 | ```
351 | 
352 | Available hooks:
353 | 
354 | - lm_start_status_message: status message at the start of calling dspy.LM.
355 | - lm_end_status_message: status message at the end of calling dspy.LM.
356 | - module_start_status_message: status message at the start of calling a dspy.Module.
357 | - module_end_status_message: status message at the start of calling a dspy.Module.
358 | - tool_start_status_message: status message at the start of calling dspy.Tool.
359 | - tool_end_status_message: status message at the end of calling dspy.Tool.
360 | 
361 | Each hook should return a string containing the status message.
362 | 
363 | After creating the message provider, just pass it to `dspy.streamify`, and you can enable both
364 | status message streaming and output token streaming. Please see the example below. The intermediate
365 | status message is represented in the class `dspy.streaming.StatusMessage`, so we need to have
366 | another condition check to capture it.
367 | 
368 | ```python
369 | import asyncio
370 | 
371 | import dspy
372 | 
373 | lm = dspy.LM("openai/gpt-4o-mini", cache=False)
374 | dspy.settings.configure(lm=lm)
375 | 
376 | 
377 | class MyModule(dspy.Module):
378 |     def __init__(self):
379 |         super().__init__()
380 | 
381 |         self.tool = dspy.Tool(lambda x: 2 * x, name="double_the_number")
382 |         self.predict = dspy.ChainOfThought("num1, num2->sum")
383 | 
384 |     def forward(self, num, **kwargs):
385 |         num2 = self.tool(x=num)
386 |         return self.predict(num1=num, num2=num2)
387 | 
388 | 
389 | class MyStatusMessageProvider(dspy.streaming.StatusMessageProvider):
390 |     def tool_start_status_message(self, instance, inputs):
391 |         return f"Calling Tool {instance.name} with inputs {inputs}..."
392 | 
393 |     def tool_end_status_message(self, outputs):
394 |         return f"Tool finished with output: {outputs}!"
395 | 
396 | 
397 | predict = MyModule()
398 | stream_listeners = [
399 |     # dspy.ChainOfThought has a built-in output field called "reasoning".
400 |     dspy.streaming.StreamListener(signature_field_name="reasoning"),
401 | ]
402 | stream_predict = dspy.streamify(
403 |     predict,
404 |     stream_listeners=stream_listeners,
405 |     status_message_provider=MyStatusMessageProvider(),
406 | )
407 | 
408 | 
409 | async def read_output_stream():
410 |     output = stream_predict(num=3)
411 | 
412 |     return_value = None
413 |     async for chunk in output:
414 |         if isinstance(chunk, dspy.streaming.StreamResponse):
415 |             print(chunk)
416 |         elif isinstance(chunk, dspy.Prediction):
417 |             return_value = chunk
418 |         elif isinstance(chunk, dspy.streaming.StatusMessage):
419 |             print(chunk)
420 |     return return_value
421 | 
422 | 
423 | program_output = asyncio.run(read_output_stream())
424 | print("Final output: ", program_output)
425 | ```
426 | 
427 | Sample output:
428 | 
429 | ```
430 | StatusMessage(message='Calling tool double_the_number...')
431 | StatusMessage(message='Tool calling finished! Querying the LLM with tool calling results...')
432 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk='To')
433 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' find')
434 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' the')
435 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' sum')
436 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' of')
437 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' the')
438 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' two')
439 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' numbers')
440 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=',')
441 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' we')
442 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' simply')
443 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' add')
444 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' them')
445 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' together')
446 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk='.')
447 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' Here')
448 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=',')
449 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' ')
450 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk='3')
451 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' plus')
452 | StreamResponse(predict_name='predict.predict', signature_field_name='reasoning', chunk=' 6 equals 9.')
453 | Final output:  Prediction(
454 |     reasoning='To find the sum of the two numbers, we simply add them together. Here, 3 plus 6 equals 9.',
455 |     sum='9'
456 | )
457 | ```
458 | 
459 | ## Synchronous Streaming
460 | 
461 | By default calling a streamified DSPy program produces an async generator. In order to get back
462 | a sync generator, you can set the flag `async_streaming=False`:
463 | 
464 | 
465 | ```python
466 | import os
467 | 
468 | import dspy
469 | 
470 | os.environ["OPENAI_API_KEY"] = "your_api_key"
471 | 
472 | dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
473 | 
474 | predict = dspy.Predict("question->answer")
475 | 
476 | # Enable streaming for the 'answer' field
477 | stream_predict = dspy.streamify(
478 |     predict,
479 |     stream_listeners=[dspy.streaming.StreamListener(signature_field_name="answer")],
480 |     async_streaming=False,
481 | )
482 | 
483 | output = stream_predict(question="why did a chicken cross the kitchen?")
484 | 
485 | program_output = None
486 | for chunk in output:
487 |     if isinstance(chunk, dspy.streaming.StreamResponse):
488 |         print(chunk)
489 |     elif isinstance(chunk, dspy.Prediction):
490 |         program_output = chunk
491 | print(f"Program output: {program_output}")
492 | ```
```

tutorials/tool_use/index.ipynb
```
1 | {
2 |  "cells": [
3 |   {
4 |    "cell_type": "markdown",
5 |    "metadata": {},
6 |    "source": [
7 |     "# Tutorial: Advanced Tool Use\n",
8 |     "\n",
9 |     "Let's walk through a quick example of building and prompt-optimizing a DSPy agent for advanced tool use. We'll do this for the challenging task [ToolHop](https://arxiv.org/abs/2501.02506) but with an even stricter evaluation criteria.\n",
10 |     "\n",
11 |     "Install the latest DSPy via `pip install -U dspy` and follow along. You will also need to `pip install func_timeout datasets`."
12 |    ]
13 |   },
14 |   {
15 |    "cell_type": "markdown",
16 |    "metadata": {},
17 |    "source": [
18 |     "<details>\n",
19 |     "<summary>Recommended: Set up MLflow Tracing to understand what's happening under the hood.</summary>\n",
20 |     "\n",
21 |     "### MLflow DSPy Integration\n",
22 |     "\n",
23 |     "<a href=\"https://mlflow.org/\">MLflow</a> is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below.\n",
24 |     "\n",
25 |     "1. Install MLflow\n",
26 |     "\n",
27 |     "```bash\n",
28 |     "%pip install mlflow>=2.20\n",
29 |     "```\n",
30 |     "\n",
31 |     "2. Start MLflow UI in a separate terminal\n",
32 |     "```bash\n",
33 |     "mlflow ui --port 5000\n",
34 |     "```\n",
35 |     "\n",
36 |     "3. Connect the notebook to MLflow\n",
37 |     "```python\n",
38 |     "import mlflow\n",
39 |     "\n",
40 |     "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
41 |     "mlflow.set_experiment(\"DSPy\")\n",
42 |     "```\n",
43 |     "\n",
44 |     "4. Enabling tracing.\n",
45 |     "```python\n",
46 |     "mlflow.dspy.autolog()\n",
47 |     "```\n",
48 |     "\n",
49 |     "To learn more about the integration, visit [MLflow DSPy Documentation](https://mlflow.org/docs/latest/llms/dspy/index.html) as well.\n",
50 |     "</details>"
51 |    ]
52 |   },
53 |   {
54 |    "cell_type": "markdown",
55 |    "metadata": {},
56 |    "source": [
57 |     "In this tutorial, we'll demonstrate the new experimental `dspy.SIMBA` prompt optimizer, which tends to be powerful for larger LLMs and harder tasks. Using this, we'll improve our agent from 35% accuracy to 60%."
58 |    ]
59 |   },
60 |   {
61 |    "cell_type": "code",
62 |    "execution_count": 1,
63 |    "metadata": {},
64 |    "outputs": [],
65 |    "source": [
66 |     "import dspy\n",
67 |     "import ujson\n",
68 |     "import random\n",
69 |     "\n",
70 |     "gpt4o = dspy.LM(\"openai/gpt-4o\", temperature=0.7)\n",
71 |     "dspy.configure(lm=gpt4o)"
72 |    ]
73 |   },
74 |   {
75 |    "cell_type": "markdown",
76 |    "metadata": {},
77 |    "source": [
78 |     "Let's now download the data."
79 |    ]
80 |   },
81 |   {
82 |    "cell_type": "code",
83 |    "execution_count": 2,
84 |    "metadata": {},
85 |    "outputs": [
86 |     {
87 |      "name": "stdout",
88 |      "output_type": "stream",
89 |      "text": [
90 |       "Downloading 'ToolHop.json'...\n"
91 |      ]
92 |     }
93 |    ],
94 |    "source": [
95 |     "from dspy.utils import download\n",
96 |     "\n",
97 |     "download(\"https://huggingface.co/datasets/bytedance-research/ToolHop/resolve/main/data/ToolHop.json\")\n",
98 |     "\n",
99 |     "data = ujson.load(open(\"ToolHop.json\"))\n",
100 |     "random.Random(0).shuffle(data)"
101 |    ]
102 |   },
103 |   {
104 |    "cell_type": "markdown",
105 |    "metadata": {},
106 |    "source": [
107 |     "Then let's prepare a cleaned set of examples. The ToolHop task is interesting in that the agent gets a _unique set_ of tools (functions) to use separately for each request. Thus, it needs to learn how to use _any_ such tools effectively in practice."
108 |    ]
109 |   },
110 |   {
111 |    "cell_type": "code",
112 |    "execution_count": 3,
113 |    "metadata": {},
114 |    "outputs": [],
115 |    "source": [
116 |     "import re\n",
117 |     "import inspect\n",
118 |     "\n",
119 |     "examples = []\n",
120 |     "fns2code = {}\n",
121 |     "\n",
122 |     "def finish(answer: str):\n",
123 |     "    \"\"\"Conclude the trajectory and return the final answer.\"\"\"\n",
124 |     "    return answer\n",
125 |     "\n",
126 |     "for datapoint in data:\n",
127 |     "    func_dict = {}\n",
128 |     "    for func_code in datapoint[\"functions\"]:\n",
129 |     "        cleaned_code = func_code.rsplit(\"\\n\\n# Example usage\", 1)[0]\n",
130 |     "        fn_name = re.search(r\"^\\s*def\\s+([a-zA-Z0-9_]+)\\s*\\(\", cleaned_code)\n",
131 |     "        fn_name = fn_name.group(1) if fn_name else None\n",
132 |     "\n",
133 |     "        if not fn_name:\n",
134 |     "            continue\n",
135 |     "\n",
136 |     "        local_vars = {}\n",
137 |     "        exec(cleaned_code, {}, local_vars)\n",
138 |     "        fn_obj = local_vars.get(fn_name)\n",
139 |     "\n",
140 |     "        if callable(fn_obj):\n",
141 |     "            func_dict[fn_name] = fn_obj\n",
142 |     "            assert fn_obj not in fns2code, f\"Duplicate function found: {fn_name}\"\n",
143 |     "            fns2code[fn_obj] = (fn_name, cleaned_code)\n",
144 |     "\n",
145 |     "    func_dict[\"finish\"] = finish\n",
146 |     "\n",
147 |     "    example = dspy.Example(question=datapoint[\"question\"], answer=datapoint[\"answer\"], functions=func_dict)\n",
148 |     "    examples.append(example.with_inputs(\"question\", \"functions\"))\n",
149 |     "\n",
150 |     "trainset, devset, testset = examples[:100], examples[100:400], examples[400:]"
151 |    ]
152 |   },
153 |   {
154 |    "cell_type": "markdown",
155 |    "metadata": {},
156 |    "source": [
157 |     "And let's define some helpers for the task. Here, we will define the `metric`, which will be (much) stricter than in the original paper: we'll expect the prediction to match exactly (after normalization) with the ground truth. We'll also be strict in a second way: we'll only allow the agent to take 5 steps in total, to allow for efficient deployment."
158 |    ]
159 |   },
160 |   {
161 |    "cell_type": "code",
162 |    "execution_count": 4,
163 |    "metadata": {},
164 |    "outputs": [],
165 |    "source": [
166 |     "from func_timeout import func_set_timeout\n",
167 |     "\n",
168 |     "def wrap_function_with_timeout(fn):\n",
169 |     "    @func_set_timeout(10)\n",
170 |     "    def wrapper(*args, **kwargs):\n",
171 |     "        try:\n",
172 |     "            return {\"return_value\": fn(*args, **kwargs), \"errors\": None}\n",
173 |     "        except Exception as e:\n",
174 |     "            return {\"return_value\": None, \"errors\": str(e)}\n",
175 |     "\n",
176 |     "    return wrapper\n",
177 |     "\n",
178 |     "def fn_metadata(func):\n",
179 |     "    signature = inspect.signature(func)\n",
180 |     "    docstring = inspect.getdoc(func) or \"No docstring.\"\n",
181 |     "    return dict(function_name=func.__name__, arguments=str(signature), docstring=docstring)\n",
182 |     "\n",
183 |     "def metric(example, pred, trace=None):\n",
184 |     "    gold = str(example.answer).rstrip(\".0\").replace(\",\", \"\").lower()\n",
185 |     "    pred = str(pred.answer).rstrip(\".0\").replace(\",\", \"\").lower()\n",
186 |     "    return pred == gold  # stricter than the original paper's metric!\n",
187 |     "\n",
188 |     "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=24, display_progress=True, display_table=0, max_errors=999)"
189 |    ]
190 |   },
191 |   {
192 |    "cell_type": "markdown",
193 |    "metadata": {},
194 |    "source": [
195 |     "Now, let's define the agent! The core of our agent will be based on a ReAct loop, in which the model sees the trajectory so far and the set of functions available to invoke, and decides the next tool to call.\n",
196 |     "\n",
197 |     "To keep the final agent fast, we'll limit its `max_steps` to 5 steps. We'll also run each function call with a timeout."
198 |    ]
199 |   },
200 |   {
201 |    "cell_type": "code",
202 |    "execution_count": 5,
203 |    "metadata": {},
204 |    "outputs": [],
205 |    "source": [
206 |     "class Agent(dspy.Module):\n",
207 |     "    def __init__(self, max_steps=5):\n",
208 |     "        self.max_steps = max_steps\n",
209 |     "        instructions = \"For the final answer, produce short (not full sentence) answers in which you format dates as YYYY-MM-DD, names as Firstname Lastname, and numbers without leading 0s.\"\n",
210 |     "        signature = dspy.Signature('question, trajectory, functions -> next_selected_fn, args: dict[str, Any]', instructions)\n",
211 |     "        self.react = dspy.ChainOfThought(signature)\n",
212 |     "\n",
213 |     "    def forward(self, question, functions):\n",
214 |     "        tools = {fn_name: fn_metadata(fn) for fn_name, fn in functions.items()}\n",
215 |     "        trajectory = []\n",
216 |     "\n",
217 |     "        for _ in range(self.max_steps):\n",
218 |     "            pred = self.react(question=question, trajectory=trajectory, functions=tools)\n",
219 |     "            selected_fn = pred.next_selected_fn.strip('\"').strip(\"'\")\n",
220 |     "            fn_output = wrap_function_with_timeout(functions[selected_fn])(**pred.args)\n",
221 |     "            trajectory.append(dict(reasoning=pred.reasoning, selected_fn=selected_fn, args=pred.args, **fn_output))\n",
222 |     "\n",
223 |     "            if selected_fn == \"finish\":\n",
224 |     "                break\n",
225 |     "\n",
226 |     "        return dspy.Prediction(answer=fn_output.get(\"return_value\", ''), trajectory=trajectory)"
227 |    ]
228 |   },
229 |   {
230 |    "cell_type": "markdown",
231 |    "metadata": {},
232 |    "source": [
233 |     "Out of the box, let's assess our `GPT-4o`-powered agent on the development set."
234 |    ]
235 |   },
236 |   {
237 |    "cell_type": "code",
238 |    "execution_count": 6,
239 |    "metadata": {},
240 |    "outputs": [
241 |     {
242 |      "name": "stderr",
243 |      "output_type": "stream",
244 |      "text": [
245 |       "2025/03/23 21:46:10 INFO dspy.evaluate.evaluate: Average Metric: 105.0 / 300 (35.0%)\n"
246 |      ]
247 |     },
248 |     {
249 |      "data": {
250 |       "text/plain": [
251 |        "35.0"
252 |       ]
253 |      },
254 |      "execution_count": 7,
255 |      "metadata": {},
256 |      "output_type": "execute_result"
257 |     }
258 |    ],
259 |    "source": [
260 |     "agent = Agent()\n",
261 |     "evaluate(agent)"
262 |    ]
263 |   },
264 |   {
265 |    "cell_type": "markdown",
266 |    "metadata": {},
267 |    "source": [
268 |     "Now, let's optimize the agent using `dspy.SIMBA`, which stands for **Stochastic Introspective Mini-Batch Ascent**. This prompt optimizer accepts arbitrary DSPy programs like our agent here and proceeds in a sequence of mini-batches seeking to make incremental improvements to the prompt instructions or few-shot examples."
269 |    ]
270 |   },
271 |   {
272 |    "cell_type": "code",
273 |    "execution_count": null,
274 |    "metadata": {},
275 |    "outputs": [],
276 |    "source": [
277 |     "simba = dspy.SIMBA(metric=metric, max_steps=12, max_demos=10)\n",
278 |     "optimized_agent = simba.compile(agent, trainset=trainset, seed=6793115)"
279 |    ]
280 |   },
281 |   {
282 |    "cell_type": "markdown",
283 |    "metadata": {},
284 |    "source": [
285 |     "Having completed this optimization, let's now evaluate our agent again. We see a substantial 71% relative gain, jumping to 60% accuracy."
286 |    ]
287 |   },
288 |   {
289 |    "cell_type": "code",
290 |    "execution_count": 8,
291 |    "metadata": {},
292 |    "outputs": [
293 |     {
294 |      "name": "stderr",
295 |      "output_type": "stream",
296 |      "text": [
297 |       "2025/03/23 21:46:21 INFO dspy.evaluate.evaluate: Average Metric: 182.0 / 300 (60.7%)"
298 |      ]
299 |     },
300 |     {
301 |      "data": {
302 |       "text/plain": [
303 |        "60.67"
304 |       ]
305 |      },
306 |      "execution_count": 8,
307 |      "metadata": {},
308 |      "output_type": "execute_result"
309 |     }
310 |    ],
311 |    "source": [
312 |     "evaluate(optimized_agent)"
313 |    ]
314 |   }
315 |  ],
316 |  "metadata": {
317 |   "kernelspec": {
318 |    "display_name": "jun2024_py310",
319 |    "language": "python",
320 |    "name": "python3"
321 |   },
322 |   "language_info": {
323 |    "codemirror_mode": {
324 |     "name": "ipython",
325 |     "version": 3
326 |    },
327 |    "file_extension": ".py",
328 |    "mimetype": "text/x-python",
329 |    "name": "python",
330 |    "nbconvert_exporter": "python",
331 |    "pygments_lexer": "ipython3",
332 |    "version": "3.10.14"
333 |   }
334 |  },
335 |  "nbformat": 4,
336 |  "nbformat_minor": 2
337 | }
```

tutorials/yahoo_finance_react/index.md
```
1 | # Financial Analysis with DSPy ReAct and Yahoo Finance News
2 | 
3 | This tutorial shows how to build a financial analysis agent using DSPy ReAct with [LangChain's Yahoo Finance News tool](https://python.langchain.com/docs/integrations/tools/yahoo_finance_news/) for real-time market analysis.
4 | 
5 | ## What You'll Build
6 | 
7 | A financial agent that fetches news, analyzes sentiment, and provides investment insights.
8 | 
9 | ## Setup
10 | 
11 | ```bash
12 | pip install dspy langchain langchain-community yfinance
13 | ```
14 | 
15 | ## Step 1: Convert LangChain Tool to DSPy
16 | 
17 | ```python
18 | import dspy
19 | from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool
20 | from dspy.adapters.types.tool import Tool
21 | import json
22 | import yfinance as yf
23 | 
24 | # Configure DSPy
25 | lm = dspy.LM(model='openai/gpt-4o-mini')
26 | dspy.configure(lm=lm)
27 | 
28 | # Convert LangChain Yahoo Finance tool to DSPy
29 | yahoo_finance_tool = YahooFinanceNewsTool()
30 | finance_news_tool = Tool.from_langchain(yahoo_finance_tool)
31 | ```
32 | 
33 | ## Step 2: Create Supporting Financial Tools
34 | 
35 | ```python
36 | def get_stock_price(ticker: str) -> str:
37 |     """Get current stock price and basic info."""
38 |     try:
39 |         stock = yf.Ticker(ticker)
40 |         info = stock.info
41 |         hist = stock.history(period="1d")
42 |         
43 |         if hist.empty:
44 |             return f"Could not retrieve data for {ticker}"
45 |         
46 |         current_price = hist['Close'].iloc[-1]
47 |         prev_close = info.get('previousClose', current_price)
48 |         change_pct = ((current_price - prev_close) / prev_close * 100) if prev_close else 0
49 |         
50 |         result = {
51 |             "ticker": ticker,
52 |             "price": round(current_price, 2),
53 |             "change_percent": round(change_pct, 2),
54 |             "company": info.get('longName', ticker)
55 |         }
56 |         
57 |         return json.dumps(result)
58 |     except Exception as e:
59 |         return f"Error: {str(e)}"
60 | 
61 | def compare_stocks(tickers: str) -> str:
62 |     """Compare multiple stocks (comma-separated)."""
63 |     try:
64 |         ticker_list = [t.strip().upper() for t in tickers.split(',')]
65 |         comparison = []
66 |         
67 |         for ticker in ticker_list:
68 |             stock = yf.Ticker(ticker)
69 |             info = stock.info
70 |             hist = stock.history(period="1d")
71 |             
72 |             if not hist.empty:
73 |                 current_price = hist['Close'].iloc[-1]
74 |                 prev_close = info.get('previousClose', current_price)
75 |                 change_pct = ((current_price - prev_close) / prev_close * 100) if prev_close else 0
76 |                 
77 |                 comparison.append({
78 |                     "ticker": ticker,
79 |                     "price": round(current_price, 2),
80 |                     "change_percent": round(change_pct, 2)
81 |                 })
82 |         
83 |         return json.dumps(comparison)
84 |     except Exception as e:
85 |         return f"Error: {str(e)}"
86 | ```
87 | 
88 | ## Step 3: Build the Financial ReAct Agent
89 | 
90 | ```python
91 | class FinancialAnalysisAgent(dspy.Module):
92 |     """ReAct agent for financial analysis using Yahoo Finance data."""
93 |     
94 |     def __init__(self):
95 |         super().__init__()
96 |         
97 |         # Combine all tools
98 |         self.tools = [
99 |             finance_news_tool,  # LangChain Yahoo Finance News
100 |             get_stock_price,
101 |             compare_stocks
102 |         ]
103 |         
104 |         # Initialize ReAct
105 |         self.react = dspy.ReAct(
106 |             signature="financial_query -> analysis_response",
107 |             tools=self.tools,
108 |             max_iters=6
109 |         )
110 |     
111 |     def forward(self, financial_query: str):
112 |         return self.react(financial_query=financial_query)
113 | ```
114 | 
115 | ## Step 4: Run Financial Analysis
116 | 
117 | ```python
118 | def run_financial_demo():
119 |     """Demo of the financial analysis agent."""
120 |     
121 |     # Initialize agent
122 |     agent = FinancialAnalysisAgent()
123 |     
124 |     # Example queries
125 |     queries = [
126 |         "What's the latest news about Apple (AAPL) and how might it affect the stock price?",
127 |         "Compare AAPL, GOOGL, and MSFT performance",
128 |         "Find recent Tesla news and analyze sentiment"
129 |     ]
130 |     
131 |     for query in queries:
132 |         print(f"Query: {query}")
133 |         response = agent(financial_query=query)
134 |         print(f"Analysis: {response.analysis_response}")
135 |         print("-" * 50)
136 | 
137 | # Run the demo
138 | if __name__ == "__main__":
139 |     run_financial_demo()
140 | ```
141 | 
142 | ## Example Output
143 | 
144 | When you run the agent with a query like "What's the latest news about Apple?", it will:
145 | 
146 | 1. Use the Yahoo Finance News tool to fetch recent Apple news
147 | 2. Get current stock price data
148 | 3. Analyze the information and provide insights
149 | 
150 | **Sample Response:**
151 | ```
152 | Analysis: Given the current price of Apple (AAPL) at $196.58 and the slight increase of 0.48%, it appears that the stock is performing steadily in the market. However, the inability to access the latest news means that any significant developments that could influence investor sentiment and stock price are unknown. Investors should keep an eye on upcoming announcements or market trends that could impact Apple's performance, especially in comparison to other tech stocks like Microsoft (MSFT), which is also showing a positive trend.
153 | ```
154 | 
155 | ## Key Benefits
156 | 
157 | - **Tool Integration**: Seamlessly combine LangChain tools with DSPy ReAct
158 | - **Real-time Data**: Access current market data and news
159 | - **Extensible**: Easy to add more financial analysis tools
160 | - **Intelligent Reasoning**: ReAct framework provides step-by-step analysis
161 | 
162 | This tutorial shows how DSPy's ReAct framework works with LangChain's financial tools to create intelligent market analysis agents.
```

api/optimizers/GEPA/GEPA_Advanced.md
```
1 | # dspy.GEPA - Advanced Features
2 | 
3 | ## Custom Instruction Proposers
4 | 
5 | ### What is instruction_proposer?
6 | 
7 | The `instruction_proposer` is the component responsible for invoking the `reflection_lm` and proposing new prompts during GEPA optimization. When GEPA identifies underperforming components in your DSPy program, the instruction proposer analyzes execution traces, feedback, and failures to generate improved instructions tailored to the observed issues.
8 | 
9 | ### Default Implementation
10 | 
11 | By default, GEPA uses the built-in instruction proposer from the [GEPA library](https://github.com/gepa-ai/gepa), which implements the [`ProposalFn`](https://github.com/gepa-ai/gepa/blob/main/src/gepa/core/adapter.py). The [default proposer](https://github.com/gepa-ai/gepa/blob/main/src/gepa/proposer/reflective_mutation/reflective_mutation.py#L53-L75) uses this prompt template:
12 | 
13 | ````
14 | I provided an assistant with the following instructions to perform a task for me:
15 | ```
16 | <curr_instructions>
17 | ```
18 | 
19 | The following are examples of different task inputs provided to the assistant along with the assistant's response for each of them, and some feedback on how the assistant's response could be better:
20 | ```
21 | <inputs_outputs_feedback>
22 | ```
23 | 
24 | Your task is to write a new instruction for the assistant.
25 | 
26 | Read the inputs carefully and identify the input format and infer detailed task description about the task I wish to solve with the assistant.
27 | 
28 | Read all the assistant responses and the corresponding feedback. Identify all niche and domain specific factual information about the task and include it in the instruction, as a lot of it may not be available to the assistant in the future. The assistant may have utilized a generalizable strategy to solve the task, if so, include that in the instruction as well.
29 | 
30 | Provide the new instructions within ``` blocks.
31 | ````
32 | 
33 | This template is automatically filled with:
34 | 
35 | - `<curr_instructions>`: The current instruction being optimized
36 | - `<inputs_outputs_feedback>`: Structured markdown containing predictor inputs, generated outputs, and evaluation feedback
37 | 
38 | Example of default behavior:
39 | 
40 | ```python
41 | # Default instruction proposer is used automatically
42 | gepa = dspy.GEPA(
43 |     metric=my_metric,
44 |     reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
45 |     auto="medium"
46 | )
47 | optimized_program = gepa.compile(student, trainset=examples)
48 | ```
49 | 
50 | ### When to Use Custom instruction_proposer
51 | 
52 | **Note:** Custom instruction proposers are an advanced feature. Most users should start with the default proposer, which works well for most text-based optimization tasks.
53 | 
54 | Consider implementing a custom instruction proposer when you need:
55 | 
56 | - **Multi-modal handling**: Process images (dspy.Image) alongside textual information in your inputs
57 | - **Nuanced control on limits and length constraints**: Have more fine-grained control over instruction length, format, and structural requirements
58 | - **Domain-specific information**: Inject specialized knowledge, terminology, or context that the default proposer lacks and cannot be provided via feedback_func. This is an advanced feature, and most users should not need to use this.
59 | - **Provider-specific prompting guides**: Optimize instructions for specific LLM providers (OpenAI, Anthropic, etc.) with their unique formatting preferences
60 | - **Coupled component updates**: Handle situations where 2 or more components need to be updated together in a coordinated manner, rather than optimizing each component independently (refer to component_selector parameter, in [Custom Component Selection](#custom-component-selection) section, for related functionality)
61 | - **External knowledge integration**: Connect to databases, APIs, or knowledge bases during instruction generation
62 | 
63 | ### Available Options
64 | 
65 | **Built-in Options:**
66 | 
67 | - **Default Proposer**: The standard GEPA instruction proposer (used when `instruction_proposer=None`). The default instruction proposer IS an instruction proposer as well! It is the most general one, that was used for the diverse experiments reported in the GEPA paper and tutorials.
68 | - **MultiModalInstructionProposer**: Handles `dspy.Image` inputs and structured multimodal content.
69 | 
70 | ```python
71 | from dspy.teleprompt.gepa.instruction_proposal import MultiModalInstructionProposer
72 | 
73 | # For tasks involving images or multimodal inputs
74 | gepa = dspy.GEPA(
75 |     metric=my_metric,
76 |     reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
77 |     instruction_proposer=MultiModalInstructionProposer(),
78 |     auto="medium"
79 | )
80 | ```
81 | 
82 | We invite community contributions of new instruction proposers for specialized domains as the [GEPA library](https://github.com/gepa-ai/gepa) continues to grow.
83 | 
84 | ### How to Implement Custom Instruction Proposers
85 | 
86 | Custom instruction proposers must implement the `ProposalFn` protocol by defining a callable class or function. GEPA will call your proposer during optimization:
87 | 
88 | ```python
89 | from dspy.teleprompt.gepa.gepa_utils import ReflectiveExample
90 | 
91 | class CustomInstructionProposer:
92 |     def __call__(
93 |         self,
94 |         candidate: dict[str, str],                          # Candidate component name -> instruction mapping to be updated in this round
95 |         reflective_dataset: dict[str, list[ReflectiveExample]],  # Component -> examples with structure: {"Inputs": ..., "Generated Outputs": ..., "Feedback": ...}
96 |         components_to_update: list[str]                     # Which components to improve
97 |     ) -> dict[str, str]:                                    # Return new instruction mapping only for components being updated
98 |         # Your custom instruction generation logic here
99 |         return updated_instructions
100 | 
101 | # Or as a function:
102 | def custom_instruction_proposer(candidate, reflective_dataset, components_to_update):
103 |     # Your custom instruction generation logic here
104 |     return updated_instructions
105 | ```
106 | 
107 | **Reflective Dataset Structure:**
108 | 
109 | - `dict[str, list[ReflectiveExample]]` - Maps component names to lists of examples
110 | - `ReflectiveExample` TypedDict contains:
111 |   - `Inputs: dict[str, Any]` - Predictor inputs (may include dspy.Image objects)
112 |   - `Generated_Outputs: dict[str, Any] | str` - Success: output fields dict, Failure: error message
113 |   - `Feedback: str` - Always a string from metric function or auto-generated by GEPA
114 | 
115 | #### Basic Example: Word Limit Proposer
116 | 
117 | ```python
118 | import dspy
119 | from gepa.core.adapter import ProposalFn
120 | from dspy.teleprompt.gepa.gepa_utils import ReflectiveExample
121 | 
122 | class GenerateWordLimitedInstruction(dspy.Signature):
123 |     """Given a current instruction and feedback examples, generate an improved instruction with word limit constraints."""
124 | 
125 |     current_instruction = dspy.InputField(desc="The current instruction that needs improvement")
126 |     feedback_summary = dspy.InputField(desc="Feedback from examples that might include both positive and negative cases")
127 |     max_words = dspy.InputField(desc="Maximum number of words allowed in the new instruction")
128 | 
129 |     improved_instruction = dspy.OutputField(desc="A new instruction that fixes the issues while staying under the max_words limit")
130 | 
131 | class WordLimitProposer(ProposalFn):
132 |     def __init__(self, max_words: int = 1000):
133 |         self.max_words = max_words
134 |         self.instruction_improver = dspy.ChainOfThought(GenerateWordLimitedInstruction)
135 | 
136 |     def __call__(self, candidate: dict[str, str], reflective_dataset: dict[str, list[ReflectiveExample]], components_to_update: list[str]) -> dict[str, str]:
137 |         updated_components = {}
138 | 
139 |         for component_name in components_to_update:
140 |             if component_name not in candidate or component_name not in reflective_dataset:
141 |                 continue
142 | 
143 |             current_instruction = candidate[component_name]
144 |             component_examples = reflective_dataset[component_name]
145 | 
146 |             # Create feedback summary
147 |             feedback_text = "\n".join([
148 |                 f"Example {i+1}: {ex.get('Feedback', 'No feedback')}"
149 |                 for i, ex in enumerate(component_examples)  # Limit examples to prevent context overflow
150 |             ])
151 | 
152 |             # Use the module to improve the instruction
153 |             result = self.instruction_improver(
154 |                 current_instruction=current_instruction,
155 |                 feedback_summary=feedback_text,
156 |                 max_words=self.max_words
157 |             )
158 | 
159 |             updated_components[component_name] = result.improved_instruction
160 | 
161 |         return updated_components
162 | 
163 | # Usage
164 | gepa = dspy.GEPA(
165 |     metric=my_metric,
166 |     reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
167 |     instruction_proposer=WordLimitProposer(max_words=700),
168 |     auto="medium"
169 | )
170 | ```
171 | 
172 | #### Advanced Example: RAG-Enhanced Instruction Proposer
173 | 
174 | ```python
175 | import dspy
176 | from gepa.core.adapter import ProposalFn
177 | from dspy.teleprompt.gepa.gepa_utils import ReflectiveExample
178 | 
179 | class GenerateDocumentationQuery(dspy.Signature):
180 |     """Analyze examples with feedback to identify common issue patterns and generate targeted database queries for retrieving relevant documentation.
181 | 
182 |     Your goal is to search a document database for guidelines that address the problematic patterns found in the examples. Look for recurring issues, error types, or failure modes in the feedback, then craft specific search queries that will find documentation to help resolve these patterns."""
183 | 
184 |     current_instruction = dspy.InputField(desc="The current instruction that needs improvement")
185 |     examples_with_feedback = dspy.InputField(desc="Examples with their feedback showing what issues occurred and any recurring patterns")
186 | 
187 |     failure_patterns: str = dspy.OutputField(desc="Summarize the common failure patterns identified in the examples")
188 | 
189 |     retrieval_queries: list[str] = dspy.OutputField(desc="Specific search queries to find relevant documentation in the database that addresses the common issue patterns identified in the problematic examples")
190 | 
191 | class GenerateRAGEnhancedInstruction(dspy.Signature):
192 |     """Generate improved instructions using retrieved documentation and examples analysis."""
193 | 
194 |     current_instruction = dspy.InputField(desc="The current instruction that needs improvement")
195 |     relevant_documentation = dspy.InputField(desc="Retrieved guidelines and best practices from specialized documentation")
196 |     examples_with_feedback = dspy.InputField(desc="Examples showing what issues occurred with the current instruction")
197 | 
198 |     improved_instruction: str = dspy.OutputField(desc="Enhanced instruction that incorporates retrieved guidelines and addresses the issues shown in the examples")
199 | 
200 | class RAGInstructionImprover(dspy.Module):
201 |     """Module that uses RAG to improve instructions with specialized documentation."""
202 | 
203 |     def __init__(self, retrieval_model):
204 |         super().__init__()
205 |         self.retrieve = retrieval_model  # Could be dspy.Retrieve or custom retriever
206 |         self.query_generator = dspy.ChainOfThought(GenerateDocumentationQuery)
207 |         self.generate_answer = dspy.ChainOfThought(GenerateRAGEnhancedInstruction)
208 | 
209 |     def forward(self, current_instruction: str, component_examples: list):
210 |         """Improve instruction using retrieved documentation."""
211 | 
212 |         # Let LM analyze examples and generate targeted retrieval queries
213 |         query_result = self.query_generator(
214 |             current_instruction=current_instruction,
215 |             examples_with_feedback=component_examples
216 |         )
217 | 
218 |         results = self.retrieve.query(
219 |             query_texts=query_result.retrieval_queries,
220 |             n_results=3
221 |         )
222 | 
223 |         relevant_docs_parts = []
224 |         for i, (query, query_docs) in enumerate(zip(query_result.retrieval_queries, results['documents'])):
225 |             if query_docs:
226 |                 docs_formatted = "\n".join([f"  - {doc}" for doc in query_docs])
227 |                 relevant_docs_parts.append(
228 |                     f"**Search Query #{i+1}**: {query}\n"
229 |                     f"**Retrieved Guidelines**:\n{docs_formatted}"
230 |                 )
231 | 
232 |         relevant_docs = "\n\n" + "="*60 + "\n\n".join(relevant_docs_parts) + "\n" + "="*60
233 | 
234 |         # Generate improved instruction with retrieved context
235 |         result = self.generate_answer(
236 |             current_instruction=current_instruction,
237 |             relevant_documentation=relevant_docs,
238 |             examples_with_feedback=component_examples
239 |         )
240 | 
241 |         return result
242 | 
243 | class DocumentationEnhancedProposer(ProposalFn):
244 |     """Instruction proposer that accesses specialized documentation via RAG."""
245 | 
246 |     def __init__(self, documentation_retriever):
247 |         """
248 |         Args:
249 |             documentation_retriever: A retrieval model that can search your specialized docs
250 |                                    Could be dspy.Retrieve, ChromadbRM, or custom retriever
251 |         """
252 |         self.instruction_improver = RAGInstructionImprover(documentation_retriever)
253 | 
254 |     def __call__(self, candidate: dict[str, str], reflective_dataset: dict[str, list[ReflectiveExample]], components_to_update: list[str]) -> dict[str, str]:
255 |         updated_components = {}
256 | 
257 |         for component_name in components_to_update:
258 |             if component_name not in candidate or component_name not in reflective_dataset:
259 |                 continue
260 | 
261 |             current_instruction = candidate[component_name]
262 |             component_examples = reflective_dataset[component_name]
263 | 
264 |             result = self.instruction_improver(
265 |                 current_instruction=current_instruction,
266 |                 component_examples=component_examples
267 |             )
268 | 
269 |             updated_components[component_name] = result.improved_instruction
270 | 
271 |         return updated_components
272 | 
273 | import chromadb
274 | 
275 | client = chromadb.Client()
276 | collection = client.get_collection("instruction_guidelines")
277 | 
278 | gepa = dspy.GEPA(
279 |     metric=task_specific_metric,
280 |     reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
281 |     instruction_proposer=DocumentationEnhancedProposer(collection),
282 |     auto="medium"
283 | )
284 | ```
285 | 
286 | #### Integration Patterns
287 | 
288 | **Using Custom Proposer with External LM:**
289 | 
290 | ```python
291 | class ExternalLMProposer(ProposalFn):
292 |     def __init__(self):
293 |         # Manage your own LM instance
294 |         self.external_lm = dspy.LM('gemini/gemini-2.5-pro')
295 | 
296 |     def __call__(self, candidate, reflective_dataset, components_to_update):
297 |         updated_components = {}
298 | 
299 |         with dspy.context(lm=self.external_lm):
300 |             # Your custom logic here using self.external_lm
301 |             for component_name in components_to_update:
302 |                 # ... implementation
303 |                 pass
304 | 
305 |         return updated_components
306 | 
307 | gepa = dspy.GEPA(
308 |     metric=my_metric,
309 |     reflection_lm=None,  # Optional when using custom proposer
310 |     instruction_proposer=ExternalLMProposer(),
311 |     auto="medium"
312 | )
313 | ```
314 | 
315 | **Best Practices:**
316 | 
317 | - **Use the full power of DSPy**: Leverage DSPy components like `dspy.Module`, `dspy.Signature`, and `dspy.Predict` to create your instruction proposer rather than direct LM calls. Consider `dspy.Refine` for constraint satisfaction, `dspy.ChainOfThought` for complex reasoning tasks, and compose multiple modules for sophisticated instruction improvement workflows
318 | - **Enable holistic feedback analysis**: While dspy.GEPA's `GEPAFeedbackMetric` processes one (gold, prediction) pair at a time, instruction proposers receive all examples for a component in batch, enabling cross-example pattern detection and systematic issue identification.
319 | - **Mind data serialization**: Serializing everything to strings might not be ideal - handle complex input types (like `dspy.Image`) by maintaining their structure for better LM processing
320 | - **Test thoroughly**: Test your custom proposer with representative failure cases
321 | 
322 | ## Custom Component Selection
323 | 
324 | ### What is component_selector?
325 | 
326 | The `component_selector` parameter controls which components (predictors) in your DSPy program are selected for optimization at each GEPA iteration. Instead of the default round-robin approach that updates one component at a time, you can implement custom selection strategies that choose single or multiple components based on optimization state, performance trajectories, and other contextual information.
327 | 
328 | ### Default Behavior
329 | 
330 | By default, GEPA uses a **round-robin strategy** (`RoundRobinReflectionComponentSelector`) that cycles through components sequentially, optimizing one component per iteration:
331 | 
332 | ```python
333 | # Default round-robin component selection
334 | gepa = dspy.GEPA(
335 |     metric=my_metric,
336 |     reflection_lm=dspy.LM(model="gpt-5", temperature=1.0, max_tokens=32000, api_key=api_key),
337 |     # component_selector="round_robin"  # This is the default
338 |     auto="medium"
339 | )
340 | ```
341 | 
342 | ### Built-in Selection Strategies
343 | 
344 | **String-based selectors:**
345 | 
346 | - `"round_robin"` (default): Cycles through components one at a time
347 | - `"all"`: Selects all components for simultaneous optimization
348 | 
349 | ```python
350 | # Optimize all components simultaneously
351 | gepa = dspy.GEPA(
352 |     metric=my_metric,
353 |     reflection_lm=reflection_lm,
354 |     component_selector="all",  # Update all components together
355 |     auto="medium"
356 | )
357 | 
358 | # Explicit round-robin selection
359 | gepa = dspy.GEPA(
360 |     metric=my_metric,
361 |     reflection_lm=reflection_lm,
362 |     component_selector="round_robin",  # One component per iteration
363 |     auto="medium"
364 | )
365 | ```
366 | 
367 | ### When to Use Custom Component Selection
368 | 
369 | Consider implementing custom component selection when you need:
370 | 
371 | - **Dependency-aware optimization**: Update related components together (e.g., a classifier and its input formatter)
372 | - **LLM-driven selection**: Let an LLM analyze trajectories and decide which components need attention
373 | - **Resource-conscious optimization**: Balance optimization thoroughness with computational budget
374 | 
375 | ### Custom Component Selector Protocol
376 | 
377 | Custom component selectors must implement the [`ReflectionComponentSelector`](https://github.com/gepa-ai/gepa/blob/main/src/gepa/proposer/reflective_mutation/base.py) protocol by defining a callable class or function. GEPA will call your selector during optimization:
378 | 
379 | ```python
380 | from dspy.teleprompt.gepa.gepa_utils import GEPAState, Trajectory
381 | 
382 | class CustomComponentSelector:
383 |     def __call__(
384 |         self,
385 |         state: GEPAState,                    # Complete optimization state with history
386 |         trajectories: list[Trajectory],      # Execution traces from the current minibatch
387 |         subsample_scores: list[float],       # Scores for each example in the current minibatch
388 |         candidate_idx: int,                  # Index of the current program candidate being optimized
389 |         candidate: dict[str, str],           # Component name -> instruction mapping
390 |     ) -> list[str]:                          # Return list of component names to optimize
391 |         # Your custom component selection logic here
392 |         return selected_components
393 | 
394 | # Or as a function:
395 | def custom_component_selector(state, trajectories, subsample_scores, candidate_idx, candidate):
396 |     # Your custom component selection logic here
397 |     return selected_components
398 | ```
399 | 
400 | ### Custom Implementation Example
401 | 
402 | Here's a simple function that alternates between optimizing different halves of your components:
403 | 
404 | ```python
405 | def alternating_half_selector(state, trajectories, subsample_scores, candidate_idx, candidate):
406 |     """Optimize half the components on even iterations, half on odd iterations."""
407 |     components = list(candidate.keys())
408 | 
409 |     # If there's only one component, always optimize it
410 |     if len(components) <= 1:
411 |         return components
412 | 
413 |     mid_point = len(components) // 2
414 | 
415 |     # Use state.i (iteration counter) to alternate between halves
416 |     if state.i % 2 == 0:
417 |         # Even iteration: optimize first half
418 |         return components[:mid_point]
419 |     else:
420 |         # Odd iteration: optimize second half
421 |         return components[mid_point:]
422 | 
423 | # Usage
424 | gepa = dspy.GEPA(
425 |     metric=my_metric,
426 |     reflection_lm=reflection_lm,
427 |     component_selector=alternating_half_selector,
428 |     auto="medium"
429 | )
430 | ```
431 | 
432 | ### Integration with Custom Instruction Proposers
433 | 
434 | Component selectors work seamlessly with custom instruction proposers. The selector determines which components to update, then the instruction proposer generates new instructions for those components:
435 | 
436 | ```python
437 | # Combined custom selector + custom proposer
438 | gepa = dspy.GEPA(
439 |     metric=my_metric,
440 |     reflection_lm=reflection_lm,
441 |     component_selector=alternating_half_selector,
442 |     instruction_proposer=WordLimitProposer(max_words=500),
443 |     auto="medium"
444 | )
445 | ```
```

api/optimizers/GEPA/overview.md
```
1 | # dspy.GEPA: Reflective Prompt Optimizer
2 | 
3 | **GEPA** (Genetic-Pareto) is a reflective optimizer proposed in "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning" (Agrawal et al., 2025, [arxiv:2507.19457](https://arxiv.org/abs/2507.19457)), that adaptively evolves _textual components_ (such as prompts) of arbitrary systems. In addition to scalar scores returned by metrics, users can also provide GEPA with a text feedback to guide the optimization process. Such textual feedback provides GEPA more visibility into why the system got the score that it did, and then GEPA can introspect to identify how to improve the score. This allows GEPA to propose high performing prompts in very few rollouts.
4 | 
5 | <!-- START_API_REF -->
6 | ::: dspy.GEPA
7 |     handler: python
8 |     options:
9 |         members:
10 |             - auto_budget
11 |             - compile
12 |             - get_params
13 |         show_source: true
14 |         show_root_heading: true
15 |         heading_level: 2
16 |         docstring_style: google
17 |         show_root_full_path: true
18 |         show_object_full_path: false
19 |         separate_signature: false
20 |         inherited_members: true
21 | :::
22 | <!-- END_API_REF -->
23 | 
24 | One of the key insights behind GEPA is its ability to leverage domain-specific textual feedback. Users should provide a feedback function as the GEPA metric, which has the following call signature:
25 | <!-- START_API_REF -->
26 | ::: dspy.teleprompt.gepa.gepa.GEPAFeedbackMetric
27 |     handler: python
28 |     options:
29 |         members:
30 |             - __call__
31 |         show_source: true
32 |         show_root_heading: true
33 |         heading_level: 2
34 |         docstring_style: google
35 |         show_root_full_path: true
36 |         show_object_full_path: false
37 |         separate_signature: false
38 |         inherited_members: true
39 | :::
40 | <!-- END_API_REF -->
41 | 
42 | When `track_stats=True`, GEPA returns detailed results about all of the proposed candidates, and metadata about the optimization run. The results are available in the `detailed_results` attribute of the optimized program returned by GEPA, and has the following type:
43 | <!-- START_API_REF -->
44 | ::: dspy.teleprompt.gepa.gepa.DspyGEPAResult
45 |     handler: python
46 |     options:
47 |         show_source: true
48 |         show_root_heading: true
49 |         heading_level: 2
50 |         docstring_style: google
51 |         show_root_full_path: true
52 |         show_object_full_path: false
53 |         separate_signature: false
54 |         inherited_members: true
55 | :::
56 | <!-- END_API_REF -->
57 | 
58 | ## Usage Examples
59 | 
60 | See GEPA usage tutorials in [GEPA Tutorials](../../../tutorials/gepa_ai_program/index.md).
61 | 
62 | ### Inference-Time Search
63 | 
64 | GEPA can act as a test-time/inference search mechanism. By setting your `valset` to your _evaluation batch_ and using `track_best_outputs=True`, GEPA produces for each batch element the highest-scoring outputs found during the evolutionary search.
65 | 
66 | ```python
67 | gepa = dspy.GEPA(metric=metric, track_stats=True, ...)
68 | new_prog = gepa.compile(student, trainset=my_tasks, valset=my_tasks)
69 | highest_score_achieved_per_task = new_prog.detailed_results.highest_score_achieved_per_val_task
70 | best_outputs = new_prog.detailed_results.best_outputs_valset
71 | ```
72 | 
73 | ## How Does GEPA Work?
74 | 
75 | ### 1. **Reflective Prompt Mutation**
76 | 
77 | GEPA uses LLMs to _reflect_ on structured execution traces (inputs, outputs, failures, feedback), targeting a chosen module and proposing a new instruction/program text tailored to real observed failures and rich textual/environmental feedback.
78 | 
79 | ### 2. **Rich Textual Feedback as Optimization Signal**
80 | 
81 | GEPA can leverage _any_ textual feedback available—not just scalar rewards. This includes evaluation logs, code traces, failed parses, constraint violations, error message strings, or even isolated submodule-specific feedback. This allows actionable, domain-aware optimization. 
82 | 
83 | ### 3. **Pareto-based Candidate Selection**
84 | 
85 | Rather than evolving just the _best_ global candidate (which leads to local optima or stagnation), GEPA maintains a Pareto frontier: the set of candidates which achieve the highest score on at least one evaluation instance. In each iteration, the next candidate to mutate is sampled (with probability proportional to coverage) from this frontier, guaranteeing both exploration and robust retention of complementary strategies.
86 | 
87 | ### Algorithm Summary
88 | 
89 | 1. **Initialize** the candidate pool with the the unoptimized program.
90 | 2. **Iterate**:
91 |    - **Sample a candidate** (from Pareto frontier).
92 |    - **Sample a minibatch** from the train set.
93 |    - **Collect execution traces + feedbacks** for module rollout on minibatch.
94 |    - **Select a module** of the candidate for targeted improvement.
95 |    - **LLM Reflection:** Propose a new instruction/prompt for the targeted module using reflective meta-prompting and the gathered feedback.
96 |    - **Roll out the new candidate** on the minibatch; **if improved, evaluate on Pareto validation set**.
97 |    - **Update the candidate pool/Pareto frontier.**
98 |    - **[Optionally] System-aware merge/crossover**: Combine best-performing modules from distinct lineages.
99 | 3. **Continue** until rollout or metric budget is exhausted. 
100 | 4. **Return** candidate with best aggregate performance on validation.
101 | 
102 | ## Implementing Feedback Metrics
103 | 
104 | A well-designed metric is central to GEPA's sample efficiency and learning signal richness. GEPA expects the metric to returns a `dspy.Prediction(score=..., feedback=...)`. GEPA leverages natural language traces from LLM-based workflows for optimization, preserving intermediate trajectories and errors in plain text rather than reducing them to numerical rewards. This mirrors human diagnostic processes, enabling clearer identification of system behaviors and bottlenecks.
105 | 
106 | Practical Recipe for GEPA-Friendly Feedback:
107 | 
108 | - **Leverage Existing Artifacts**: Use logs, unit tests, evaluation scripts, and profiler outputs; surfacing these often suffices.
109 | - **Decompose Outcomes**: Break scores into per-objective components (e.g., correctness, latency, cost, safety) and attribute errors to steps.
110 | - **Expose Trajectories**: Label pipeline stages, reporting pass/fail with salient errors (e.g., in code generation pipelines).
111 | - **Ground in Checks**: Employ automatic validators (unit tests, schemas, simulators) or LLM-as-a-judge for non-verifiable tasks (as in PUPA).
112 | - **Prioritize Clarity**: Focus on error coverage and decision points over technical complexity.
113 | 
114 | ### Examples
115 | 
116 | - **Document Retrieval** (e.g., HotpotQA): List correctly retrieved, incorrect, or missed documents, beyond mere Recall/F1 scores.
117 | - **Multi-Objective Tasks** (e.g., PUPA): Decompose aggregate scores to reveal contributions from each objective, highlighting tradeoffs (e.g., quality vs. privacy).
118 | - **Stacked Pipelines** (e.g., code generation: parse → compile → run → profile → evaluate): Expose stage-specific failures; natural-language traces often suffice for LLM self-correction.
119 | 
120 | ## Custom Instruction Proposal
121 | 
122 | For advanced customization of GEPA's instruction proposal mechanism, including custom instruction proposers and component selectors, see [Advanced Features](GEPA_Advanced.md).
123 | 
124 | ## Further Reading
125 | 
126 | - [GEPA Paper: arxiv:2507.19457](https://arxiv.org/abs/2507.19457)
127 | - [GEPA Github](https://github.com/gepa-ai/gepa) - This repository provides the core GEPA evolution pipeline used by `dspy.GEPA` optimizer.
128 | - [DSPy Tutorials](../../../tutorials/gepa_ai_program/index.md)
```
