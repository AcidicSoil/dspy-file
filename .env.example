# ---- dspyteach CLI configuration ------------------------------------------
# Select the language-model provider. Options: ollama, lmstudio, openai.
DSPYTEACH_PROVIDER=ollama

# Model identifier for the selected provider.
# For Ollama, use the local tag. For LM Studio/OpenAI, use the model id from /v1/models.
DSPYTEACH_MODEL=hf.co/Mungert/Qwen3-4B-Thinking-2507-GGUF:Q4_K_M

# Uncomment when targeting an OpenAI-compatible endpoint (e.g. LM Studio).
#DSPYTEACH_API_BASE=http://localhost:1234/v1

# API key for OpenAI-compatible providers. For LM Studio you can leave this as 'lm-studio'.
#DSPYTEACH_API_KEY=lm-studio

# OpenAI platform key (fallback when DSPYTEACH_PROVIDER=openai and DSPYTEACH_API_KEY is unset).
#OPENAI_API_KEY=
